{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b08e32bd",
   "metadata": {},
   "source": [
    "# OGGM Glacier Modeling - HMA CMIP6 Scenarios\n",
    "\n",
    "This notebook performs glacier state simulation using the OGGM model with CMIP6 climate scenarios for future projections. The workflow is refactored from the original regional analysis to provide a complete modeling pipeline including:\n",
    "\n",
    "- Historical simulation with ERA5-Land data\n",
    "- CMIP6 climate data processing\n",
    "- Multiple SSP scenario projections (SSP126, SSP245, SSP370, SSP585)\n",
    "- Comprehensive sensitivity analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3a93d9",
   "metadata": {},
   "source": [
    "## Configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dbde38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for HMA region\n",
    "subregion = 'HMA'\n",
    "boundary_shapefile = '/mnt/402D567E601BAE10/OGGM/tmp/HMA_one.shp'\n",
    "working_dir = '/mnt/3FE827E84836B503/OGGM_results/climate_extreme_HMA_glacier_modeling_CMIP6_v3'\n",
    "\n",
    "# ERA5-Land local data paths (adapt these paths as needed)\n",
    "era5_local_dir = {\n",
    "        'tmp':'/mnt/3FE827E84836B503/ERA5_Land_monthly/era5_land_monthly_t2m_1950-2025_flat_HMA.nc',\n",
    "        'pre':'/mnt/3FE827E84836B503/ERA5_Land_monthly/era5_land_monthly_prcp_1950-2025_flat_HMA.nc',\n",
    "        'inv':'/mnt/3FE827E84836B503/ERA5_Land_monthly/era5_land_invariant_flat_HMA.nc'\n",
    "}\n",
    "\n",
    "# RGI regions covering Qilian Shan\n",
    "rgi_regions = [13, 14, 15]  # Central Asia, South Asia East, South Asia West\n",
    "rgi_version = '62'\n",
    "\n",
    "# Projection settings\n",
    "projection_start_year = 2025\n",
    "projection_end_year = 2100\n",
    "\n",
    "# CMIP6 scenarios to analyze\n",
    "cmip6_scenarios = ['ssp126', 'ssp245', 'ssp370', 'ssp585']\n",
    "cmip6_models = ['BCC-CSM2-MR', 'CAMS-CSM1-0', 'CESM2', 'CESM2-WACCM', \n",
    "                'EC-Earth3', 'EC-Earth3-Veg', 'FGOALS-f3-L', 'GFDL-ESM4',\n",
    "                'INM-CM4-8', 'INM-CM5-0', 'MPI-ESM1-2-HR', 'MRI-ESM2-0', 'NorESM2-MM']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360a3b08",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb006d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import os\n",
    "from time import gmtime, strftime\n",
    "import geopandas as gpd\n",
    "import shapely.geometry as shpg\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# OGGM imports\n",
    "from oggm import utils, workflow, tasks, graphics, global_tasks, shop\n",
    "import oggm.cfg as cfg\n",
    "from oggm.shop import gcm_climate, ecmwf\n",
    "from oggm.core import massbalance, climate\n",
    "from oggm.core.massbalance import MultipleFlowlineMassBalance\n",
    "from oggm.shop import create_scieno\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.robust.robust_linear_model import RLM\n",
    "from statsmodels.robust.norms import HuberT\n",
    "\n",
    "import pickle\n",
    "from datetime import datetime as dt\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('ticks')\n",
    "sns.set_context('notebook')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8d75fa",
   "metadata": {},
   "source": [
    "## Initialize OGGM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9937d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OGGM\n",
    "cfg.initialize(logging_level='WARNING')\n",
    "\n",
    "# Enable multiprocessing\n",
    "cfg.PARAMS['use_multiprocessing'] = True\n",
    "#cfg.PARAMS['mp_processes'] = 96\n",
    "\n",
    "# Enable geometry storage for analysis\n",
    "cfg.PARAMS['store_model_geometry'] = True\n",
    "cfg.PARAMS['store_fl_diagnostics'] = True\n",
    "\n",
    "# Set working directory\n",
    "cfg.PATHS['working_dir'] = utils.mkdir(working_dir, reset=False)  # reset to True for first use\n",
    "cfg.PARAMS['cfl_min_dt'] = 30\n",
    "cfg.PARAMS['continue_on_error'] = True\n",
    "cfg.PATHS['rgi_dir'] = './rgi'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e725dee8",
   "metadata": {},
   "source": [
    "## Select HMA glaciers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a1e3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read HMA boundary shapefile\n",
    "basin = gpd.read_file(boundary_shapefile)\n",
    "\n",
    "# Get RGI glaciers within the boundary for all specified regions\n",
    "gdf_sel = gpd.GeoDataFrame()\n",
    "for region in rgi_regions:\n",
    "    fr = utils.get_rgi_region_file(region, version=rgi_version)\n",
    "    gdf = gpd.read_file(fr)\n",
    "    in_bas = [basin.geometry.contains(shpg.Point(x, y))[0] for (x, y) in zip(gdf.CenLon, gdf.CenLat)]\n",
    "    gdf_region = gdf.loc[in_bas]\n",
    "    gdf_sel = gdf_sel._append(gdf_region)\n",
    "\n",
    "print(f\"Selected {len(gdf_sel)} glaciers in HMA region\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef04d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdirs = workflow.init_glacier_directories(gdf_sel['RGIId'].tolist())\n",
    "print(f\"Initialized {len(gdirs)} glacier directories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa1031a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize glacier directories from pre-processed data\n",
    "base_url = 'https://cluster.klima.uni-bremen.de/~oggm/gdirs/oggm_v1.6/L1-L2_files/centerlines/'\n",
    "gdirs = workflow.init_glacier_directories(gdf_sel, from_prepro_level=2, prepro_border=160, \n",
    "                                          prepro_base_url=base_url)\n",
    "print(f\"Initialized {len(gdirs)} glacier directories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc6024b-d912-4fe7-bf6f-590de8e2c9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdir_file = cfg.PATHS['working_dir'] + '/gdir_list.pkl'\n",
    "with open(gdir_file, 'wb') as f:\n",
    "    pickle.dump(gdirs, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71d6edf-d847-4f36-92ae-13c24cdf731b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gdir_file = cfg.PATHS['working_dir'] + '/gdir_list.pkl'\n",
    "with open(gdir_file, 'rb') as f:\n",
    "    gdirs = pickle.load(f)\n",
    "\n",
    "# gdirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3071f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out glaciers without complete preprocessing data\n",
    "path = cfg.PATHS['working_dir'] + '/per_glacier'\n",
    "incomplete_glaciers = []\n",
    "\n",
    "if os.path.exists(path):\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        parts = root.replace(path, '').strip(os.sep).split(os.sep)\n",
    "        if len(parts) == 3 and parts[2].startswith('RGI'):\n",
    "            inversion_file = os.path.join(root, 'inversion_flowlines.pkl')\n",
    "            if not os.path.exists(inversion_file):\n",
    "                incomplete_glaciers.append(parts[2])\n",
    "\n",
    "# Remove incomplete glaciers\n",
    "if len(incomplete_glaciers) > 0:\n",
    "    print(f\"Removing {len(incomplete_glaciers)} incomplete glaciers...\")\n",
    "    gdirs = [gdir for gdir in gdirs if gdir.rgi_id not in incomplete_glaciers]\n",
    "    print(f\"Remaining glaciers: {len(gdirs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbd6ddf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## SELECTE 500 GLACIERS FOR TEST\n",
    "\n",
    "start_idx, end_idx = 85400, 85900\n",
    "gdirs = gdirs[start_idx:end_idx]\n",
    "# gdirs\n",
    "\n",
    "# random_indices = np.random.choice(50000, 500, replace=False)\n",
    "# random_indices = np.sort(random_indices) # 500 Random Numbers\n",
    "# gdirs = [gdirs[i] for i in random_indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe23f38",
   "metadata": {},
   "source": [
    "## Climate preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbceda24",
   "metadata": {},
   "source": [
    "### Historical ERA5-Land"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bedcfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process ERA5-Land data for Qilian Shan\n",
    "print(\"Processing ERA5-Land data (this may take a while)...\")\n",
    "workflow.execute_entity_task(ecmwf.process_ecmwf_data, gdirs, dataset='ERA5L-LATEST',\n",
    "                            download=False, local_path_dict=era5_local_dir)\n",
    "\n",
    "# Remove glaciers without valid climate data\n",
    "valid_gdirs = []\n",
    "for gdir in gdirs:\n",
    "    fpath = gdir.dir + '/climate_historical.nc'\n",
    "    try:\n",
    "        tmp = xr.open_dataset(fpath)\n",
    "        valid_gdirs.append(gdir)\n",
    "        tmp.close()\n",
    "    except:\n",
    "        pass\n",
    "gdirs = valid_gdirs\n",
    "print(f\"Valid glaciers: {len(gdirs)}\")\n",
    "\n",
    "# Compile historical climate data\n",
    "climate_hist = utils.compile_climate_input(gdirs, filename='climate_historical', input_filesuffix='', path=False)\n",
    "climate_hist.to_netcdf(cfg.PATHS['working_dir']+'/ERA5_'+subregion+ f'_{start_idx}-{end_idx-1}.nc')\n",
    "\n",
    "print(\"Historical climate data compiled\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd9f68f",
   "metadata": {},
   "source": [
    "### CMIP6 Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7129060",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download and process CMIP6 data for each model and scenario\n",
    "print(\"Processing CMIP6 data for future projections...\")\n",
    "\n",
    "# Base URLs for CMIP6 data (update as needed)\n",
    "base_url_temp = '/mnt/402D567E601BAE10/OGGM/download_cache/cluster.klima.uni-bremen.de/~oggm/cmip6/GCM/{}/{}_{}_r1i1p1f1_tas.nc'\n",
    "base_url_precip = '/mnt/402D567E601BAE10/OGGM/download_cache/cluster.klima.uni-bremen.de/~oggm/cmip6/GCM/{}/{}_{}_r1i1p1f1_pr.nc'\n",
    "\n",
    "# Process each GCM and SSP scenario combination\n",
    "for gcm in cmip6_models:\n",
    "    print(f\"Processing model: {gcm}\")\n",
    "    for ssp in cmip6_scenarios:\n",
    "        # print(f\"  - Scenario: {ssp}\")\n",
    "        \n",
    "        # File paths for temperature and precipitation\n",
    "        ft = base_url_temp.format(gcm, gcm, ssp)\n",
    "        fp = base_url_precip.format(gcm, gcm, ssp)\n",
    "        \n",
    "        try:\n",
    "            # Process CMIP6 data for glaciers\n",
    "            workflow.execute_entity_task(\n",
    "                gcm_climate.process_cmip_data, gdirs,\n",
    "                filesuffix='_{}_{}'.format(gcm, ssp),\n",
    "                year_range=('2000', '2019'),  # Standard CMIP6 projection period, try different year ranges,include 2019\n",
    "                fpath_temp=ft,\n",
    "                fpath_precip=fp\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"    Warning: Failed to process {gcm} {ssp}: {e}\")\n",
    "            continue\n",
    "\n",
    "print(\"CMIP6 data processing complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d167c076",
   "metadata": {},
   "source": [
    "### Plot corrected temp and prcp series in CMIP6 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26005f10",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gcm_all = []  # in this array all datasets going to be stored with additional coordinates GCM and SSP\n",
    "creation_date = strftime(\"%Y-%m-%d %H:%M:%S\", gmtime())  # here add the current time for info\n",
    "for GCM in cmip6_models:  # loop through all GCMs\n",
    "    for SSP in cmip6_scenarios:  # loop through all SSPs\n",
    "        # rid = '_{}_{}_0019cor'.format(GCM, SSP)\n",
    "        rid = '_{}_{}'.format(GCM, SSP)\n",
    "        fpath = gdirs[0].dir+'/gcm_data'+rid+'.nc'\n",
    "        if os.path.exists(fpath) == False:\n",
    "            continue\n",
    "        else:\n",
    "            gcm_tmp = utils.compile_climate_input(gdirs, filename='gcm_data', input_filesuffix=rid, path=False)  # 聚合所有冰川的CMIP6气候数据\n",
    "        # compile_climate_input: month to date float, 1950-1-15 -> 1950.0\n",
    "        # gcm_tmp = xr.open_dataset(cfg.PATHS['working_dir']+'/climate_input'+rid+'.nc')\n",
    "\n",
    "        gcm_tmp.coords['GCM'] = GCM  # add GCM as a coordinate\n",
    "        gcm_tmp.coords['GCM'].attrs['description'] = 'used Global circulation Model'  # add a description for GCM\n",
    "        gcm_tmp = gcm_tmp.expand_dims(\"GCM\")  # add GCM as a dimension to all Data variables\n",
    "\n",
    "        gcm_tmp.coords['SSP'] = SSP  # add SSP as a coordinate\n",
    "        gcm_tmp.coords['SSP'].attrs['description'] = 'used Representative Concentration Pathway'  # add a description for SSP\n",
    "        gcm_tmp = gcm_tmp.expand_dims(\"SSP\")  # add SSP as a dimension to all Data variables\n",
    "\n",
    "        gcm_tmp.attrs['creation_date'] = creation_date  # also add todays date for info\n",
    "        gcm_all.append(gcm_tmp)  # add the dataset with extra coordinates to our final ds_all array\n",
    "\n",
    "        gcm_tmp.close()\n",
    "    \n",
    "gcm_merged = xr.combine_by_coords(gcm_all, fill_value=np.nan)  # define how the missing GCM, SSP combinations should be filled\n",
    "gcm_merged.to_netcdf(cfg.PATHS['working_dir']+'/GCM_'+subregion+ f'_cmip6_{start_idx}-{end_idx-1}.nc')\n",
    "gcm_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abee34bd-c73e-40a1-9082-1bf71c2c3cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_hist = xr.open_dataset(cfg.PATHS['working_dir']+'/ERA5_'+subregion+ f'_{start_idx}-{end_idx-1}.nc')\n",
    "# Calculate regional mean\n",
    "climate_hist_region = climate_hist.mean(dim='rgi_id', skipna=True, keep_attrs=True)#看一下\n",
    "\n",
    "gcm_merged = xr.open_dataset(cfg.PATHS['working_dir']+'/GCM_'+subregion+ f'_cmip6_{start_idx}-{end_idx-1}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188955c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_four_ssp = gcm_merged.mean(dim='rgi_id',  # over which dimension the sum should be taken, here we want to sum up over all glacier ids\n",
    "                            skipna=True,  # ignore nan values\n",
    "                            keep_attrs=True  # keep the variable descriptions\n",
    "                                    ).mean(dim='GCM',  # over which dimension the sum should be taken, here we want to sum up over all glacier ids\n",
    "                            skipna=True,  # ignore nan values\n",
    "                            keep_attrs=True # keep the variable descriptions\n",
    "                                        ).sel(time=slice(\"1950\", \"2101\"))# time to float\n",
    "mean_four_ssp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9093d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_label = pd.date_range(start=\"01/01/1950\", end=\"12/01/2100\", freq=\"MS\")\n",
    "mean_four_ssp['time'] = date_label\n",
    "\n",
    "fig, axes = plt.subplots(1, 2 , figsize=(12, 5))\n",
    "for i, ssp in enumerate(cmip6_scenarios):\n",
    "    selected_ssp = mean_four_ssp.sel(SSP=ssp).to_dataframe()\n",
    "    axes[0].plot(selected_ssp.index, selected_ssp['prcp'], label=ssp, alpha=0.8, color=plt.get_cmap('Accent')(i))\n",
    "    axes[1].plot(selected_ssp.index, selected_ssp['temp'], label=ssp, alpha=0.8, color=plt.get_cmap('Accent')(i))\n",
    "    # STL\n",
    "    prcp_trend = STL(selected_ssp['prcp'], period=12, trend=241, seasonal=13).fit().trend\n",
    "    temp_trend = STL(selected_ssp['temp'], period=12, trend=241, seasonal=13).fit().trend\n",
    "    \n",
    "    axes[0].plot(prcp_trend.index, prcp_trend.values, label=ssp+' trend', color=plt.get_cmap('Accent')(i+4))\n",
    "    axes[1].plot(temp_trend.index, temp_trend.values, label=ssp+' trend', color=plt.get_cmap('Accent')(i+4))\n",
    "\n",
    "axes[0].legend()\n",
    "axes[1].legend()\n",
    "axes[0].set_title('Precipitation')\n",
    "axes[1].set_title('Temperature')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83f15b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_label = pd.date_range(start=\"01/01/1950\", end=\"12/01/2100\", freq=\"MS\")\n",
    "mean_four_ssp['time'] = date_label\n",
    "\n",
    "fig, axes = plt.subplots(1, 2 , figsize=(12, 5))\n",
    "for i, ssp in enumerate(cmip6_scenarios):\n",
    "    selected_ssp = mean_four_ssp.sel(SSP=ssp).to_dataframe()\n",
    "    axes[0].plot(selected_ssp.resample('AS').sum().index, selected_ssp.resample('AS').sum()['prcp'], label=ssp, color=plt.get_cmap('Accent')(i))\n",
    "    axes[1].plot(selected_ssp.resample('AS').mean().index, selected_ssp.resample('AS').mean()['temp'], label=ssp, color=plt.get_cmap('Accent')(i))\n",
    "    # STL\n",
    "    prcp_trend = STL(selected_ssp['prcp'], period=12, trend=241, seasonal=13).fit().trend\n",
    "    temp_trend = STL(selected_ssp['temp'], period=12, trend=241, seasonal=13).fit().trend\n",
    "    \n",
    "    axes[0].plot(prcp_trend.index, prcp_trend.values*12, label=ssp+' trend', color=plt.get_cmap('Accent')(i+4))\n",
    "    axes[1].plot(temp_trend.index, temp_trend.values, label=ssp+' trend', color=plt.get_cmap('Accent')(i+4))\n",
    "    \n",
    "axes[0].legend()\n",
    "axes[1].legend()\n",
    "axes[0].set_title('Precipitation (mm/yr)')\n",
    "axes[1].set_title('Temperature (°C)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6400def8",
   "metadata": {},
   "source": [
    "### Compare corrected CMIP6 with ERA5-Land"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146ca7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_four_ssp = gcm_merged.mean(dim='rgi_id',\n",
    "                                skipna=True,\n",
    "                                keep_attrs=True  # keep the variable descriptions\n",
    "                                    ).mean(dim='GCM',\n",
    "                                skipna=True,  # ignore nan values\n",
    "                                keep_attrs=True # keep the variable descriptions\n",
    "                                ).sel(time=slice(\"1950\", \"2101\")) #to 2024-12\n",
    "\n",
    "compare_era5 = climate_hist_region.sel(time=slice(\"1950\", \"2025\"))\n",
    "\n",
    "date_label = pd.date_range(start=\"01/01/1950\", end=\"12/01/2024\", freq=\"MS\")\n",
    "mean_four_ssp['time'] = pd.date_range(start=\"01/01/1950\", end=\"12/01/2100\", freq=\"MS\")\n",
    "compare_era5['time'] = date_label\n",
    "\n",
    "fig, axes = plt.subplots(1, 2 , figsize=(12, 5))\n",
    "\n",
    "for i, ssp in enumerate(cmip6_scenarios):\n",
    "    selected_ssp = mean_four_ssp.sel(SSP=ssp).to_dataframe()\n",
    "    axes[0].plot(selected_ssp.index, selected_ssp['prcp'], label=ssp, color=plt.get_cmap('Accent')(i)) #mm/yr\n",
    "    axes[1].plot(selected_ssp.index, selected_ssp['temp'], label=ssp, color=plt.get_cmap('Accent')(i)) #°C\n",
    "    # STL\n",
    "    prcp_trend = STL(selected_ssp['prcp'], period=12, trend=241, seasonal=13).fit().trend\n",
    "    temp_trend = STL(selected_ssp['temp'], period=12, trend=241, seasonal=13).fit().trend\n",
    "    \n",
    "    axes[0].plot(prcp_trend.index, prcp_trend.values, label=ssp+' trend', color=plt.get_cmap('Accent')(i+4))\n",
    "    axes[1].plot(temp_trend.index, temp_trend.values, label=ssp+' trend', color=plt.get_cmap('Accent')(i+4))\n",
    "\n",
    "era5_df = compare_era5.to_dataframe()\n",
    "axes[0].plot(era5_df.index, era5_df['prcp'], label='ERA5-Land', color='black', alpha=0.5, linestyle='--')\n",
    "axes[1].plot(era5_df.index, era5_df['temp'], label='ERA5-Land', color='black', alpha=0.5, linestyle='--')\n",
    "prcp_trend = STL(era5_df['prcp'], period=12, trend=241, seasonal=13).fit().trend\n",
    "temp_trend = STL(era5_df['temp'], period=12, trend=241, seasonal=13).fit().trend\n",
    "axes[0].plot(prcp_trend.index, prcp_trend.values, label='ERA5_trend', color='k')\n",
    "axes[1].plot(temp_trend.index, temp_trend.values, label='ERA5_trend', color='k')\n",
    "\n",
    "# axes[0].legend()\n",
    "# axes[1].legend()\n",
    "axes[0].set_title('Precipitation (mm/month)')\n",
    "axes[1].set_title('Temperature (°C)')\n",
    "axes[0].set_xlim(dt(2008,1,1), dt(2025,1,1))\n",
    "axes[1].set_xlim(dt(2012,1,1), dt(2025,1,1))\n",
    "\n",
    "axes[0].set_ylim(0,50)\n",
    "axes[1].set_ylim(2,8)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c67ff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2 , figsize=(12, 5))\n",
    "for i, ssp in enumerate(cmip6_scenarios):\n",
    "    selected_ssp = mean_four_ssp.sel(SSP=ssp).to_dataframe()\n",
    "    axes[0].plot(selected_ssp.resample('AS').sum().index, selected_ssp.resample('AS').sum()['prcp'], label=ssp\n",
    "                , color=plt.get_cmap('Accent')(i))\n",
    "    axes[1].plot(selected_ssp.resample('AS').mean().index, selected_ssp.resample('AS').mean()['temp'], label=ssp\n",
    "                , color=plt.get_cmap('Accent')(i))\n",
    "    # STL\n",
    "    prcp_trend = STL(selected_ssp['prcp'], period=12, trend=241, seasonal=13).fit().trend\n",
    "    temp_trend = STL(selected_ssp['temp'], period=12, trend=241, seasonal=13).fit().trend\n",
    "    \n",
    "    axes[0].plot(prcp_trend.index, prcp_trend.values*12, label=ssp+' trend', color=plt.get_cmap('Accent')(i+4))\n",
    "    axes[1].plot(temp_trend.index, temp_trend.values, label=ssp+' trend', color=plt.get_cmap('Accent')(i+4))\n",
    "\n",
    "era5_df = compare_era5.to_dataframe()\n",
    "axes[0].plot(era5_df.resample('AS').sum().index, era5_df.resample('AS').sum()['prcp'], label='ERA5-Land', color='black')\n",
    "axes[1].plot(era5_df.resample('AS').mean().index, era5_df.resample('AS').mean()['temp'], label='ERA5-Land', color='black')\n",
    "\n",
    "prcp_trend = STL(era5_df['prcp'], period=12, trend=241, seasonal=13).fit().trend\n",
    "temp_trend = STL(era5_df['temp'], period=12, trend=241, seasonal=13).fit().trend\n",
    "axes[0].plot(prcp_trend.index, prcp_trend.values*12, label='ERA5_trend', color='k')\n",
    "axes[1].plot(temp_trend.index, temp_trend.values, label='ERA5_trend', color='k')\n",
    "\n",
    "axes[0].legend()\n",
    "axes[1].legend()\n",
    "axes[0].set_title('Precipitation (mm/year)')\n",
    "axes[1].set_title('Temperature (°C)')\n",
    "axes[0].set_xlim(dt(2000,1,1), dt(2035,1,1))\n",
    "axes[1].set_xlim(dt(2000,1,1), dt(2035,1,1))\n",
    "\n",
    "# axes[1].set_ylim(-15, -12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e43598",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_time_range = pd.date_range(start='2018-01-01', end='2024-12-01', freq='MS')\n",
    "\n",
    "for i, ssp in enumerate(cmip6_scenarios):\n",
    "    selected_ssp = mean_four_ssp.sel(SSP=ssp).to_dataframe()\n",
    "    temp_trend = STL(selected_ssp['temp'], period=12, trend=241, seasonal=13).fit().trend\n",
    "\n",
    "    print(f'{ssp}: {temp_trend.loc[target_time_range].mean()}')\n",
    "    \n",
    "\n",
    "era5_df = compare_era5.to_dataframe()\n",
    "temp_trend = STL(era5_df['temp'], period=12, trend=241, seasonal=13).fit().trend\n",
    "print(f'ERA5-Land: {temp_trend.loc[target_time_range].mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545680de-8805-4a1a-8c75-49de3ea0c903",
   "metadata": {},
   "source": [
    "### Comparison for a single glacier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41425296-bfee-437d-8845-44f30f475134",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 21\n",
    "\n",
    "mean_four_ssp = gcm_merged.sel(rgi_id=gdirs[idx].rgi_id\n",
    "                            ).mean(dim='GCM',\n",
    "                                skipna=True,  # ignore nan values\n",
    "                                keep_attrs=True # keep the variable descriptions\n",
    "                                ).sel(time=slice(\"1950\", \"2101\")) #to 2024-12\n",
    "\n",
    "compare_era5 = climate_hist.sel(rgi_id=gdirs[idx].rgi_id).sel(time=slice(\"1950\", \"2025\"))\n",
    "\n",
    "date_label = pd.date_range(start=\"01/01/1950\", end=\"12/01/2024\", freq=\"MS\")\n",
    "mean_four_ssp['time'] = pd.date_range(start=\"01/01/1950\", end=\"12/01/2100\", freq=\"MS\")\n",
    "compare_era5['time'] = date_label\n",
    "\n",
    "fig, axes = plt.subplots(1, 2 , figsize=(12, 5))\n",
    "\n",
    "for i, ssp in enumerate(cmip6_scenarios):\n",
    "    selected_ssp = mean_four_ssp.sel(SSP=ssp).to_dataframe()\n",
    "    axes[0].plot(selected_ssp.index, selected_ssp['prcp'], label=ssp, color=plt.get_cmap('Accent')(i)) #mm/yr\n",
    "    axes[1].plot(selected_ssp.index, selected_ssp['temp'], label=ssp, color=plt.get_cmap('Accent')(i)) #°C\n",
    "    # STL\n",
    "    prcp_trend = STL(selected_ssp['prcp'], period=12, trend=241, seasonal=13).fit().trend\n",
    "    temp_trend = STL(selected_ssp['temp'], period=12, trend=241, seasonal=13).fit().trend\n",
    "    \n",
    "    axes[0].plot(prcp_trend.index, prcp_trend.values, label=ssp+' trend', color=plt.get_cmap('Accent')(i+4))\n",
    "    axes[1].plot(temp_trend.index, temp_trend.values, label=ssp+' trend', color=plt.get_cmap('Accent')(i+4))\n",
    "\n",
    "era5_df = compare_era5.to_dataframe()\n",
    "axes[0].plot(era5_df.index, era5_df['prcp'], label='ERA5-Land', color='black', alpha=0.5, linestyle='--')\n",
    "axes[1].plot(era5_df.index, era5_df['temp'], label='ERA5-Land', color='black', alpha=0.5, linestyle='--')\n",
    "prcp_trend = STL(era5_df['prcp'], period=12, trend=241, seasonal=13).fit().trend\n",
    "temp_trend = STL(era5_df['temp'], period=12, trend=241, seasonal=13).fit().trend\n",
    "axes[0].plot(prcp_trend.index, prcp_trend.values, label='ERA5_trend', color='k')\n",
    "axes[1].plot(temp_trend.index, temp_trend.values, label='ERA5_trend', color='k')\n",
    "\n",
    "axes[0].legend()\n",
    "# axes[1].legend()\n",
    "axes[0].set_title(f'Precipitation (mm/month) {gdirs[idx].rgi_id}')\n",
    "axes[1].set_title(f'Temperature (°C) {gdirs[idx].rgi_id}')\n",
    "axes[0].set_xlim(dt(2000,1,1), dt(2035,1,1))\n",
    "axes[1].set_xlim(dt(2000,1,1), dt(2035,1,1))\n",
    "\n",
    "axes[1].set_ylim(2,8)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461d5f70-d2bd-458a-bc03-85f1061de7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2 , figsize=(12, 5))\n",
    "for i, ssp in enumerate(cmip6_scenarios):\n",
    "    selected_ssp = mean_four_ssp.sel(SSP=ssp).to_dataframe()\n",
    "    axes[0].plot(selected_ssp.resample('AS').sum().index, selected_ssp.resample('AS').sum()['prcp'], label=ssp\n",
    "                , color=plt.get_cmap('Accent')(i))\n",
    "    axes[1].plot(selected_ssp.resample('AS').mean().index, selected_ssp.resample('AS').mean()['temp'], label=ssp\n",
    "                , color=plt.get_cmap('Accent')(i))\n",
    "    # STL\n",
    "    prcp_trend = STL(selected_ssp['prcp'], period=12, trend=241, seasonal=13).fit().trend\n",
    "    temp_trend = STL(selected_ssp['temp'], period=12, trend=241, seasonal=13).fit().trend\n",
    "    \n",
    "    axes[0].plot(prcp_trend.index, prcp_trend.values*12, label=ssp+' trend', color=plt.get_cmap('Accent')(i+4))\n",
    "    axes[1].plot(temp_trend.index, temp_trend.values, label=ssp+' trend', color=plt.get_cmap('Accent')(i+4))\n",
    "\n",
    "era5_df = compare_era5.to_dataframe()\n",
    "axes[0].plot(era5_df.resample('AS').sum().index, era5_df.resample('AS').sum()['prcp'], label='ERA5-Land', color='black')\n",
    "axes[1].plot(era5_df.resample('AS').mean().index, era5_df.resample('AS').mean()['temp'], label='ERA5-Land', color='black')\n",
    "\n",
    "prcp_trend = STL(era5_df['prcp'], period=12, trend=241, seasonal=13).fit().trend\n",
    "temp_trend = STL(era5_df['temp'], period=12, trend=241, seasonal=13).fit().trend\n",
    "axes[0].plot(prcp_trend.index, prcp_trend.values*12, label='ERA5_trend', color='k')\n",
    "axes[1].plot(temp_trend.index, temp_trend.values, label='ERA5_trend', color='k')\n",
    "\n",
    "axes[0].legend()\n",
    "axes[1].legend()\n",
    "axes[0].set_title(f'Precipitation (mm/year) {gdirs[idx].rgi_id}')\n",
    "axes[1].set_title(f'Temperature (°C) {gdirs[idx].rgi_id}')\n",
    "axes[0].set_xlim(dt(2000,1,1), dt(2035,1,1))\n",
    "axes[1].set_xlim(dt(2000,1,1), dt(2035,1,1))\n",
    "\n",
    "# axes[1].set_ylim(-8, -2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c24556-027d-4f9c-93c9-1612f3c4725d",
   "metadata": {},
   "source": [
    "### simulate extreme fluctuations for CMIP6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9bed37-31f2-41c2-bd59-ef8c10f13aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# four ssps, 13 GCMs for all rgis\n",
    "target_cmip = gcm_merged.sel(time=slice(\"1950\", \"2101\")) #to 2100-12, because time in gcm_merged is float\n",
    "# for all rgis\n",
    "ref_era5 = climate_hist.sel(time=slice(\"1950\", \"2025\"))\n",
    "ref_era5['time'] = pd.date_range(start=\"01/01/1950\", end=\"12/01/2024\", freq=\"MS\")\n",
    "target_cmip['time'] = pd.date_range(start=\"01/01/1950\", end=\"12/01/2100\", freq=\"MS\")\n",
    "\n",
    "adjust_cmip = []\n",
    "\n",
    "for gdir in gdirs:\n",
    "    print(f'Simulate extremes for {gdir.rgi_id}')\n",
    "    \n",
    "    ref_date_range = pd.date_range(start=f\"01/01/1990\", end=f\"12/01/2024\", freq=\"MS\")\n",
    "    ref_era5_df = ref_era5.sel(rgi_id=gdir.rgi_id).to_dataframe().loc[ref_date_range] # 1990-2024\n",
    "\n",
    "    for GCM in cmip6_models:  # loop through all GCMs\n",
    "        print(f'    {GCM}:')\n",
    "        for SSP in cmip6_scenarios:  # loop through all SSPs\n",
    "            selected_cmip = target_cmip.sel(rgi_id=gdir.rgi_id, GCM=GCM, SSP=SSP)\n",
    "            selected_cmip_df = selected_cmip.to_dataframe()\n",
    "            selected_cmip_df['year'] = selected_cmip_df.index.year\n",
    "            selected_cmip_df['month'] = selected_cmip_df.index.month\n",
    "            print(f\"        {SSP}: {(~selected_cmip_df['prcp'].notna()).sum()} NaNs; {(selected_cmip_df['prcp']==0).sum()} zeros\")\n",
    "\n",
    "            selected_cmip_df_nona = selected_cmip_df.loc[selected_cmip_df['temp'].notna()]\n",
    "            max_date_nona = selected_cmip_df_nona.index.max()\n",
    "\n",
    "            # score for future years\n",
    "            future_cmip_df = selected_cmip_df.loc[(selected_cmip_df['year']>=2025) & (selected_cmip_df.index<=max_date_nona)] #monthly data\n",
    "            hist_cmip_df = selected_cmip_df.loc[ref_date_range]\n",
    "\n",
    "            adjusted_temp = selected_cmip_df['temp'].copy()\n",
    "            adjusted_prcp = selected_cmip_df['prcp'].copy()\n",
    "\n",
    "            # future adjustment using QDM\n",
    "            for m in range(1, 13): # Loop 1 to 12\n",
    "                m_hist_era5_df = ref_era5_df[ref_era5_df.index.month==m]\n",
    "                m_hist_cmip_df = hist_cmip_df[hist_cmip_df.index.month==m]\n",
    "                m_future_cmip_df = future_cmip_df[future_cmip_df.index.month==m]\n",
    "\n",
    "                for date, date_info in m_future_cmip_df.iterrows():\n",
    "                    cmip_temp = date_info['temp']\n",
    "                    τ_temp = (m_future_cmip_df['temp']<=cmip_temp).sum()/len(m_future_cmip_df['temp']) # quantile of future temp\n",
    "                    \n",
    "                    hist_era5_τ_temp = np.nanpercentile(m_hist_era5_df['temp'], τ_temp*100)\n",
    "                    hist_cmip6_τ_temp = np.nanpercentile(m_hist_cmip_df['temp'], τ_temp*100)\n",
    "                    corrected_cmip_temp = hist_era5_τ_temp + (cmip_temp - hist_cmip6_τ_temp)\n",
    "                    # for prcp, times\n",
    "                    cmip_prcp = date_info['prcp']\n",
    "                    τ_prcp = (m_future_cmip_df['prcp']<=cmip_prcp).sum()/len(m_future_cmip_df['prcp']) # quantile of future prcp\n",
    "\n",
    "                    hist_era5_τ_prcp = np.nanpercentile(m_hist_era5_df['prcp'], τ_prcp*100)\n",
    "                    hist_cmip6_τ_prcp = np.nanpercentile(m_hist_cmip_df['prcp'], τ_prcp*100)\n",
    "                    scalling_factor = cmip_prcp / hist_cmip6_τ_prcp\n",
    "                    if (scalling_factor > 4) or (np.isinf(scalling_factor)) or (np.isnan(scalling_factor)):\n",
    "                        scalling_factor = np.nanpercentile(m_future_cmip_df['prcp'], 50) / np.nanpercentile(m_hist_cmip_df['prcp'], 50) # use general trend\n",
    "                        if (scalling_factor > 4) or (np.isinf(scalling_factor)) or (np.isnan(scalling_factor)):\n",
    "                            scalling_factor = np.nanpercentile(future_cmip_df['prcp'], 50) / np.nanpercentile(hist_cmip_df['prcp'], 50)\n",
    "\n",
    "                    corrected_cmip_prcp = hist_era5_τ_prcp * scalling_factor\n",
    "\n",
    "                    adjusted_temp.loc[date] = corrected_cmip_temp\n",
    "                    adjusted_prcp.loc[date] = corrected_cmip_prcp\n",
    "\n",
    "            adjusted_prcp = adjusted_prcp.mask(adjusted_prcp<0, 0)\n",
    "            adjusted_df = pd.DataFrame({\n",
    "                'prcp': adjusted_prcp,\n",
    "                'temp': adjusted_temp\n",
    "            })\n",
    "            # turn to nc\n",
    "            adjusted_ds = selected_cmip.copy(deep=False)\n",
    "            \n",
    "            # 更新 prcp 和 temp 的值\n",
    "            adjusted_ds['prcp'] = xr.DataArray(\n",
    "                adjusted_df['prcp'].values,\n",
    "                dims=selected_cmip.prcp.dims,\n",
    "                coords={dim: selected_cmip.coords[dim] for dim in selected_cmip.prcp.dims},\n",
    "                attrs=selected_cmip.prcp.attrs if hasattr(selected_cmip.prcp, 'attrs') else {}\n",
    "            )\n",
    "            adjusted_ds['temp'] = xr.DataArray(\n",
    "                adjusted_df['temp'].values,\n",
    "                dims=selected_cmip.temp.dims,\n",
    "                coords={dim: selected_cmip.coords[dim] for dim in selected_cmip.temp.dims},\n",
    "                attrs=selected_cmip.temp.attrs if hasattr(selected_cmip.temp, 'attrs') else {}\n",
    "            )\n",
    "            \n",
    "            dims_to_expand = []\n",
    "            for coord_name in ['rgi_id', 'GCM', 'SSP']:\n",
    "                if coord_name in adjusted_ds.coords and coord_name not in adjusted_ds.dims:\n",
    "                    dims_to_expand.append(coord_name)\n",
    "            \n",
    "            if dims_to_expand:\n",
    "                adjusted_ds = adjusted_ds.expand_dims(dims_to_expand)\n",
    "\n",
    "            adjust_cmip.append(adjusted_ds)\n",
    "            print(f\"        {SSP}: {(~adjusted_df['prcp'].notna()).sum()} NaNs; {(adjusted_df['prcp']==0).sum()} zeros\")\n",
    "            \n",
    "\n",
    "adjust_cmip_merged = xr.combine_by_coords(adjust_cmip, fill_value=np.nan)\n",
    "adjust_cmip_merged.to_netcdf(cfg.PATHS['working_dir']+'/GCM_'+subregion+f'_cmip6_{start_idx}-{end_idx-1}_hist_extreme_repli.nc')\n",
    "adjust_cmip_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a186d2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for gdir in gdirs:\n",
    "#     print(f'{gdir.rgi_id}:')\n",
    "\n",
    "for GCM in cmip6_models:  # loop through all GCMs\n",
    "    print(f'    {GCM}:')\n",
    "    for SSP in cmip6_scenarios:  # loop through all SSPs\n",
    "        temp_cmip = adjust_cmip_merged.sel(rgi_id='RGI60-13.16449', GCM=GCM, SSP=SSP)\n",
    "        temp_df = temp_cmip.to_dataframe()\n",
    "\n",
    "        print(f\"        {SSP}: {(~temp_df['prcp'].notna()).sum()} NaNs\")\n",
    "        print(f\"        {SSP}: {(temp_df['prcp']==0).sum()} zeros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642356d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2, DQM\n",
    "# four ssps, 13 GCMs for all rgis\n",
    "target_cmip = gcm_merged.sel(time=slice(\"1950\", \"2101\")) #to 2100-12, because time in gcm_merged is float\n",
    "# for all rgis\n",
    "ref_era5 = climate_hist.sel(time=slice(\"1950\", \"2025\"))\n",
    "ref_era5['time'] = pd.date_range(start=\"01/01/1950\", end=\"12/01/2024\", freq=\"MS\")\n",
    "target_cmip['time'] = pd.date_range(start=\"01/01/1950\", end=\"12/01/2100\", freq=\"MS\")\n",
    "\n",
    "adjust_cmip = []\n",
    "\n",
    "for gdir in gdirs:\n",
    "    print(f'Simulate extremes for {gdir.rgi_id}')\n",
    "    \n",
    "    ref_date_range = pd.date_range(start=f\"01/01/1990\", end=f\"12/01/2024\", freq=\"MS\")\n",
    "    era5_df = ref_era5.sel(rgi_id=gdir.rgi_id).to_dataframe()\n",
    "    era5_temp_trend = STL(era5_df['temp'], period=12, trend=241, seasonal=13).fit().trend\n",
    "    era5_prcp_trend = STL(era5_df['prcp'], period=12, trend=241, seasonal=13).fit().trend\n",
    "    era5_df['temp_trend'] = era5_temp_trend\n",
    "    era5_df['temp_leftover'] = era5_df['temp'] - era5_df['temp_trend']\n",
    "    era5_df['prcp_trend'] = era5_prcp_trend\n",
    "    era5_df['prcp_leftover'] = era5_df['prcp'] - era5_df['prcp_trend'] # fluc\n",
    "    ref_era5_df = era5_df.loc[ref_date_range] # 1990-2024\n",
    "\n",
    "    for GCM in cmip6_models:  # loop through all GCMs\n",
    "        for SSP in cmip6_scenarios:  # loop through all SSPs\n",
    "            selected_cmip = target_cmip.sel(rgi_id=gdir.rgi_id, GCM=GCM, SSP=SSP)\n",
    "            selected_cmip_df = selected_cmip.to_dataframe()\n",
    "            selected_cmip_df['year'] = selected_cmip_df.index.year\n",
    "            selected_cmip_df['month'] = selected_cmip_df.index.month\n",
    "\n",
    "            cmip_temp_trend = STL(selected_cmip_df['temp'], period=12, trend=241, seasonal=13).fit().trend\n",
    "            cmip_prcp_trend = STL(selected_cmip_df['prcp'], period=12, trend=241, seasonal=13).fit().trend\n",
    "            selected_cmip_df['temp_trend'] = cmip_temp_trend\n",
    "            selected_cmip_df['temp_leftover'] = selected_cmip_df['temp'] - selected_cmip_df['temp_trend']\n",
    "            selected_cmip_df['prcp_trend'] = cmip_prcp_trend\n",
    "            selected_cmip_df['prcp_leftover'] = selected_cmip_df['prcp'] - selected_cmip_df['prcp_trend'] # fluc\n",
    "\n",
    "            selected_cmip_df_nona = selected_cmip_df.loc[selected_cmip_df['temp'].notna()]\n",
    "            max_date_nona = selected_cmip_df_nona.index.max()\n",
    "\n",
    "            # score for future years\n",
    "            future_cmip_df = selected_cmip_df.loc[(selected_cmip_df['year']>=2025) & (selected_cmip_df.index<=max_date_nona)] #monthly data\n",
    "            hist_cmip_df = selected_cmip_df.loc[ref_date_range]\n",
    "\n",
    "            adjusted_temp = selected_cmip_df['temp'].copy()\n",
    "            adjusted_prcp = selected_cmip_df['prcp'].copy()\n",
    "\n",
    "            # future adjustment using QDM\n",
    "            for m in range(1, 13): # Loop 1 to 12\n",
    "                m_hist_era5_df = ref_era5_df[ref_era5_df.index.month==m]\n",
    "                m_hist_cmip_df = hist_cmip_df[hist_cmip_df.index.month==m]\n",
    "                m_future_cmip_df = future_cmip_df[future_cmip_df.index.month==m]\n",
    "\n",
    "                m_future_cmip_df['τ_temp'] = m_future_cmip_df['temp_leftover'].rank(method='average', pct=True)\n",
    "                m_future_cmip_df['τ_prcp'] = m_future_cmip_df['prcp_leftover'].rank(method='average', pct=True)\n",
    "\n",
    "                for date, date_info in m_future_cmip_df.iterrows():\n",
    "                    cmip_temp_trend = date_info['temp_trend']\n",
    "                    τ_temp = date_info['τ_temp']\n",
    "                    hist_era5_τ_temp_flu = np.nanpercentile(m_hist_era5_df['temp_leftover'], τ_temp*100)\n",
    "                    corrected_cmip_temp = cmip_temp_trend + hist_era5_τ_temp_flu\n",
    "                    # for prcp\n",
    "                    cmip_prcp_trend = date_info['prcp_trend']\n",
    "                    τ_prcp = date_info['τ_prcp']\n",
    "                    hist_era5_τ_prcp_flu = np.nanpercentile(m_hist_era5_df['prcp_leftover'], τ_prcp*100)\n",
    "                    corrected_cmip_prcp = cmip_prcp_trend + hist_era5_τ_prcp_flu\n",
    "                    if corrected_cmip_prcp<0:\n",
    "                        corrected_cmip_prcp = 0\n",
    "                    \n",
    "                    adjusted_temp.loc[date] = corrected_cmip_temp\n",
    "                    adjusted_prcp.loc[date] = corrected_cmip_prcp\n",
    "\n",
    "            adjusted_prcp = adjusted_prcp.mask(adjusted_prcp<0, 0)\n",
    "            adjusted_df = pd.DataFrame({\n",
    "                'prcp': adjusted_prcp,\n",
    "                'temp': adjusted_temp\n",
    "            })\n",
    "            # turn to nc\n",
    "            adjusted_ds = selected_cmip.copy(deep=False)\n",
    "            \n",
    "            # 更新 prcp 和 temp 的值\n",
    "            adjusted_ds['prcp'] = xr.DataArray(\n",
    "                adjusted_df['prcp'].values,\n",
    "                dims=selected_cmip.prcp.dims,\n",
    "                coords={dim: selected_cmip.coords[dim] for dim in selected_cmip.prcp.dims},\n",
    "                attrs=selected_cmip.prcp.attrs if hasattr(selected_cmip.prcp, 'attrs') else {}\n",
    "            )\n",
    "            adjusted_ds['temp'] = xr.DataArray(\n",
    "                adjusted_df['temp'].values,\n",
    "                dims=selected_cmip.temp.dims,\n",
    "                coords={dim: selected_cmip.coords[dim] for dim in selected_cmip.temp.dims},\n",
    "                attrs=selected_cmip.temp.attrs if hasattr(selected_cmip.temp, 'attrs') else {}\n",
    "            )\n",
    "            \n",
    "            dims_to_expand = []\n",
    "            for coord_name in ['rgi_id', 'GCM', 'SSP']:\n",
    "                if coord_name in adjusted_ds.coords and coord_name not in adjusted_ds.dims:\n",
    "                    dims_to_expand.append(coord_name)\n",
    "            \n",
    "            if dims_to_expand:\n",
    "                adjusted_ds = adjusted_ds.expand_dims(dims_to_expand)\n",
    "\n",
    "            adjust_cmip.append(adjusted_ds)\n",
    "\n",
    "adjust_cmip_merged = xr.combine_by_coords(adjust_cmip, fill_value=np.nan)\n",
    "adjust_cmip_merged.to_netcdf(cfg.PATHS['working_dir']+'/GCM_'+subregion+f'_cmip6_{start_idx}-{end_idx-1}_hist_extreme_repli2.nc')\n",
    "adjust_cmip_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c72928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_cmip = target_cmip.sel(rgi_id=gdir.rgi_id, GCM=GCM, SSP=SSP)\n",
    "# selected_cmip_df = selected_cmip.to_dataframe()\n",
    "# selected_cmip_df['year'] = selected_cmip_df.index.year\n",
    "\n",
    "(adjusted_prcp-selected_cmip_df['prcp']).iloc[1500:].plot()\n",
    "print((adjusted_prcp-selected_cmip_df['prcp']).iloc[1500:].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df1241c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(adjusted_temp-selected_cmip_df['temp']).iloc[1500:].plot()\n",
    "print((adjusted_temp-selected_cmip_df['temp']).iloc[1500:].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0912004a-c82c-435b-9848-e0054982fff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cmip = gcm_merged.sel(time=slice(\"1950\", \"2101\")) #to 2100-12, because time in gcm_merged is float\n",
    "target_cmip['time'] = pd.date_range(start=\"01/01/1950\", end=\"12/01/2100\", freq=\"MS\")\n",
    "\n",
    "adjust_cmip_merged = xr.open_dataset(cfg.PATHS['working_dir']+'/GCM_'+subregion+f'_cmip6_{start_idx}-{end_idx-1}_hist_extreme_repli.nc')\n",
    "adjust_cmip_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e43157a",
   "metadata": {},
   "source": [
    "#### Average all rgis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3f5b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compared with CMIP6 without scaling fluc\n",
    "mean_four_ssp_adjusted = adjust_cmip_merged.mean(dim='rgi_id',\n",
    "                                skipna=True,\n",
    "                                keep_attrs=True  # keep the variable descriptions\n",
    "                                    ).mean(dim='GCM',\n",
    "                                skipna=True,  # ignore nan values\n",
    "                                keep_attrs=True # keep the variable descriptions\n",
    "                                ).sel(time=slice(\"1950\", \"2101\")) #to 2100-12\n",
    "\n",
    "compare_era5 = climate_hist_region.sel(time=slice(\"1950\", \"2025\"))\n",
    "mean_four_ssp_adjusted['time'] = pd.date_range(start=\"01/01/1950\", end=\"12/01/2100\", freq=\"MS\")\n",
    "compare_era5['time'] = pd.date_range(start=\"01/01/1950\", end=\"12/01/2024\", freq=\"MS\")\n",
    "\n",
    "mean_four_ssp = gcm_merged.mean(dim='rgi_id',\n",
    "                                skipna=True,\n",
    "                                keep_attrs=True  # keep the variable descriptions\n",
    "                                    ).mean(dim='GCM',\n",
    "                                skipna=True,  # ignore nan values\n",
    "                                keep_attrs=True # keep the variable descriptions\n",
    "                                ).sel(time=slice(\"1950\", \"2101\")) #to 2100-12\n",
    "mean_four_ssp['time'] = pd.date_range(start=\"01/01/1950\", end=\"12/01/2100\", freq=\"MS\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2 , figsize=(15, 5))\n",
    "for i, ssp in enumerate(cmip6_scenarios[3:4]):\n",
    "    selected_ssp_scaled = mean_four_ssp_adjusted.sel(SSP=ssp).to_dataframe()\n",
    "    axes[0].plot(selected_ssp_scaled.index, selected_ssp_scaled['prcp'], label=ssp, alpha=0.8, color=plt.get_cmap('Accent')(i), linewidth=3)\n",
    "    axes[1].plot(selected_ssp_scaled.index, selected_ssp_scaled['temp'], label=ssp, alpha=0.8, color=plt.get_cmap('Accent')(i), linewidth=3)\n",
    "    # STL\n",
    "    prcp_trend = STL(selected_ssp_scaled['prcp'], period=12, trend=241, seasonal=13).fit().trend\n",
    "    temp_trend = STL(selected_ssp_scaled['temp'], period=12, trend=241, seasonal=13).fit().trend\n",
    "    axes[0].plot(prcp_trend.index, prcp_trend.values, label=ssp+' trend', color=plt.get_cmap('Accent')(i+4))\n",
    "    axes[1].plot(temp_trend.index, temp_trend.values, label=ssp+' trend', color=plt.get_cmap('Accent')(i+4))\n",
    "\n",
    "    # plot CMIP6 without scaling fluc\n",
    "    selected_ssp = mean_four_ssp.sel(SSP=ssp).to_dataframe()\n",
    "    axes[0].plot(selected_ssp.index, selected_ssp['prcp'], alpha=1, color=plt.get_cmap('Accent')(i+2), linestyle='--')#not adjusted\n",
    "    axes[1].plot(selected_ssp.index, selected_ssp['temp'], alpha=1, color=plt.get_cmap('Accent')(i+2), linestyle='--')\n",
    "    # STL\n",
    "    prcp_trend = STL(selected_ssp['prcp'], period=12, trend=241, seasonal=13).fit().trend\n",
    "    temp_trend = STL(selected_ssp['temp'], period=12, trend=241, seasonal=13).fit().trend\n",
    "    axes[0].plot(prcp_trend.index, prcp_trend.values, color=plt.get_cmap('Accent')(i+4), linestyle='--',label=ssp+' trend (not adjusted)')\n",
    "    axes[1].plot(temp_trend.index, temp_trend.values, color=plt.get_cmap('Accent')(i+4), linestyle='--',label=ssp+' trend (not adjusted)')\n",
    "\n",
    "axes[0].legend()\n",
    "axes[1].legend()\n",
    "axes[0].set_title('Precipitation (mm/month)')\n",
    "axes[1].set_title('Temperature (°C)')\n",
    "axes[0].set_xlim(dt(2050,1,1), dt(2100,1,1))\n",
    "axes[1].set_xlim(dt(2080,1,1), dt(2100,1,1))\n",
    "# axes[0].set_xlim(dt(2025,1,1), dt(2101,1,1))\n",
    "# axes[1].set_xlim(dt(2025,1,1), dt(2101,1,1))\n",
    "axes[0].set_ylim(0, 50)\n",
    "axes[1].set_ylim(4, 10) #(-25, -18)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782da34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(selected_ssp_scaled.loc[pd.date_range('2025-1-1', '2100-12-1', freq='MS'), 'temp'].mean())\n",
    "print(selected_ssp.loc[pd.date_range('2025-1-1', '2100-12-1', freq='MS'), 'temp'].mean())\n",
    "\n",
    "print(selected_ssp_scaled.loc[pd.date_range('2025-1-1', '2100-12-1', freq='MS'), 'prcp'].mean())\n",
    "print(selected_ssp.loc[pd.date_range('2025-1-1', '2100-12-1', freq='MS'), 'prcp'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c454c696",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2 , figsize=(12, 5))\n",
    "for i, ssp in enumerate(cmip6_scenarios[1:2]):\n",
    "    selected_ssp_scaled = mean_four_ssp_adjusted.sel(SSP=ssp).to_dataframe()\n",
    "    axes[0].plot(selected_ssp_scaled.resample('AS').sum().index, selected_ssp_scaled.resample('AS').sum()['prcp'], label=ssp, color=plt.get_cmap('Accent')(i))\n",
    "    axes[1].plot(selected_ssp_scaled.resample('AS').mean().index, selected_ssp_scaled.resample('AS').mean()['temp'], label=ssp, color=plt.get_cmap('Accent')(i))\n",
    "    # STL\n",
    "    prcp_trend = STL(selected_ssp_scaled['prcp'], period=12, trend=241, seasonal=13).fit().trend\n",
    "    temp_trend = STL(selected_ssp_scaled['temp'], period=12, trend=241, seasonal=13).fit().trend\n",
    "    axes[0].plot(prcp_trend.index, prcp_trend.values*12, label=ssp+' trend', color=plt.get_cmap('Accent')(i+4))\n",
    "    axes[1].plot(temp_trend.index, temp_trend.values, label=ssp+' trend', color=plt.get_cmap('Accent')(i+4))\n",
    "\n",
    "    # plot CMIP6 without scaling fluc\n",
    "    selected_ssp = mean_four_ssp.sel(SSP=ssp).to_dataframe()\n",
    "    axes[0].plot(selected_ssp.resample('AS').sum().index, selected_ssp.resample('AS').sum()['prcp'], color=plt.get_cmap('Accent')(i), linestyle='--')\n",
    "    axes[1].plot(selected_ssp.resample('AS').mean().index, selected_ssp.resample('AS').mean()['temp'], color=plt.get_cmap('Accent')(i), linestyle='--')\n",
    "    prcp_trend = STL(selected_ssp['prcp'], period=12, trend=241, seasonal=13).fit().trend\n",
    "    temp_trend = STL(selected_ssp['temp'], period=12, trend=241, seasonal=13).fit().trend\n",
    "    axes[0].plot(prcp_trend.index, prcp_trend.values*12, color=plt.get_cmap('Accent')(i+4), linestyle='--', label=ssp+' trend (not adjusted)')\n",
    "    axes[1].plot(temp_trend.index, temp_trend.values, color=plt.get_cmap('Accent')(i+4), linestyle='--', label=ssp+' trend (not adjusted)')\n",
    "    \n",
    "axes[0].legend()\n",
    "axes[1].legend()\n",
    "axes[0].set_title('Precipitation (mm/yr)')\n",
    "axes[1].set_title('Temperature (°C)')\n",
    "axes[0].set_xlim(dt(2020,1,1), dt(2100,1,1))\n",
    "axes[1].set_xlim(dt(2060,1,1), dt(2100,1,1))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f18502",
   "metadata": {},
   "source": [
    "#### single glacier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146e4cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compared with CMIP6 without scaling fluc\n",
    "mean_four_ssp_adjusted = adjust_cmip_merged.sel(rgi_id=gdirs[-1].rgi_id, \n",
    "                                                GCM=cmip6_models[-1], time=slice(\"1950\", \"2101\")) #to 2100-12\n",
    "mean_four_ssp_adjusted['time'] = pd.date_range(start=\"01/01/1950\", end=\"12/01/2100\", freq=\"MS\")\n",
    "\n",
    "mean_four_ssp = gcm_merged.sel(rgi_id=gdirs[-1].rgi_id, \n",
    "                               GCM=cmip6_models[-1], time=slice(\"1950\", \"2101\")) #to 2100-12\n",
    "mean_four_ssp['time'] = pd.date_range(start=\"01/01/1950\", end=\"12/01/2100\", freq=\"MS\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2 , figsize=(15, 5))\n",
    "for i, ssp in enumerate(cmip6_scenarios[2:3]):\n",
    "    selected_ssp_scaled = mean_four_ssp_adjusted.sel(SSP=ssp).to_dataframe()\n",
    "    axes[0].plot(selected_ssp_scaled.index, selected_ssp_scaled['prcp'], label=ssp, alpha=0.8, color=plt.get_cmap('Accent')(i), linewidth=3)\n",
    "    axes[1].plot(selected_ssp_scaled.index, selected_ssp_scaled['temp'], label=ssp, alpha=0.8, color=plt.get_cmap('Accent')(i), linewidth=3)\n",
    "    # STL\n",
    "    prcp_trend = STL(selected_ssp_scaled['prcp'], period=12, trend=241, seasonal=13).fit().trend\n",
    "    temp_trend = STL(selected_ssp_scaled['temp'], period=12, trend=241, seasonal=13).fit().trend\n",
    "    axes[0].plot(prcp_trend.index, prcp_trend.values, label=ssp+' trend', color=plt.get_cmap('Accent')(i+4), zorder=5)\n",
    "    axes[1].plot(temp_trend.index, temp_trend.values, label=ssp+' trend', color=plt.get_cmap('Accent')(i+4))\n",
    "\n",
    "    # plot CMIP6 without scaling fluc\n",
    "    selected_ssp = mean_four_ssp.sel(SSP=ssp).to_dataframe()\n",
    "    axes[0].plot(selected_ssp.index, selected_ssp['prcp'], alpha=1, color=plt.get_cmap('Accent')(i+2), linestyle='--')#not adjusted\n",
    "    axes[1].plot(selected_ssp.index, selected_ssp['temp'], alpha=1, color=plt.get_cmap('Accent')(i+2), linestyle='--')\n",
    "    # STL\n",
    "    prcp_trend = STL(selected_ssp['prcp'], period=12, trend=241, seasonal=13).fit().trend\n",
    "    temp_trend = STL(selected_ssp['temp'], period=12, trend=241, seasonal=13).fit().trend\n",
    "    axes[0].plot(prcp_trend.index, prcp_trend.values, color=plt.get_cmap('Accent')(i+4), linestyle='--',label=ssp+' trend (not adjusted)', zorder=5)\n",
    "    axes[1].plot(temp_trend.index, temp_trend.values, color=plt.get_cmap('Accent')(i+4), linestyle='--',label=ssp+' trend (not adjusted)')\n",
    "\n",
    "axes[0].legend()\n",
    "axes[1].legend()\n",
    "axes[0].set_title('Precipitation (mm/month)')\n",
    "axes[1].set_title('Temperature (°C)')\n",
    "axes[0].set_xlim(dt(2040,1,1), dt(2100,1,1))\n",
    "axes[1].set_xlim(dt(2080,1,1), dt(2100,1,1))\n",
    "# axes[0].set_xlim(dt(2025,1,1), dt(2101,1,1))\n",
    "# axes[1].set_xlim(dt(2025,1,1), dt(2101,1,1))\n",
    "axes[0].set_ylim(80, 150)\n",
    "# axes[1].set_ylim(4, 8) #(-25, -18)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf308cd",
   "metadata": {},
   "source": [
    "## Glacier clibration and inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae9877e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get geodetic mass balance reference data for calibration\n",
    "altimetry_filepath = '/mnt/3FE827E84836B503/Altimetry_model_input/Altimetrymb_rgiregion_result_for_OGGM0024.csv'\n",
    "df_ref_dmdtda = utils.get_geodetic_mb_dataframe(file_path=altimetry_filepath)\n",
    "\n",
    "# Calibrate each glacier using geodetic mass balance\n",
    "print(\"Calibrating glaciers using geodetic mass balance (this may take a while)...\")\n",
    "for gdir in gdirs:\n",
    "    ref_period = '2000-01-01_2024-01-01'\n",
    "    \n",
    "    try:\n",
    "        # Get reference mass balance for this glacier\n",
    "        df_ref_dmdtda0 = df_ref_dmdtda.loc[gdir.rgi_id]\n",
    "        # df_ref_dmdtda0 = df_ref_dmdtda0.loc[df_ref_dmdtda0['period'] == ref_period]\n",
    "        dmdtda_reference = df_ref_dmdtda0['dmdtda'] * 1000  # m/yr -> mm/yr\n",
    "        \n",
    "        # Perform mu* calibration\n",
    "        climate.mu_star_calibration_from_geodetic_mb(\n",
    "            gdir, \n",
    "            ignore_hydro_months=True,\n",
    "            ref_mb=dmdtda_reference,\n",
    "            ref_period=ref_period,\n",
    "            step_height_for_corr=10,\n",
    "            max_height_change_for_corr=5000,\n",
    "            min_mu_star=20,\n",
    "            max_mu_star=500\n",
    "        )\n",
    "    except Exception as e:\n",
    "        # Skip glaciers that fail calibration\n",
    "        print(f\"Calibration failed for {gdir.rgi_id}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Remove glaciers without valid calibration\n",
    "valid_gdirs = []\n",
    "for gdir in gdirs:\n",
    "    try:\n",
    "        df = gdir.read_json('local_mustar')\n",
    "        mu_star = df['mu_star_glacierwide']\n",
    "        if not pd.isna(mu_star):\n",
    "            valid_gdirs.append(gdir)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "gdirs = valid_gdirs\n",
    "print(f\"Successfully calibrated {len(gdirs)} glaciers\")\n",
    "\n",
    "# Compute apparent mass balance and run inversion\n",
    "print(\"Computing ice thickness through inversion...\")\n",
    "for gdir in gdirs:\n",
    "    climate.apparent_mb_from_any_mb(gdir)\n",
    "\n",
    "# Run inversion tasks\n",
    "global_tasks.inversion_tasks(gdirs, glen_a=None, fs=None, filter_inversion_output=True)\n",
    "\n",
    "# Initialize present-time glacier\n",
    "workflow.execute_entity_task(tasks.init_present_time_glacier, gdirs, filesuffix='')\n",
    "\n",
    "print(\"Ice thickness computation complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aead6240-e90d-4640-99fd-1bec4d45102c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = cfg.PATHS['working_dir'] + '/per_glacier'\n",
    "gdf_sel2 = []\n",
    "\n",
    "if os.path.exists(path):\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        # Path structure: per_glacier/rgi6/XX/rgi_id/inversion_flowlines.pkl\n",
    "        parts = root.replace(path, '').strip(os.sep).split(os.sep)\n",
    "        if len(parts) == 3 and parts[2].startswith('RGI'):  # RGI ID level\n",
    "            inversion_file = os.path.join(root, 'inversion_output.pkl')\n",
    "            if not os.path.exists(inversion_file):\n",
    "                gdf_sel2.append(parts[2])\n",
    "\n",
    "# Remove incomplete glaciers\n",
    "if len(gdf_sel2) > 0:\n",
    "    print(f\"Removing {len(gdf_sel2)} incomplete glaciers...\")\n",
    "    gdirs[:] = [gdir for gdir in gdirs if gdir.rgi_id not in gdf_sel2]\n",
    "    print(f\"Remaining glaciers: {len(gdirs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efdeba9",
   "metadata": {},
   "source": [
    "## CMIP projection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c8e38a",
   "metadata": {},
   "source": [
    "### Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64d5c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate future forcing\n",
    "workflow.execute_entity_task(\n",
    "                create_scieno.repulicate_cmip6,\n",
    "                gdirs,\n",
    "                start_year=1950,\n",
    "                end_year=2100,\n",
    "                output_filesuffix='_normal',\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32695e1-c1d6-404f-8558-cacc8f6e111e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run projections for each CMIP6 scenario and model\n",
    "print(\"Running CMIP6 scenario projections (2025-2100)...\")\n",
    "\n",
    "projection_results = []\n",
    "\n",
    "for ssp in cmip6_scenarios:\n",
    "    print(f\"\\nProcessing scenario: {ssp}\")\n",
    "    \n",
    "    for gcm in cmip6_models:\n",
    "        print(f\"  - Model: {gcm}\")\n",
    "        \n",
    "        # Define file suffix for this model-scenario combination\n",
    "        raw_filesuffix = f'_{gcm}_{ssp}'\n",
    "        extreme_filesuffix = f'_{gcm}_{ssp}_normal'\n",
    "        projection_id = f'_proj_{gcm}_{ssp}_normal_{start_idx}-{end_idx-1}'\n",
    "            \n",
    "        try:\n",
    "            # Run projection simulation\n",
    "            workflow.execute_entity_task(\n",
    "                tasks.run_with_hydro, gdirs,\n",
    "                run_task=tasks.run_from_climate_data,\n",
    "                climate_filename='gcm_data',\n",
    "                climate_input_filesuffix=extreme_filesuffix,\n",
    "                fixed_geometry_spinup_yr=2000,\n",
    "                ref_area_from_y0=True,\n",
    "                output_filesuffix=projection_id,\n",
    "                store_monthly_hydro=False\n",
    "            )\n",
    "            \n",
    "            path = cfg.PATHS['working_dir'] + '/per_glacier'\n",
    "            gdf_sel2 = []\n",
    "            for root, dirs, files in os.walk(path):\n",
    "                # Path structure: per_glacier/rgi6/XX/rgi_id/inversion_flowlines.pkl\n",
    "                parts = root.replace(path, '').strip(os.sep).split(os.sep)\n",
    "                if len(parts) == 3 and parts[2].startswith('RGI'):  # RGI ID level\n",
    "                    result_file = os.path.join(root, 'model_diagnostics'+projection_id+'.nc')\n",
    "                    if not os.path.exists(result_file):\n",
    "                        gdf_sel2.append(parts[2])\n",
    "            if len(gdf_sel2) > 0:\n",
    "                gdirs[:] = [gdir for gdir in gdirs if gdir.rgi_id not in gdf_sel2]\n",
    "                print(f\"        Remaining glaciers: {len(gdirs)}\")\n",
    "                        \n",
    "            # Compile results for this model-scenario\n",
    "            ds_projection = utils.compile_run_output(gdirs, input_filesuffix=projection_id, path=False)\n",
    "            ds_projection = ds_projection.assign_coords(GCM=gcm, SSP=ssp)\n",
    "            ds_projection = ds_projection.expand_dims(['GCM', 'SSP'])\n",
    "\n",
    "            projection_results.append(ds_projection)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    Error running projection for {gcm} {ssp}: {e}\")\n",
    "            continue\n",
    "\n",
    "ds_all_projections = xr.combine_by_coords(projection_results, fill_value=np.nan, combine_attrs='override')\n",
    "ds_all_projections = ds_all_projections.sortby('rgi_id')\n",
    "output_path = cfg.PATHS['working_dir'] + f'/run_output_cmip6_normal_{start_idx}-{end_idx-1}.nc'\n",
    "ds_all_projections.to_netcdf(output_path)\n",
    "\n",
    "# for ssp in cmip6_scenarios:\n",
    "#     for gcm in cmip6_models:\n",
    "#         tmp_file_path = cfg.PATHS['working_dir'] + f'/run_output' + f'_proj_{gcm}_{ssp}_normal_{start_idx}-{end_idx-1}.nc'\n",
    "#         os.remove(tmp_file_path)\n",
    "\n",
    "print(\"\\nCMIP6 scenario projections complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285dbcca",
   "metadata": {},
   "source": [
    "#### Dynamic spinup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9cb351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic spining up\n",
    "SPINUP_START_YR = 1979 \n",
    "# Define file suffix for this model-scenario combination\n",
    "spinup_filesuffix = f'_spinup'\n",
    "print(\"Running CMIP6 scenario projections (2025-2100) with Dynamic Spinup...\")\n",
    "\n",
    "# spinning-up\n",
    "workflow.execute_entity_task(\n",
    "    tasks.run_dynamic_spinup, gdirs,\n",
    "    climate_input_filesuffix=None, \n",
    "    spinup_start_yr=SPINUP_START_YR,         \n",
    "    minimise_for='area',              \n",
    "    precision_percent=3,\n",
    "    precision_absolute=3,       \n",
    "    add_fixed_geometry_spinup=True,              \n",
    "    output_filesuffix=spinup_filesuffix          \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66c6a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "spinup_filesuffix = f'_spinup'\n",
    "path = cfg.PATHS['working_dir'] + '/per_glacier'\n",
    "gdf_sel2 = []\n",
    "\n",
    "for root, dirs, files in os.walk(path):\n",
    "    # Path structure: per_glacier/rgi6/XX/rgi_id/inversion_flowlines.pkl\n",
    "    parts = root.replace(path, '').strip(os.sep).split(os.sep)\n",
    "    if len(parts) == 3 and parts[2].startswith('RGI'):  # RGI ID level\n",
    "        result_file = os.path.join(root, 'model_diagnostics'+spinup_filesuffix+'.nc')\n",
    "        if not os.path.exists(result_file):\n",
    "            gdf_sel2.append(parts[2])\n",
    "if len(gdf_sel2) > 0:\n",
    "    gdirs[:] = [gdir for gdir in gdirs if gdir.rgi_id not in gdf_sel2]\n",
    "\n",
    "print(f\"Dynamic spinup complete. Glaciers ready: {len(gdirs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dde947",
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_results_sp = []\n",
    "\n",
    "for ssp in cmip6_scenarios:\n",
    "    print(f\"\\nProcessing scenario: {ssp}\")\n",
    "    \n",
    "    for gcm in cmip6_models:\n",
    "        print(f\"  - Model: {gcm}\")\n",
    "        # raw_filesuffix = f'_{gcm}_{ssp}'\n",
    "        extreme_filesuffix = f'_{gcm}_{ssp}_normal'\n",
    "        projection_id = f'_proj_{gcm}_{ssp}_normal_spinup_{start_idx}-{end_idx-1}'\n",
    "            \n",
    "        try:\n",
    "            # Run projection simulation\n",
    "            workflow.execute_entity_task(\n",
    "                tasks.run_with_hydro, gdirs,\n",
    "                run_task=tasks.run_from_climate_data,\n",
    "                climate_filename='gcm_data',\n",
    "                climate_input_filesuffix=extreme_filesuffix,\n",
    "                init_model_filesuffix=spinup_filesuffix,\n",
    "                init_model_yr=2000,\n",
    "                ref_area_from_y0=True,\n",
    "                output_filesuffix=projection_id,\n",
    "                store_monthly_hydro=False\n",
    "            )\n",
    "            \n",
    "            path = cfg.PATHS['working_dir'] + '/per_glacier'\n",
    "            gdf_sel2 = []\n",
    "            for root, dirs, files in os.walk(path):\n",
    "                # Path structure: per_glacier/rgi6/XX/rgi_id/inversion_flowlines.pkl\n",
    "                parts = root.replace(path, '').strip(os.sep).split(os.sep)\n",
    "                if len(parts) == 3 and parts[2].startswith('RGI'):  # RGI ID level\n",
    "                    result_file = os.path.join(root, 'model_diagnostics'+projection_id+'.nc')\n",
    "                    if not os.path.exists(result_file):\n",
    "                        gdf_sel2.append(parts[2])\n",
    "            if len(gdf_sel2) > 0:\n",
    "                gdirs[:] = [gdir for gdir in gdirs if gdir.rgi_id not in gdf_sel2]\n",
    "                print(f\"        Remaining glaciers: {len(gdirs)}\")\n",
    "                        \n",
    "            # Compile results for this model-scenario\n",
    "            ds_projection = utils.compile_run_output(gdirs, input_filesuffix=projection_id, path=False)\n",
    "            ds_projection = ds_projection.assign_coords(GCM=gcm, SSP=ssp)\n",
    "            ds_projection = ds_projection.expand_dims(['GCM', 'SSP'])\n",
    "\n",
    "            projection_results_sp.append(ds_projection)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    Error running projection for {gcm} {ssp}: {e}\")\n",
    "            continue\n",
    "\n",
    "ds_all_projections = xr.combine_by_coords(projection_results_sp, fill_value=np.nan, combine_attrs='override')\n",
    "ds_all_projections = ds_all_projections.sortby('rgi_id')\n",
    "output_path = cfg.PATHS['working_dir'] + f'/run_output_cmip6_normal_spinup_{start_idx}-{end_idx-1}.nc'\n",
    "ds_all_projections.to_netcdf(output_path)\n",
    "\n",
    "print(\"\\nCMIP6 scenario projections complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf19830",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_all_projections = xr.open_dataset(cfg.PATHS['working_dir'] + f'/run_output_cmip6_normal_{start_idx}-{end_idx-1}.nc')\n",
    "ds_all_projections_sp = xr.open_dataset(cfg.PATHS['working_dir'] + f'/run_output_cmip6_normal_spinup_{start_idx}-{end_idx-1}.nc')\n",
    "\n",
    "ds_all_projections.sum(\n",
    "    dim='rgi_id',\n",
    "    skipna=True,\n",
    "    keep_attrs=True,\n",
    ").sel(SSP='ssp585').sel(GCM='CAMS-CSM1-0').volume.plot(label='no spinup')# to 2100\n",
    "\n",
    "ds_all_projections_sp.sum(\n",
    "    dim='rgi_id',\n",
    "    skipna=True,\n",
    "    keep_attrs=True,\n",
    ").sel(SSP='ssp585').sel(GCM='CAMS-CSM1-0').volume.plot(label='spinup')# to 2100\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997f9375",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_date = np.mean([item.rgi_date for item in gdirs])\n",
    "ref_volume = np.sum([tasks.get_inversion_volume(item) for item in gdirs])\n",
    "\n",
    "ds_all_projections.sum(\n",
    "                dim='rgi_id',\n",
    "                skipna=True,\n",
    "                keep_attrs=True,\n",
    "            ).mean(\n",
    "                dim='GCM',\n",
    "                skipna=True,\n",
    "                keep_attrs=True,\n",
    "            ).sel(SSP='ssp585').volume.plot(label='no spinup')# to 2100\n",
    "\n",
    "ds_all_projections_sp.sum(\n",
    "                dim='rgi_id',\n",
    "                skipna=True,\n",
    "                keep_attrs=True,\n",
    "            ).mean(\n",
    "                dim='GCM',\n",
    "                skipna=True,\n",
    "                keep_attrs=True,\n",
    "            ).sel(SSP='ssp585').volume.plot(label='spinup')# to 2100\n",
    "\n",
    "plt.scatter(ref_date, ref_volume, c='red')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab79f44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_date = np.mean([item.rgi_date for item in gdirs])\n",
    "ref_area = np.sum([item.rgi_area_m2 for item in gdirs])\n",
    "\n",
    "ds_all_projections.sum(\n",
    "                dim='rgi_id',\n",
    "                skipna=True,\n",
    "                keep_attrs=True,\n",
    "            ).mean(\n",
    "                dim='GCM',\n",
    "                skipna=True,\n",
    "                keep_attrs=True,\n",
    "            ).sel(SSP='ssp585').area.plot(label='no spinup')# to 2100\n",
    "\n",
    "ds_all_projections_sp.sum(\n",
    "                dim='rgi_id',\n",
    "                skipna=True,\n",
    "                keep_attrs=True,\n",
    "            ).mean(\n",
    "                dim='GCM',\n",
    "                skipna=True,\n",
    "                keep_attrs=True,\n",
    "            ).sel(SSP='ssp585').area.plot(label='spinup')# to 2100\n",
    "\n",
    "plt.scatter(ref_date, ref_area, c='red')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c928038d",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_file_list = []\n",
    "\n",
    "for item in os.listdir(cfg.PATHS['working_dir']):\n",
    "    if item.startswith('run_output_cmip6_normal_') and \"all\" not in item:  \n",
    "        file_path = os.path.join(cfg.PATHS['working_dir'], item)\n",
    "        if os.path.isfile(file_path):  # 确保是文件\n",
    "            processed_file_list.append(file_path)\n",
    "\n",
    "if len(processed_file_list) > 0:\n",
    "    print(f\"\\nFound {len(processed_file_list)} files to merge...\")\n",
    "    compile_result = xr.open_mfdataset(processed_file_list, combine='by_coords')\n",
    "    # sort by rgi_id\n",
    "    compile_result = compile_result.sortby('rgi_id')\n",
    "    compile_result.to_netcdf(os.path.join(cfg.PATHS['working_dir'], 'run_output_cmip6_normal_all.nc'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff1aba3",
   "metadata": {},
   "source": [
    "### Repulicate extremes upon QDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a76ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate future forcing\n",
    "workflow.execute_entity_task(\n",
    "                create_scieno.simulate_future_extremes_QDM,\n",
    "                gdirs,\n",
    "                start_year=1950,\n",
    "                end_year=2100,\n",
    "                future_start_year=2025,\n",
    "                future_cooling_factor=None,\n",
    "                output_filesuffix='_repu_his_extremes_QDM',\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ab9162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run projections for each CMIP6 scenario and model\n",
    "print(\"Running CMIP6 scenario projections (2025-2100)...\")\n",
    "\n",
    "projection_results = []\n",
    "\n",
    "for ssp in cmip6_scenarios:\n",
    "    print(f\"\\nProcessing scenario: {ssp}\")\n",
    "    \n",
    "    for gcm in cmip6_models:\n",
    "        print(f\"  - Model: {gcm}\")\n",
    "        \n",
    "        # Define file suffix for this model-scenario combination\n",
    "        raw_filesuffix = f'_{gcm}_{ssp}'\n",
    "        extreme_filesuffix = f'_{gcm}_{ssp}_repu_his_extremes_QDM'\n",
    "        projection_id = f'_proj_{gcm}_{ssp}_repu_his_extremes_QDM_{start_idx}-{end_idx-1}'\n",
    "            \n",
    "        try:\n",
    "            # Run projection simulation\n",
    "            workflow.execute_entity_task(\n",
    "                tasks.run_with_hydro, gdirs,\n",
    "                run_task=tasks.run_from_climate_data,\n",
    "                climate_filename='gcm_data',\n",
    "                climate_input_filesuffix=extreme_filesuffix,\n",
    "                fixed_geometry_spinup_yr=2000,\n",
    "                ref_area_from_y0=True,\n",
    "                output_filesuffix=projection_id,\n",
    "                store_monthly_hydro=False\n",
    "            )\n",
    "            \n",
    "            path = cfg.PATHS['working_dir'] + '/per_glacier'\n",
    "            gdf_sel2 = []\n",
    "            for root, dirs, files in os.walk(path):\n",
    "                # Path structure: per_glacier/rgi6/XX/rgi_id/inversion_flowlines.pkl\n",
    "                parts = root.replace(path, '').strip(os.sep).split(os.sep)\n",
    "                if len(parts) == 3 and parts[2].startswith('RGI'):  # RGI ID level\n",
    "                    result_file = os.path.join(root, 'model_diagnostics'+projection_id+'.nc')\n",
    "                    if not os.path.exists(result_file):\n",
    "                        gdf_sel2.append(parts[2])\n",
    "            if len(gdf_sel2) > 0:\n",
    "                gdirs[:] = [gdir for gdir in gdirs if gdir.rgi_id not in gdf_sel2]\n",
    "                print(f\"        Remaining glaciers: {len(gdirs)}\")\n",
    "                        \n",
    "            # Compile results for this model-scenario\n",
    "            ds_projection = utils.compile_run_output(gdirs, input_filesuffix=projection_id, path=False)\n",
    "            ds_projection = ds_projection.assign_coords(GCM=gcm, SSP=ssp)\n",
    "            ds_projection = ds_projection.expand_dims(['GCM', 'SSP'])\n",
    "\n",
    "            projection_results.append(ds_projection)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    Error running projection for {gcm} {ssp}: {e}\")\n",
    "            continue\n",
    "\n",
    "ds_all_projections = xr.combine_by_coords(projection_results, fill_value=np.nan, combine_attrs='override')\n",
    "ds_all_projections = ds_all_projections.sortby('rgi_id')\n",
    "output_path = cfg.PATHS['working_dir'] + f'/run_output_cmip6_repu_his_extremes_QDM_{start_idx}-{end_idx-1}.nc'\n",
    "ds_all_projections.to_netcdf(output_path)\n",
    "\n",
    "print(\"\\nCMIP6 scenario projections complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa617b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_results_sp = []\n",
    "\n",
    "for ssp in cmip6_scenarios:\n",
    "    print(f\"\\nProcessing scenario: {ssp}\")\n",
    "    \n",
    "    for gcm in cmip6_models:\n",
    "        print(f\"  - Model: {gcm}\")\n",
    "        # raw_filesuffix = f'_{gcm}_{ssp}'\n",
    "        extreme_filesuffix = f'_{gcm}_{ssp}_repu_his_extremes_QDM'\n",
    "        projection_id = f'_proj_{gcm}_{ssp}_repu_his_extremes_QDM_spinup_{start_idx}-{end_idx-1}'\n",
    "            \n",
    "        try:\n",
    "            # Run projection simulation\n",
    "            workflow.execute_entity_task(\n",
    "                tasks.run_with_hydro, gdirs,\n",
    "                run_task=tasks.run_from_climate_data,\n",
    "                climate_filename='gcm_data',\n",
    "                climate_input_filesuffix=extreme_filesuffix,\n",
    "                init_model_filesuffix=spinup_filesuffix,\n",
    "                init_model_yr=2000,\n",
    "                ref_area_from_y0=True,\n",
    "                output_filesuffix=projection_id,\n",
    "                store_monthly_hydro=False\n",
    "            )\n",
    "            \n",
    "            path = cfg.PATHS['working_dir'] + '/per_glacier'\n",
    "            gdf_sel2 = []\n",
    "            for root, dirs, files in os.walk(path):\n",
    "                # Path structure: per_glacier/rgi6/XX/rgi_id/inversion_flowlines.pkl\n",
    "                parts = root.replace(path, '').strip(os.sep).split(os.sep)\n",
    "                if len(parts) == 3 and parts[2].startswith('RGI'):  # RGI ID level\n",
    "                    result_file = os.path.join(root, 'model_diagnostics'+projection_id+'.nc')\n",
    "                    if not os.path.exists(result_file):\n",
    "                        gdf_sel2.append(parts[2])\n",
    "            if len(gdf_sel2) > 0:\n",
    "                gdirs[:] = [gdir for gdir in gdirs if gdir.rgi_id not in gdf_sel2]\n",
    "                print(f\"        Remaining glaciers: {len(gdirs)}\")\n",
    "                        \n",
    "            # Compile results for this model-scenario\n",
    "            ds_projection = utils.compile_run_output(gdirs, input_filesuffix=projection_id, path=False)\n",
    "            ds_projection = ds_projection.assign_coords(GCM=gcm, SSP=ssp)\n",
    "            ds_projection = ds_projection.expand_dims(['GCM', 'SSP'])\n",
    "\n",
    "            projection_results_sp.append(ds_projection)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    Error running projection for {gcm} {ssp}: {e}\")\n",
    "            continue\n",
    "\n",
    "ds_all_projections = xr.combine_by_coords(projection_results_sp, fill_value=np.nan, combine_attrs='override')\n",
    "ds_all_projections = ds_all_projections.sortby('rgi_id')\n",
    "output_path = cfg.PATHS['working_dir'] + f'/run_output_cmip6_repu_his_extremes_QDM_spinup_{start_idx}-{end_idx-1}.nc'\n",
    "ds_all_projections.to_netcdf(output_path)\n",
    "\n",
    "print(\"\\nCMIP6 scenario projections complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1fbc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_all_projections = xr.open_dataset(cfg.PATHS['working_dir'] + f'/run_output_cmip6_repu_his_extremes_QDM_{start_idx}-{end_idx-1}.nc')\n",
    "ds_all_projections_sp = xr.open_dataset(cfg.PATHS['working_dir'] + f'/run_output_cmip6_repu_his_extremes_QDM_spinup_{start_idx}-{end_idx-1}.nc')\n",
    "\n",
    "ds_all_projections.sum(\n",
    "    dim='rgi_id',\n",
    "    skipna=True,\n",
    "    keep_attrs=True,\n",
    ").sel(SSP='ssp585').sel(GCM='CAMS-CSM1-0').volume.plot(label='no spinup')# to 2100\n",
    "\n",
    "ds_all_projections_sp.sum(\n",
    "    dim='rgi_id',\n",
    "    skipna=True,\n",
    "    keep_attrs=True,\n",
    ").sel(SSP='ssp585').sel(GCM='CAMS-CSM1-0').volume.plot(label='spinup')# to 2100\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88674205",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_date = np.mean([item.rgi_date for item in gdirs])\n",
    "ref_volume = np.sum([tasks.get_inversion_volume(item) for item in gdirs])\n",
    "\n",
    "ds_all_projections.sum(\n",
    "                dim='rgi_id',\n",
    "                skipna=True,\n",
    "                keep_attrs=True,\n",
    "            ).mean(\n",
    "                dim='GCM',\n",
    "                skipna=True,\n",
    "                keep_attrs=True,\n",
    "            ).sel(SSP='ssp585').volume.plot(label='no spinup')# to 2100\n",
    "\n",
    "ds_all_projections_sp.sum(\n",
    "                dim='rgi_id',\n",
    "                skipna=True,\n",
    "                keep_attrs=True,\n",
    "            ).mean(\n",
    "                dim='GCM',\n",
    "                skipna=True,\n",
    "                keep_attrs=True,\n",
    "            ).sel(SSP='ssp585').volume.plot(label='spinup')# to 2100\n",
    "\n",
    "plt.scatter(ref_date, ref_volume, c='red')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be766a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_date = np.mean([item.rgi_date for item in gdirs])\n",
    "ref_area = np.sum([item.rgi_area_m2 for item in gdirs])\n",
    "\n",
    "ds_all_projections.sum(\n",
    "                dim='rgi_id',\n",
    "                skipna=True,\n",
    "                keep_attrs=True,\n",
    "            ).mean(\n",
    "                dim='GCM',\n",
    "                skipna=True,\n",
    "                keep_attrs=True,\n",
    "            ).sel(SSP='ssp585').area.plot(label='no spinup')# to 2100\n",
    "\n",
    "ds_all_projections_sp.sum(\n",
    "                dim='rgi_id',\n",
    "                skipna=True,\n",
    "                keep_attrs=True,\n",
    "            ).mean(\n",
    "                dim='GCM',\n",
    "                skipna=True,\n",
    "                keep_attrs=True,\n",
    "            ).sel(SSP='ssp585').area.plot(label='spinup')# to 2100\n",
    "\n",
    "plt.scatter(ref_date, ref_area, c='red')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3eec3fe",
   "metadata": {},
   "source": [
    "### Repulicate extremes upon QDM (cool scalor: 1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944cfc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate future forcing\n",
    "workflow.execute_entity_task(\n",
    "                create_scieno.simulate_future_extremes_QDM,\n",
    "                gdirs,\n",
    "                start_year=1950,\n",
    "                end_year=2100,\n",
    "                future_start_year=2025,\n",
    "                future_cooling_factor=1.2,\n",
    "                output_filesuffix='_repu_his_extremes_QDM_cool',\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c71de0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run projections for each CMIP6 scenario and model\n",
    "print(\"Running CMIP6 scenario projections (2025-2100)...\")\n",
    "\n",
    "projection_results = []\n",
    "\n",
    "for ssp in cmip6_scenarios:\n",
    "    print(f\"\\nProcessing scenario: {ssp}\")\n",
    "    \n",
    "    for gcm in cmip6_models:\n",
    "        print(f\"  - Model: {gcm}\")\n",
    "        \n",
    "        # Define file suffix for this model-scenario combination\n",
    "        raw_filesuffix = f'_{gcm}_{ssp}'\n",
    "        extreme_filesuffix = f'_{gcm}_{ssp}_repu_his_extremes_QDM_cool'\n",
    "        projection_id = f'_proj_{gcm}_{ssp}_repu_his_extremes_QDM_cool_{start_idx}-{end_idx-1}'\n",
    "            \n",
    "        try:\n",
    "            # Run projection simulation\n",
    "            workflow.execute_entity_task(\n",
    "                tasks.run_with_hydro, gdirs,\n",
    "                run_task=tasks.run_from_climate_data,\n",
    "                climate_filename='gcm_data',\n",
    "                climate_input_filesuffix=extreme_filesuffix,\n",
    "                fixed_geometry_spinup_yr=2000,\n",
    "                ref_area_from_y0=True,\n",
    "                output_filesuffix=projection_id,\n",
    "                store_monthly_hydro=False\n",
    "            )\n",
    "            \n",
    "            path = cfg.PATHS['working_dir'] + '/per_glacier'\n",
    "            gdf_sel2 = []\n",
    "            for root, dirs, files in os.walk(path):\n",
    "                # Path structure: per_glacier/rgi6/XX/rgi_id/inversion_flowlines.pkl\n",
    "                parts = root.replace(path, '').strip(os.sep).split(os.sep)\n",
    "                if len(parts) == 3 and parts[2].startswith('RGI'):  # RGI ID level\n",
    "                    result_file = os.path.join(root, 'model_diagnostics'+projection_id+'.nc')\n",
    "                    if not os.path.exists(result_file):\n",
    "                        gdf_sel2.append(parts[2])\n",
    "            if len(gdf_sel2) > 0:\n",
    "                gdirs[:] = [gdir for gdir in gdirs if gdir.rgi_id not in gdf_sel2]\n",
    "                print(f\"        Remaining glaciers: {len(gdirs)}\")\n",
    "                        \n",
    "            # Compile results for this model-scenario\n",
    "            ds_projection = utils.compile_run_output(gdirs, input_filesuffix=projection_id, path=False)\n",
    "            ds_projection = ds_projection.assign_coords(GCM=gcm, SSP=ssp)\n",
    "            ds_projection = ds_projection.expand_dims(['GCM', 'SSP'])\n",
    "\n",
    "            projection_results.append(ds_projection)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    Error running projection for {gcm} {ssp}: {e}\")\n",
    "            continue\n",
    "\n",
    "ds_all_projections = xr.combine_by_coords(projection_results, fill_value=np.nan, combine_attrs='override')\n",
    "ds_all_projections = ds_all_projections.sortby('rgi_id')\n",
    "output_path = cfg.PATHS['working_dir'] + f'/run_output_cmip6_repu_his_extremes_QDM_cool_{start_idx}-{end_idx-1}.nc'\n",
    "ds_all_projections.to_netcdf(output_path)\n",
    "\n",
    "print(\"\\nCMIP6 scenario projections complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cb2188",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_all_projections = xr.open_dataset(cfg.PATHS['working_dir'] + f'/run_output_cmip6_repu_his_extremes_QDM_cool_{start_idx}-{end_idx-1}.nc')\n",
    "\n",
    "ds_all_projections.sum(\n",
    "    dim='rgi_id',\n",
    "    skipna=True,\n",
    "    keep_attrs=True,\n",
    ").sel(SSP='ssp585').sel(GCM='CAMS-CSM1-0').volume.plot()# to 2100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38db4886",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_all_projections.sum(\n",
    "                dim='rgi_id',\n",
    "                skipna=True,\n",
    "                keep_attrs=True,\n",
    "            ).mean(\n",
    "                dim='GCM',\n",
    "                skipna=True,\n",
    "                keep_attrs=True,\n",
    "            ).sel(SSP='ssp585').volume.plot()# to 2100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44fffb3",
   "metadata": {},
   "source": [
    "### Repulicate extremes upon QM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22373105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate future forcing\n",
    "workflow.execute_entity_task(\n",
    "                create_scieno.simulate_future_extremes_Detrend_QM,\n",
    "                gdirs,\n",
    "                start_year=1950,\n",
    "                end_year=2100,\n",
    "                future_start_year=2025,\n",
    "                future_cooling_factor=None,\n",
    "                output_filesuffix='_repu_his_extremes_QM',\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24da720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run projections for each CMIP6 scenario and model\n",
    "print(\"Running CMIP6 scenario projections (2025-2100)...\")\n",
    "\n",
    "projection_results = []\n",
    "\n",
    "for ssp in cmip6_scenarios:\n",
    "    print(f\"\\nProcessing scenario: {ssp}\")\n",
    "    \n",
    "    for gcm in cmip6_models:\n",
    "        print(f\"  - Model: {gcm}\")\n",
    "        \n",
    "        # Define file suffix for this model-scenario combination\n",
    "        raw_filesuffix = f'_{gcm}_{ssp}'\n",
    "        extreme_filesuffix = f'_{gcm}_{ssp}_repu_his_extremes_QM'\n",
    "        projection_id = f'_proj_{gcm}_{ssp}_repu_his_extremes_QM_{start_idx}-{end_idx-1}'\n",
    "            \n",
    "        try:\n",
    "            # Run projection simulation\n",
    "            workflow.execute_entity_task(\n",
    "                tasks.run_with_hydro, gdirs,\n",
    "                run_task=tasks.run_from_climate_data,\n",
    "                climate_filename='gcm_data',\n",
    "                climate_input_filesuffix=extreme_filesuffix,\n",
    "                fixed_geometry_spinup_yr=2000,\n",
    "                ref_area_from_y0=True,\n",
    "                output_filesuffix=projection_id,\n",
    "                store_monthly_hydro=False\n",
    "            )\n",
    "            \n",
    "            path = cfg.PATHS['working_dir'] + '/per_glacier'\n",
    "            gdf_sel2 = []\n",
    "            for root, dirs, files in os.walk(path):\n",
    "                # Path structure: per_glacier/rgi6/XX/rgi_id/inversion_flowlines.pkl\n",
    "                parts = root.replace(path, '').strip(os.sep).split(os.sep)\n",
    "                if len(parts) == 3 and parts[2].startswith('RGI'):  # RGI ID level\n",
    "                    result_file = os.path.join(root, 'model_diagnostics'+projection_id+'.nc')\n",
    "                    if not os.path.exists(result_file):\n",
    "                        gdf_sel2.append(parts[2])\n",
    "            if len(gdf_sel2) > 0:\n",
    "                gdirs[:] = [gdir for gdir in gdirs if gdir.rgi_id not in gdf_sel2]\n",
    "                print(f\"        Remaining glaciers: {len(gdirs)}\")\n",
    "                        \n",
    "            # Compile results for this model-scenario\n",
    "            ds_projection = utils.compile_run_output(gdirs, input_filesuffix=projection_id, path=False)\n",
    "            ds_projection = ds_projection.assign_coords(GCM=gcm, SSP=ssp)\n",
    "            ds_projection = ds_projection.expand_dims(['GCM', 'SSP'])\n",
    "\n",
    "            projection_results.append(ds_projection)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    Error running projection for {gcm} {ssp}: {e}\")\n",
    "            continue\n",
    "\n",
    "ds_all_projections = xr.combine_by_coords(projection_results, fill_value=np.nan, combine_attrs='override')\n",
    "ds_all_projections = ds_all_projections.sortby('rgi_id')\n",
    "output_path = cfg.PATHS['working_dir'] + f'/run_output_cmip6_repu_his_extremes_QM_{start_idx}-{end_idx-1}.nc'\n",
    "ds_all_projections.to_netcdf(output_path)\n",
    "\n",
    "print(\"\\nCMIP6 scenario projections complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e29ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_results_sp = []\n",
    "\n",
    "for ssp in cmip6_scenarios:\n",
    "    print(f\"\\nProcessing scenario: {ssp}\")\n",
    "    \n",
    "    for gcm in cmip6_models:\n",
    "        print(f\"  - Model: {gcm}\")\n",
    "        # raw_filesuffix = f'_{gcm}_{ssp}'\n",
    "        extreme_filesuffix = f'_{gcm}_{ssp}_repu_his_extremes_QM'\n",
    "        projection_id = f'_proj_{gcm}_{ssp}_repu_his_extremes_QM_spinup_{start_idx}-{end_idx-1}'\n",
    "            \n",
    "        try:\n",
    "            # Run projection simulation\n",
    "            workflow.execute_entity_task(\n",
    "                tasks.run_with_hydro, gdirs,\n",
    "                run_task=tasks.run_from_climate_data,\n",
    "                climate_filename='gcm_data',\n",
    "                climate_input_filesuffix=extreme_filesuffix,\n",
    "                init_model_filesuffix=spinup_filesuffix,\n",
    "                init_model_yr=2000,\n",
    "                ref_area_from_y0=True,\n",
    "                output_filesuffix=projection_id,\n",
    "                store_monthly_hydro=False\n",
    "            )\n",
    "            \n",
    "            path = cfg.PATHS['working_dir'] + '/per_glacier'\n",
    "            gdf_sel2 = []\n",
    "            for root, dirs, files in os.walk(path):\n",
    "                # Path structure: per_glacier/rgi6/XX/rgi_id/inversion_flowlines.pkl\n",
    "                parts = root.replace(path, '').strip(os.sep).split(os.sep)\n",
    "                if len(parts) == 3 and parts[2].startswith('RGI'):  # RGI ID level\n",
    "                    result_file = os.path.join(root, 'model_diagnostics'+projection_id+'.nc')\n",
    "                    if not os.path.exists(result_file):\n",
    "                        gdf_sel2.append(parts[2])\n",
    "            if len(gdf_sel2) > 0:\n",
    "                gdirs[:] = [gdir for gdir in gdirs if gdir.rgi_id not in gdf_sel2]\n",
    "                print(f\"        Remaining glaciers: {len(gdirs)}\")\n",
    "                        \n",
    "            # Compile results for this model-scenario\n",
    "            ds_projection = utils.compile_run_output(gdirs, input_filesuffix=projection_id, path=False)\n",
    "            ds_projection = ds_projection.assign_coords(GCM=gcm, SSP=ssp)\n",
    "            ds_projection = ds_projection.expand_dims(['GCM', 'SSP'])\n",
    "\n",
    "            projection_results_sp.append(ds_projection)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"    Error running projection for {gcm} {ssp}: {e}\")\n",
    "            continue\n",
    "\n",
    "ds_all_projections = xr.combine_by_coords(projection_results_sp, fill_value=np.nan, combine_attrs='override')\n",
    "ds_all_projections = ds_all_projections.sortby('rgi_id')\n",
    "output_path = cfg.PATHS['working_dir'] + f'/run_output_cmip6_repu_his_extremes_QM_spinup_{start_idx}-{end_idx-1}.nc'\n",
    "ds_all_projections.to_netcdf(output_path)\n",
    "\n",
    "print(\"\\nCMIP6 scenario projections complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5b2e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_all_projections = xr.open_dataset(cfg.PATHS['working_dir'] + f'/run_output_cmip6_repu_his_extremes_QM_{start_idx}-{end_idx-1}.nc')\n",
    "\n",
    "ds_all_projections.sum(\n",
    "    dim='rgi_id',\n",
    "    skipna=True,\n",
    "    keep_attrs=True,\n",
    ").sel(SSP='ssp585').sel(GCM='CAMS-CSM1-0').volume.plot()# to 2100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eebbdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_all_projections.sum(\n",
    "                dim='rgi_id',\n",
    "                skipna=True,\n",
    "                keep_attrs=True,\n",
    "            ).mean(\n",
    "                dim='GCM',\n",
    "                skipna=True,\n",
    "                keep_attrs=True,\n",
    "            ).sel(SSP='ssp585').volume.plot()# to 2100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2f80d3",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12d1dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_normal= xr.open_dataset(cfg.PATHS['working_dir'] + f'/run_output_cmip6_normal_{start_idx}-{end_idx-1}.nc')\n",
    "ds_extreme1= xr.open_dataset(cfg.PATHS['working_dir'] + f'/run_output_cmip6_repu_his_extremes_QDM_{start_idx}-{end_idx-1}.nc')\n",
    "# ds_extreme2= xr.open_dataset(cfg.PATHS['working_dir'] + f'/run_output_cmip6_repu_his_extremes_QDM_cool_{start_idx}-{end_idx-1}.nc')\n",
    "ds_extreme3= xr.open_dataset(cfg.PATHS['working_dir'] + f'/run_output_cmip6_repu_his_extremes_QM_{start_idx}-{end_idx-1}.nc')\n",
    "\n",
    "ds_normal_sp= xr.open_dataset(cfg.PATHS['working_dir'] + f'/run_output_cmip6_normal_spinup_{start_idx}-{end_idx-1}.nc')\n",
    "ds_extreme1_sp= xr.open_dataset(cfg.PATHS['working_dir'] + f'/run_output_cmip6_repu_his_extremes_QDM_spinup_{start_idx}-{end_idx-1}.nc')\n",
    "ds_extreme3_sp= xr.open_dataset(cfg.PATHS['working_dir'] + f'/run_output_cmip6_repu_his_extremes_QM_spinup_{start_idx}-{end_idx-1}.nc')\n",
    "\n",
    "ds_collect = [ds_normal, ds_extreme1, ds_extreme3,\n",
    "              ds_normal_sp, ds_extreme1_sp, ds_extreme3_sp]\n",
    "labels = ['CMIP6', 'CMIP6 + QDM extremes', 'CMIP6 + QM extremes', \n",
    "          'CMIP6(spinup)', 'CMIP6 + QDM extremes(spinup)', 'CMIP6 + QM extremes(spinup)']\n",
    "colors = [plt.get_cmap('Accent')(0), plt.get_cmap('Accent')(5), plt.get_cmap('Accent')(6),\n",
    "          plt.get_cmap('Accent')(0), plt.get_cmap('Accent')(5), plt.get_cmap('Accent')(6)]\n",
    "styles = ['-', '-', '-', '--', '--', '--']\n",
    "widths = [2, 2, 2, 2, 2, 2]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, ssp in enumerate(cmip6_scenarios):\n",
    "    for ds_i, ds_item in enumerate(ds_collect):\n",
    "        mean_temp = ds_item.sum(\n",
    "                        dim='rgi_id',\n",
    "                        skipna=True,\n",
    "                    ).mean(\n",
    "                        dim='GCM',\n",
    "                        skipna=True,\n",
    "                    ).sel(SSP=ssp)\n",
    "        axes[i].plot(mean_temp.time.values, mean_temp.volume.values, linewidth=widths[ds_i], color=colors[ds_i], \n",
    "                        label=labels[ds_i], linestyle=styles[ds_i], alpha=0.7)\n",
    "        axes[i].set_xlabel('Year', fontsize=12)\n",
    "        axes[i].set_ylabel(f'Volume (m$^3$)', fontsize=12)\n",
    "        axes[i].set_title(ssp.upper(), fontsize=14)\n",
    "\n",
    "        axes[i].tick_params(labelsize=10)\n",
    "        axes[i].set_xlim([2000, 2100])\n",
    "\n",
    "    if i == 3:\n",
    "        axes[i].legend(fontsize=12, loc='best')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55385f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, ssp in enumerate(cmip6_scenarios):\n",
    "    for ds_i, ds_item in enumerate(ds_collect):\n",
    "        mean_temp = ds_item.sum(\n",
    "                        dim='rgi_id',\n",
    "                        skipna=True,\n",
    "                    ).mean(\n",
    "                        dim='GCM',\n",
    "                        skipna=True,\n",
    "                    ).sel(SSP=ssp)\n",
    "        axes[i].plot(mean_temp.time.values, mean_temp.area.values, linewidth=widths[ds_i], color=colors[ds_i], \n",
    "                        label=labels[ds_i], linestyle=styles[ds_i], alpha=0.7)\n",
    "        axes[i].set_xlabel('Year', fontsize=12)\n",
    "        axes[i].set_ylabel(f'Area (m$^2$)', fontsize=12)\n",
    "        axes[i].set_title(ssp.upper(), fontsize=14)\n",
    "\n",
    "        axes[i].tick_params(labelsize=10)\n",
    "        axes[i].set_xlim([2000, 2100])\n",
    "\n",
    "    if i == 3:\n",
    "        axes[i].legend(fontsize=12, loc='best')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48085b15",
   "metadata": {},
   "source": [
    "# Historical modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d894fde",
   "metadata": {},
   "source": [
    "### Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f2a48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run historical simulation\n",
    "file_id = f'_hist_spinup_{start_idx}-{end_idx-1}'\n",
    "workflow.execute_entity_task(\n",
    "                            tasks.run_with_hydro, gdirs,\n",
    "                            run_task=tasks.run_from_climate_data,\n",
    "                            climate_filename='climate_historical',\n",
    "                            init_model_filesuffix=spinup_filesuffix,\n",
    "                            init_model_yr=2000,\n",
    "                            ref_area_from_y0=True,\n",
    "                            output_filesuffix=file_id,\n",
    "                            store_monthly_hydro=False\n",
    "                            )\n",
    "\n",
    "path = cfg.PATHS['working_dir'] + '/per_glacier'\n",
    "gdf_sel2 = []\n",
    "if os.path.exists(path):\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        # Path structure: per_glacier/rgi6/XX/rgi_id/inversion_flowlines.pkl\n",
    "        parts = root.replace(path, '').strip(os.sep).split(os.sep)\n",
    "        if len(parts) == 3 and parts[2].startswith('RGI'):  # RGI ID level\n",
    "            result_file = os.path.join(root, f'model_diagnostics{file_id}.nc')\n",
    "            if not os.path.exists(result_file):\n",
    "                gdf_sel2.append(parts[2])\n",
    "if len(gdf_sel2) > 0:\n",
    "    print(f\"Removing {len(gdf_sel2)} incomplete glaciers...\")\n",
    "    gdirs[:] = [gdir for gdir in gdirs if gdir.rgi_id not in gdf_sel2]\n",
    "    print(f\"Remaining glaciers: {len(gdirs)}\")\n",
    "\n",
    "# Compile output\n",
    "ds_hist = utils.compile_run_output(gdirs, input_filesuffix=file_id)\n",
    "print(\"Historical simulation complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a97766",
   "metadata": {},
   "source": [
    "### Remove warming (2015-2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6ba3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_filesuffix = '_warm_rm'\n",
    "\n",
    "workflow.execute_entity_task(\n",
    "    create_scieno.remove_warming_trend, gdirs,\n",
    "    ys_start=2015,\n",
    "    ys_end=2024,\n",
    "    output_filesuffix=climate_filesuffix      \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d530ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run historical simulation\n",
    "file_id = f'_hist_warm_rm_spinup_{start_idx}-{end_idx-1}'\n",
    "workflow.execute_entity_task(\n",
    "                            tasks.run_with_hydro, gdirs,\n",
    "                            run_task=tasks.run_from_climate_data,\n",
    "                            climate_filename='climate_historical',\n",
    "                            climate_input_filesuffix=climate_filesuffix,\n",
    "                            init_model_filesuffix=spinup_filesuffix,\n",
    "                            init_model_yr=2000,\n",
    "                            ref_area_from_y0=True,\n",
    "                            output_filesuffix=file_id,\n",
    "                            store_monthly_hydro=False\n",
    "                            )\n",
    "\n",
    "path = cfg.PATHS['working_dir'] + '/per_glacier'\n",
    "gdf_sel2 = []\n",
    "if os.path.exists(path):\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        # Path structure: per_glacier/rgi6/XX/rgi_id/inversion_flowlines.pkl\n",
    "        parts = root.replace(path, '').strip(os.sep).split(os.sep)\n",
    "        if len(parts) == 3 and parts[2].startswith('RGI'):  # RGI ID level\n",
    "            result_file = os.path.join(root, f'model_diagnostics{file_id}.nc')\n",
    "            if not os.path.exists(result_file):\n",
    "                gdf_sel2.append(parts[2])\n",
    "if len(gdf_sel2) > 0:\n",
    "    print(f\"Removing {len(gdf_sel2)} incomplete glaciers...\")\n",
    "    gdirs[:] = [gdir for gdir in gdirs if gdir.rgi_id not in gdf_sel2]\n",
    "    print(f\"Remaining glaciers: {len(gdirs)}\")\n",
    "\n",
    "# Compile output\n",
    "ds_hist = utils.compile_run_output(gdirs, input_filesuffix=file_id)\n",
    "print(\"Historical simulation complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acef308d",
   "metadata": {},
   "source": [
    "### Add warming (2005-2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45218d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_filesuffix = '_warm_add'\n",
    "\n",
    "workflow.execute_entity_task(\n",
    "    create_scieno.add_warming_trend, gdirs,\n",
    "    ys_start=2005,\n",
    "    ys_end=2014,\n",
    "    output_filesuffix=climate_filesuffix      \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11325bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run historical simulation\n",
    "file_id = f'_hist_warm_add_spinup_{start_idx}-{end_idx-1}'\n",
    "workflow.execute_entity_task(\n",
    "                            tasks.run_with_hydro, gdirs,\n",
    "                            run_task=tasks.run_from_climate_data,\n",
    "                            climate_filename='climate_historical',\n",
    "                            climate_input_filesuffix=climate_filesuffix,\n",
    "                            init_model_filesuffix=spinup_filesuffix,\n",
    "                            init_model_yr=2000,\n",
    "                            ref_area_from_y0=True,\n",
    "                            output_filesuffix=file_id,\n",
    "                            store_monthly_hydro=False\n",
    "                            )\n",
    "\n",
    "path = cfg.PATHS['working_dir'] + '/per_glacier'\n",
    "gdf_sel2 = []\n",
    "if os.path.exists(path):\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        # Path structure: per_glacier/rgi6/XX/rgi_id/inversion_flowlines.pkl\n",
    "        parts = root.replace(path, '').strip(os.sep).split(os.sep)\n",
    "        if len(parts) == 3 and parts[2].startswith('RGI'):  # RGI ID level\n",
    "            result_file = os.path.join(root, f'model_diagnostics{file_id}.nc')\n",
    "            if not os.path.exists(result_file):\n",
    "                gdf_sel2.append(parts[2])\n",
    "if len(gdf_sel2) > 0:\n",
    "    print(f\"Removing {len(gdf_sel2)} incomplete glaciers...\")\n",
    "    gdirs[:] = [gdir for gdir in gdirs if gdir.rgi_id not in gdf_sel2]\n",
    "    print(f\"Remaining glaciers: {len(gdirs)}\")\n",
    "\n",
    "# Compile output\n",
    "ds_hist = utils.compile_run_output(gdirs, input_filesuffix=file_id)\n",
    "print(\"Historical simulation complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe41e8a",
   "metadata": {},
   "source": [
    "### PLot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83afa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_normal_sp= xr.open_dataset(cfg.PATHS['working_dir'] + f'/run_output_hist_spinup_{start_idx}-{end_idx-1}.nc')\n",
    "ds_extreme1_sp= xr.open_dataset(cfg.PATHS['working_dir'] + f'/run_output_hist_warm_rm_spinup_{start_idx}-{end_idx-1}.nc')\n",
    "ds_extreme3_sp= xr.open_dataset(cfg.PATHS['working_dir'] + f'/run_output_hist_warm_add_spinup_{start_idx}-{end_idx-1}.nc')\n",
    "\n",
    "ds_collect = [ds_normal_sp, ds_extreme1_sp, ds_extreme3_sp]\n",
    "labels = ['ERA5(spinup)', 'ERA5 + 2015-2024 warming removed(spinup)', 'ERA5 + 2005-2014 warming added (spinup)']\n",
    "colors = [plt.get_cmap('Accent')(5), plt.get_cmap('Accent')(0), plt.get_cmap('Accent')(6)]\n",
    "styles = ['-', '-', '-']\n",
    "widths = [2, 2, 2]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for ds_i, ds_item in enumerate(ds_collect):\n",
    "    mean_temp = ds_item.sum(\n",
    "                    dim='rgi_id',\n",
    "                    skipna=True,\n",
    "                )\n",
    "    ax.plot(mean_temp.time.values, mean_temp.volume.values, linewidth=widths[ds_i], color=colors[ds_i], \n",
    "                    label=labels[ds_i], linestyle=styles[ds_i], alpha=0.7)\n",
    "    ax.set_xlabel('Year', fontsize=12)\n",
    "    ax.set_ylabel(f'Volume (m$^3$)', fontsize=12)\n",
    "\n",
    "    ax.tick_params(labelsize=10)\n",
    "    ax.set_xlim([2000, 2026])\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733efa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for ds_i, ds_item in enumerate(ds_collect):\n",
    "    mean_temp = ds_item.sum(\n",
    "                    dim='rgi_id',\n",
    "                    skipna=True,\n",
    "                )\n",
    "    ax.plot(mean_temp.time.values, mean_temp.area.values, linewidth=widths[ds_i], color=colors[ds_i], \n",
    "                    label=labels[ds_i], linestyle=styles[ds_i], alpha=0.7)\n",
    "    ax.set_xlabel('Year', fontsize=12)\n",
    "    ax.set_ylabel(f'Area (m$^2$)', fontsize=12)\n",
    "\n",
    "    ax.tick_params(labelsize=10)\n",
    "    ax.set_xlim([2000, 2026])\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a595ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oggm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
