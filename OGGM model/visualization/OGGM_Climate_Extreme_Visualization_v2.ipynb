{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OGGM冰川模拟结果分析与可视化\n",
    "\n",
    "## 基于OGGM模型的气候极端事件对冰川质量平衡影响分析\n",
    "\n",
    "**功能概述：**\n",
    "- 读取OGGM冰川模拟结果（NetCDF格式）\n",
    "- 与测高观测数据（ICESat/ICESat-2/CryoSat-2）进行对比验证\n",
    "- 分析不同气候极端情景下的冰川响应\n",
    "- 评估未来气候投影对冰川的影响\n",
    "- 生成多种可视化图表\n",
    "\n",
    "**数据来源：**\n",
    "- OGGM模型输出（run_output_hist_hydro系列）\n",
    "- ICESat/ICESat-2/CryoSat-2测高观测\n",
    "- GRACE TWS数据\n",
    "- CMIP6气候投影数据\n",
    "\n",
    "**区域划分说明：**\n",
    "- 1个HMA整体区域\n",
    "- 15个GTN分区\n",
    "- 22个HMA22子区域\n",
    "- 1个TP（青藏高原）区域\n",
    "- 496个网格单元"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 导入必要的库和设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import ConnectionPatch\n",
    "from matplotlib.ticker import AutoMinorLocator, MultipleLocator, FuncFormatter\n",
    "from matplotlib import cm\n",
    "import matplotlib.colors as mcolors\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.robust.robust_linear_model import RLM\n",
    "\n",
    "from mpl_toolkits.axes_grid1.inset_locator import mark_inset, inset_axes\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from datetime import datetime as dt\n",
    "import time\n",
    "import geopandas as gpd\n",
    "import os\n",
    "import warnings\n",
    "import shapely.geometry as shpg\n",
    "import xarray as xr\n",
    "from hampel import hampel\n",
    "from math import ceil, sqrt\n",
    "from scipy import stats\n",
    "\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "from tqdm.notebook import tqdm\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeat\n",
    "import cartopy.io.img_tiles as cimgt\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set(style=\"ticks\")\n",
    "\n",
    "# 设置中文字体和负号显示\n",
    "plt.rcParams['font.family'] = 'Arial'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "plt.rcParams['pdf.fonttype'] = 42 \n",
    "plt.rcParams['ps.fonttype'] = 42 \n",
    "\n",
    "print(\"✓ 库导入成功\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 工具函数定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toYearFraction(date):\n",
    "    \"\"\"将日期转换为年份小数形式\"\"\"\n",
    "    def since_epoch(date):\n",
    "        return time.mktime(date.timetuple())\n",
    "    \n",
    "    year = date.year\n",
    "    start_of_this_year = dt(year=year, month=1, day=1)\n",
    "    start_of_next_year = dt(year=year+1, month=1, day=1)\n",
    "    \n",
    "    year_elapsed = since_epoch(date) - since_epoch(start_of_this_year)\n",
    "    year_duration = since_epoch(start_of_next_year) - since_epoch(start_of_this_year)\n",
    "    fraction = year_elapsed / year_duration\n",
    "    \n",
    "    return date.year + fraction\n",
    "\n",
    "\n",
    "def calc_nse(y_obs, y_sim):\n",
    "    \"\"\"计算Nash-Sutcliffe效率系数\"\"\"\n",
    "    y_obs_mean = y_obs.mean()\n",
    "    sum1 = ((y_obs - y_sim) ** 2).sum()\n",
    "    sum2 = ((y_obs - y_obs_mean) ** 2).sum()\n",
    "    nse = 1 - sum1 / sum2 if sum2 != 0 else np.nan\n",
    "    return nse\n",
    "\n",
    "\n",
    "def calc_r2(y_obs, y_sim):\n",
    "    \"\"\"计算相关系数（R²）\"\"\"\n",
    "    y_obs_mean = y_obs.mean()\n",
    "    y_sim_mean = y_sim.mean()\n",
    "    \n",
    "    numerator = ((y_sim - y_sim_mean) * (y_obs - y_obs_mean)).sum()\n",
    "    denominator = np.sqrt(((y_sim - y_sim_mean) ** 2).sum() * ((y_obs - y_obs_mean) ** 2).sum())\n",
    "    \n",
    "    r2 = (numerator / denominator) ** 2 if denominator != 0 else np.nan\n",
    "    return r2\n",
    "\n",
    "\n",
    "def calc_rmse(y_obs, y_sim):\n",
    "    \"\"\"计算均方根误差\"\"\"\n",
    "    rmse = np.sqrt(((y_sim - y_obs) ** 2).mean())\n",
    "    return rmse\n",
    "\n",
    "\n",
    "def calc_pbias(y_obs, y_sim):\n",
    "    \"\"\"计算百分比偏差\"\"\"\n",
    "    sum1 = (y_sim - y_obs).sum()\n",
    "    sum2 = y_obs.sum()\n",
    "    pbias = 100 * (sum1 / sum2) if sum2 != 0 else np.nan\n",
    "    return pbias\n",
    "\n",
    "\n",
    "def get_file_path_list(path, filetype):\n",
    "    \"\"\"递归获取指定路径下所有指定类型的文件\"\"\"\n",
    "    path_list = []\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith(filetype):\n",
    "                path_list.append(os.path.join(root, file))\n",
    "    return path_list\n",
    "\n",
    "\n",
    "def calculate_smb_from_volume(ds):\n",
    "    \"\"\"从体积数据计算比质量平衡率（SMB）\n",
    "    \n",
    "    参数:\n",
    "        ds: xarray Dataset, 包含volume和area的OGGM输出\n",
    "    返回:\n",
    "        smb: numpy array, 比质量平衡率 (m w.e./yr)\n",
    "    \"\"\"\n",
    "    smb = ((ds.volume.values[1:] - ds.volume.values[:-1]) / ds.area.values[1:]) * 0.9\n",
    "    smb2 = ((ds.volume.values[1:] - ds.volume.values[:-1]) / ds.area.values[:-1]) * 0.9\n",
    "\n",
    "    smb[np.isinf(smb)] = smb2[np.isinf(smb)]\n",
    "    return smb\n",
    "\n",
    "\n",
    "def calculate_elevation_change(smb):\n",
    "    \"\"\"从质量平衡计算累积高程变化\n",
    "    Input:\n",
    "        smb: N-1年质量平衡变化率， m w.e./yr\n",
    "    Return:\n",
    "        accumulated glacier elevation change for N-1 years, m, array\n",
    "    \n",
    "    \"\"\"\n",
    "    ele = np.cumsum(smb / 0.9)\n",
    "    return ele\n",
    "\n",
    "\n",
    "print(\"✓ 工具函数定义完成\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 数据路径配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置数据路径（请根据实际情况修改）\n",
    "CONFIG = {\n",
    "    'oggm_dir': r'E:\\revised_NCC_data\\OGGM model\\subregion_modeling\\modeling_results_v2\\[1]HMA',\n",
    "    'icesat_path': r'E:\\revised_NCC_data\\ICESat_result\\ICESat_seasonal_elevation_results.xlsx',\n",
    "    'icesat2_path': r'E:\\revised_NCC_data\\ICESat2_result\\ICESat2_seasonal_elevation_results.xlsx',\n",
    "    'cryosat2_path': r'E:\\revised_NCC_data\\CryoSat2_result\\CryoSat2_seasonal_elevation_results.xlsx',\n",
    "    'grace_path': r'E:\\HMA\\GRACE_RL06\\TWS_HMA_SSA.xlsx',\n",
    "    'icesat_year_path': r'E:\\revised_NCC_data\\ICESat_result\\ICESat_yearly_elevation_results.xlsx',\n",
    "    'icesat2_year_path': r'E:\\revised_NCC_data\\ICESat2_result\\ICESat2_yearly_elevation_results.xlsx',\n",
    "    'cryosat2_year_path': r'E:\\revised_NCC_data\\CryoSat2_result\\CryoSat2_yearly_elevation_results.xlsx',\n",
    "    'altimetry_config_path': r'E:\\HMA\\测高结果20231109.xlsx',\n",
    "    'hma_boundary': r'E:\\HMA_subregion\\regions_hma_v03_zheng\\boundary_mountain_regions_hma_v3_zheng_20200601.shp',\n",
    "    'tp_boundary': r'E:\\HMA_subregion\\TP_Boundary\\TP_boundary.shp',\n",
    "    'rgi_glacier': r'E:\\HMA\\Hugonnet\\per_glacier\\rgi_hma.shp',\n",
    "    'output_dir': r'E:\\revised_NCC_data\\OGGM model\\figure_plot'\n",
    "}\n",
    "\n",
    "# 获取OGGM数据目录列表\n",
    "path = CONFIG['oggm_dir']\n",
    "path_list = get_file_path_list(path, '.nc') if os.path.exists(path) else []\n",
    "os.makedirs(CONFIG['output_dir'], exist_ok=True)\n",
    "\n",
    "print(f\"✓ 数据路径配置完成\")\n",
    "print(f\"  找到 {len(path_list)} 个情景结果\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 加载观测数据和边界数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1季节尺度数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载测高观测数据\n",
    "print(\"正在加载测高观测数据...\")\n",
    "\n",
    "# ICESat数据\n",
    "icesat = pd.read_excel(CONFIG['icesat_path'], sheet_name='campaign_value', header=0, usecols='C:AQ')\n",
    "icesat_uncert = pd.read_excel(CONFIG['icesat_path'], sheet_name='campaign_uncert', header=0, usecols='C:AQ')\n",
    "icesat_num = pd.read_excel(CONFIG['icesat_path'], sheet_name='campaign_count', header=0, usecols='C:AQ')\n",
    "\n",
    "# ICESat-2数据\n",
    "icesat2 = pd.read_excel(CONFIG['icesat2_path'], sheet_name='month_value', header=0, usecols='A:AO')#0代表NaN\n",
    "icesat2_uncert = pd.read_excel(CONFIG['icesat2_path'], sheet_name='month_uncert', header=0, usecols='A:AO')\n",
    "icesat2_num = pd.read_excel(CONFIG['icesat2_path'], sheet_name='month_count', header=0, usecols='A:AO')\n",
    "\n",
    "# CryoSat-2数据\n",
    "cryosat2 = pd.read_excel(CONFIG['cryosat2_path'], sheet_name='month_value', header=0, usecols='A:AO')\n",
    "cryosat2_uncert = pd.read_excel(CONFIG['cryosat2_path'], sheet_name='month_uncert', header=0, usecols='A:AO')\n",
    "cryosat2_num = pd.read_excel(CONFIG['cryosat2_path'], sheet_name='month_count', header=0, usecols='A:AO')\n",
    "\n",
    "# GRACE TWS数据\n",
    "tws_trend = pd.read_excel(CONFIG['grace_path'], sheet_name='tws', header=499, nrows=255, usecols='D:AQ')\n",
    "tws = pd.read_excel(CONFIG['grace_path'], sheet_name='tws', header=233, nrows=255, usecols='D:AQ')\n",
    "\n",
    "# 测高结果（面积和校正值）\n",
    "area = pd.read_excel(CONFIG['altimetry_config_path'], sheet_name='static_new', header=0, nrows=4, usecols='B:AN')\n",
    "correction = pd.read_excel(CONFIG['altimetry_config_path'], sheet_name='static_new', header=49, nrows=4, usecols='B:AN')\n",
    "\n",
    "head = area.columns.to_list()\n",
    "\n",
    "# 初始化结果数据框\n",
    "# rate = pd.DataFrame(data=np.zeros([2, len(head)]), columns=head)\n",
    "# rate1 = pd.DataFrame(data=np.zeros([2, len(head)]), columns=head)\n",
    "# rate2 = pd.DataFrame(data=np.zeros([2, len(head)]), columns=head)\n",
    "# rate3 = pd.DataFrame(data=np.zeros([2, len(head)]), columns=head)\n",
    "\n",
    "print(\"✓ 测高观测数据加载成功\")\n",
    "print(f\"  区域数量: {len(head)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 年尺度数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ICESat数据路径\n",
    "icesat_year_path = CONFIG['icesat_year_path']\n",
    "icesat_year = pd.read_excel(\n",
    "    icesat_year_path,\n",
    "    sheet_name='year_value',\n",
    "    header=0,\n",
    "    usecols='A:AO'\n",
    ")\n",
    "icesat_uncert_year = pd.read_excel(\n",
    "    icesat_year_path,\n",
    "    sheet_name='year_uncert',\n",
    "    header=0,\n",
    "    usecols='A:AO'\n",
    ")\n",
    "icesat_num_year = pd.read_excel(\n",
    "    icesat_year_path,\n",
    "    sheet_name='year_count',\n",
    "    header=0,\n",
    "    usecols='A:AO'\n",
    ")\n",
    "\n",
    "# ICESat-2数据路径\n",
    "icesat2_year_path = CONFIG['icesat2_year_path']\n",
    "icesat2_year = pd.read_excel(\n",
    "    icesat2_year_path,\n",
    "    sheet_name='year_value',\n",
    "    header=0,\n",
    "    usecols='A:AO'\n",
    ")\n",
    "icesat2_uncert_year = pd.read_excel(\n",
    "    icesat2_year_path,\n",
    "    sheet_name='year_uncert',\n",
    "    header=0,\n",
    "    usecols='A:AO'\n",
    ")\n",
    "icesat2_num_year = pd.read_excel(\n",
    "    icesat2_year_path,\n",
    "    sheet_name='year_count',\n",
    "    header=0,\n",
    "    usecols='A:AO'\n",
    ")\n",
    "\n",
    "# CryoSat-2数据路径\n",
    "cryosat2_year_path = CONFIG['cryosat2_year_path']\n",
    "cryosat2_year = pd.read_excel(\n",
    "    cryosat2_year_path,\n",
    "    sheet_name='year_value',\n",
    "    header=0,\n",
    "    usecols='A:AO'\n",
    ")\n",
    "cryosat2_uncert_year = pd.read_excel(\n",
    "    cryosat2_year_path,\n",
    "    sheet_name='year_uncert',\n",
    "    header=0,\n",
    "    usecols='A:AO'\n",
    ")\n",
    "cryosat2_num_year = pd.read_excel(\n",
    "    cryosat2_year_path,\n",
    "    sheet_name='year_count',\n",
    "    header=0,\n",
    "    usecols='A:AO'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 时间转换为年份小数 ==========\n",
    "cryosat2 = cryosat2.rename(columns={'days':'mid_day'})\n",
    "cryosat2_num = cryosat2_num.rename(columns={'days':'mid_day'})\n",
    "cryosat2_uncert = cryosat2_uncert.rename(columns={'days':'mid_day'})\n",
    "\n",
    "icesat2 = icesat2.rename(columns={'days':'mid_day'})\n",
    "icesat2_num = icesat2_num.rename(columns={'days':'mid_day'})\n",
    "icesat2_uncert = icesat2_uncert.rename(columns={'days':'mid_day'})\n",
    "\n",
    "for idx, row in cryosat2.iterrows():\n",
    "    cryosat2.loc[idx, 'mid_day'] = toYearFraction(row['date'])\n",
    "    cryosat2_num.loc[idx, 'mid_day'] = toYearFraction(row['date'])\n",
    "    cryosat2_uncert.loc[idx, 'mid_day'] = toYearFraction(row['date'])\n",
    "cryosat2['mid_day'] = cryosat2['mid_day'].astype(float)\n",
    "cryosat2_num['mid_day'] = cryosat2_num['mid_day'].astype(float)\n",
    "cryosat2_uncert['mid_day'] = cryosat2_uncert['mid_day'].astype(float)\n",
    "\n",
    "for idx, row in icesat2.iterrows():\n",
    "    icesat2.loc[idx, 'mid_day'] = toYearFraction(row['date'])\n",
    "    icesat2_num.loc[idx, 'mid_day'] = toYearFraction(row['date'])\n",
    "    icesat2_uncert.loc[idx, 'mid_day'] = toYearFraction(row['date'])\n",
    "icesat2['mid_day'] = icesat2['mid_day'].astype(float)\n",
    "icesat2_num['mid_day'] = icesat2_num['mid_day'].astype(float)\n",
    "icesat2_uncert['mid_day'] = icesat2_uncert['mid_day'].astype(float)\n",
    "\n",
    "for idx, row in icesat.iterrows():\n",
    "    icesat.loc[idx, 'mid_day'] = toYearFraction(row['date'])\n",
    "    icesat_num.loc[idx, 'mid_day'] = toYearFraction(row['date'])\n",
    "    icesat_uncert.loc[idx, 'mid_day'] = toYearFraction(row['date'])\n",
    "icesat['mid_day'] = icesat['mid_day'].astype(float)\n",
    "icesat_num['mid_day'] = icesat_num['mid_day'].astype(float)\n",
    "icesat_uncert['mid_day'] = icesat_uncert['mid_day'].astype(float)\n",
    "\n",
    "# for year data\n",
    "icesat_year_corr = icesat_year.iloc[:7, :]\n",
    "icesat_year_corr[head] = icesat_year_corr[head] + correction.loc[2, :]\n",
    "icesat_uncert_year = icesat_uncert_year.iloc[:7, :]\n",
    "icesat_num_year = icesat_num_year.iloc[:7, :]\n",
    "\n",
    "cryosat2_year_corr = cryosat2_year.iloc[:16, :]\n",
    "cryosat2_year_corr[head] = cryosat2_year_corr[head] + correction.loc[0, :]\n",
    "cryosat2_uncert_year = cryosat2_uncert_year.iloc[:16, :]\n",
    "cryosat2_num_year = cryosat2_num_year.iloc[:16, :]\n",
    "\n",
    "icesat2_year_corr = icesat2_year.iloc[:6, :]\n",
    "icesat2_uncert_year = icesat2_uncert_year.iloc[:6, :]\n",
    "icesat2_num_year = icesat2_num_year.iloc[:6, :]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 加载空间边界数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载空间边界数据\n",
    "print(\"正在加载空间边界数据...\")\n",
    "\n",
    "HMA = gpd.read_file(CONFIG['hma_boundary'])\n",
    "TP = gpd.read_file(CONFIG['tp_boundary'])\n",
    "glacier_gdf = gpd.read_file(CONFIG['rgi_glacier'])\n",
    "\n",
    "print(\"✓ 空间边界数据加载成功\")\n",
    "print(f\"  HMA子区域数: {len(HMA)}\")\n",
    "print(f\"  冰川数量: {len(glacier_gdf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 需要保证坐标系一致\n",
    "print(HMA.crs)\n",
    "print(glacier_gdf.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 数据预处理\n",
    "\n",
    "### 4.1 裁剪到子区域"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import oggm_utils \n",
    "import importlib\n",
    "importlib.reload(oggm_utils)\n",
    "from oggm_utils import aggregate_to_region\n",
    "\n",
    "worker_func = partial(aggregate_to_region, \n",
    "                    glacier_gdf=glacier_gdf, \n",
    "                    oggm_path_list=path_list, \n",
    "                    config=CONFIG)\n",
    "NUM_CORES = 10\n",
    "results = []\n",
    "\n",
    "with Pool(processes=NUM_CORES) as pool:\n",
    "    for res in tqdm(pool.imap_unordered(worker_func, HMA.iterrows()),\n",
    "                                total=len(HMA), desc='处理区域'):\n",
    "        results.append(res)\n",
    "\n",
    "for res in results:\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 按冰川面积分组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_area(glacier_gdf, area_min=None, area_max=None, output_prefix=''):\n",
    "    \"\"\"\n",
    "    按冰川面积范围筛选并处理数据\n",
    "    \n",
    "    参数:\n",
    "        area_min: float, 最小面积（km²）\n",
    "        area_max: float, 最大面积（km²）\n",
    "        output_prefix: str, 输出文件前缀\n",
    "    \"\"\"\n",
    "    gdf_sel = glacier_gdf.copy()\n",
    "    \n",
    "    if area_min is not None:\n",
    "        gdf_sel = gdf_sel.loc[gdf_sel['Area'] >= area_min]\n",
    "    \n",
    "    if area_max is not None:\n",
    "        gdf_sel = gdf_sel.loc[gdf_sel['Area'] < area_max]\n",
    "    \n",
    "    print(f\"筛选后冰川数量: {len(gdf_sel)}\")\n",
    "    \n",
    "    files = get_file_path_list(CONFIG['oggm_dir'], '.nc')\n",
    "    \n",
    "    for i, file in enumerate(files):\n",
    "        ds = xr.open_dataset(file)\n",
    "        mask = ds['rgi_id'].isin(gdf_sel['RGIId'])\n",
    "        ds_selected = ds.where(mask, drop=True)\n",
    "        \n",
    "        output_file = f\"{CONFIG['output_path']}/categorized_by_area/{output_prefix}/{os.path.basename(file)}\"\n",
    "        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "        ds_selected.to_netcdf(output_file)\n",
    "\n",
    "        print(f\"  已处理文件 {os.path.basename(file)}\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 趋势计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算特定时段趋势\n",
    "def calculate_grid_trends(ele_grid, num_grid, suffix, start_time=None, end_time=None, return_hampel=True, use_num=True):\n",
    "    \"\"\"\n",
    "    计算所有网格的ICESat-2高程变化趋势 (修改版)\n",
    "    \n",
    "    核心步骤：\n",
    "    1. 应用数量阈值过滤（<20个观测值设为NaN）\n",
    "    2. 使用Hampel滤波器去除时间序列中的异常值\n",
    "    3. 鲁棒线性回归（RLM）计算趋势、p-value和95%置信区间\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        包含每个网格趋势的DataFrame\n",
    "        - 'Slope': 斜率/趋势 (m/yr)\n",
    "        - 'Pvalue': p-value (显著性)\n",
    "        - 'CI/2': 95%置信区间半宽度 (m/yr)\n",
    "        - 'Intercept': 截距 (m)\n",
    "    \"\"\"\n",
    "    head_grid = ele_grid.columns.tolist()[2:] # 1,2,...,496\n",
    "    # 1. 应用数量阈值\n",
    "    for i in head_grid:\n",
    "        if use_num:\n",
    "            mask = (num_grid[i] < 20) | (ele_grid[i] == 0) | (pd.isna(num_grid[i]))\n",
    "        else:\n",
    "            mask = pd.isna(num_grid[i])\n",
    "        ele_grid.loc[mask, i] = np.nan\n",
    "    \n",
    "    # 初始化 results DataFrame\n",
    "    results = pd.DataFrame(\n",
    "        index=head_grid, \n",
    "        columns=['Slope', 'Pvalue', 'CI/2', 'Intercept'],\n",
    "        dtype=float  # 预先指定数据类型\n",
    "    )\n",
    "    \n",
    "    ele_grid_copy = ele_grid.copy()\n",
    "    hampel_results = ele_grid.copy()\n",
    "    hampel_results[head_grid] = np.nan\n",
    "    if start_time is not None:\n",
    "        ele_grid_copy = ele_grid_copy[(ele_grid_copy['date']>=start_time) & (ele_grid_copy['date']<end_time)]\n",
    "    \n",
    "    ele_grid_copy.sort_values(by='mid_day', inplace=True)\n",
    "        \n",
    "    for i in head_grid:\n",
    "        valid_data = ele_grid_copy[[i, 'mid_day']].dropna()\n",
    "        \n",
    "        if len(valid_data) < 10:\n",
    "            continue\n",
    "        \n",
    "        grid = valid_data.reset_index(drop=True)#连续索引\n",
    "        year = grid['mid_day']\n",
    "        dh = grid[i]\n",
    "        \n",
    "        # 2. Hampel滤波器去除异常值\n",
    "        hamp_series = hampel(dh, window_size=5, n=3, imputation=True)\n",
    "        hamp_series = hamp_series.astype(float)\n",
    "        \n",
    "        # 3. 鲁棒线性回归 (RLM)\n",
    "        X = sm.add_constant(year)\n",
    "        y = hamp_series\n",
    "        \n",
    "        rlm_model = RLM(y, X, M=sm.robust.norms.HuberT())\n",
    "        rlm_results = rlm_model.fit()\n",
    "\n",
    "        slope = rlm_results.params.iloc[1]\n",
    "        intercept = rlm_results.params.iloc[0]\n",
    "        pvalue = rlm_results.pvalues.iloc[1]\n",
    "        \n",
    "        # 计算95%置信区间\n",
    "        conf_int = rlm_results.conf_int(alpha=0.05)\n",
    "        # conf_int 是一个 DataFrame，[1] 是斜率的置信区间\n",
    "        CI_half_width = (conf_int.iloc[1, 1] - conf_int.iloc[1, 0]) / 2\n",
    "\n",
    "        results.loc[i, 'Slope'] = slope\n",
    "        results.loc[i, 'Pvalue'] = pvalue\n",
    "        results.loc[i, 'CI/2'] = CI_half_width\n",
    "        results.loc[i, 'Intercept'] = intercept\n",
    "\n",
    "    results.columns = [f'Slope_{suffix}', f'Pvalue_{suffix}', f'CI/2_{suffix}', f'Intercept_{suffix}']\n",
    "    if not return_hampel:\n",
    "        return results, None\n",
    "\n",
    "    ele_grid2 = ele_grid.sort_values(by='mid_day', inplace=False)\n",
    "    for i in head_grid:\n",
    "        valid_data = ele_grid2[[i, 'mid_day']].dropna()\n",
    "        if len(valid_data) < 10:\n",
    "            continue\n",
    "\n",
    "        grid = valid_data.reset_index(drop=True)#连续索引\n",
    "        year = grid['mid_day']\n",
    "        dh = grid[i]\n",
    "        \n",
    "        # 2. Hampel滤波器去除异常值\n",
    "        hamp_series = hampel(dh, window_size=5, n=3, imputation=True)\n",
    "        hamp_series = hamp_series.astype(float)\n",
    "        hampel_results.loc[valid_data.index, i] = hamp_series.values\n",
    "    \n",
    "    \n",
    "    return results, hampel_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icesat_corrected = icesat.copy()\n",
    "cryosat2_corrected = cryosat2.copy()\n",
    "tws_corrected = tws.copy()\n",
    "tws_trend_corrected = tws_trend.copy()\n",
    "\n",
    "for region_name in head:\n",
    "    # ========== 系统偏差校正 ==========\n",
    "    corr = correction.loc[0, region_name]   # CryoSat-2校正值\n",
    "    corr2 = correction.loc[1, region_name]  # TWS校正值\n",
    "    corr3 = correction.loc[2, region_name]  # ICESat校正值\n",
    "\n",
    "    icesat_corrected[region_name] = icesat_corrected[region_name] + corr3\n",
    "    cryosat2_corrected[region_name] = cryosat2_corrected[region_name] + corr\n",
    "    #tws_corrected[region_name] = tws_corrected[region_name] + corr2\n",
    "    #tws_trend_corrected[region_name] = tws_trend_corrected[region_name] + corr2\n",
    "\n",
    "# ========== 计算趋势 ==========\n",
    "trend03to09, hampel_icesat_corrected = calculate_grid_trends(icesat_corrected, \n",
    "                                                                icesat_num, 'ice1', \n",
    "                                                                start_time=None, end_time=None)#7 years\n",
    "trend10to18, hampel_cryosat2_corrected = calculate_grid_trends(cryosat2_corrected, \n",
    "                                                                cryosat2_num, 'cryo2', \n",
    "                                                                start_time=dt(2010,1,1), end_time=dt(2019, 1, 1))#8.5 years\n",
    "trend18to25, hampel_icesat2_corrected = calculate_grid_trends(icesat2, \n",
    "                                                                icesat2_num, 'ice2', \n",
    "                                                                start_time=dt(2018,1,1), end_time=dt(2025, 10, 1))#7 years\n",
    "# 年结果在上面已经校正了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. OGGM模拟结果与测高观测对比\n",
    "\n",
    "### 5.1 提取并对比单个子区域的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_oggm_with_altimetry(region_idx, oggm_corr_list, output_dir=None, display_legend=False):\n",
    "    \"\"\"\n",
    "    对比OGGM模拟结果与测高观测数据\n",
    "    \n",
    "    参数:\n",
    "        region_idx: int, 区域索引（对应head列表）,16-37\n",
    "        path_idx: int, OGGM数据路径索引\n",
    "        oggm_corr_list: Dataframe\n",
    "    \"\"\"\n",
    "    region_name = head[region_idx]\n",
    "    region_name_clean = region_name\n",
    "\n",
    "    if '.1' in region_name:\n",
    "        region_name_clean = region_name_clean.replace('.1', '')\n",
    "    if '/' in region_name:\n",
    "        region_name_clean = region_name_clean.replace('/', '-')\n",
    "        \n",
    "    # 加载OGGM数据\n",
    "    hist_filename = 'run_output_hist_spinup_all.nc'\n",
    "    ds1 = xr.open_dataset(os.path.join(os.path.dirname(CONFIG['oggm_dir']), f'[{region_idx+1}]{region_name_clean}', hist_filename))\\\n",
    "                .sel(time=slice(2000, 2025)).sum(dim='rgi_id', skipna=True, keep_attrs=True) \n",
    "    \n",
    "    # 计算质量平衡和高程变化\n",
    "    smb1 = calculate_smb_from_volume(ds1) # m w.s./yr, 2000-2024\n",
    "    ele1 = calculate_elevation_change(smb1) # 累计高程变化，2000-2024，2024年整年\n",
    "    \n",
    "    # 绘制时间序列\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    scalor_factor = 4\n",
    "    ax.plot(\n",
    "        hampel_icesat_corrected['date'], hampel_icesat_corrected[region_name],\n",
    "        '^-', color=plt.cm.Accent(0), linewidth=1, markersize=1.5,\n",
    "        label='ICESat'\n",
    "    )\n",
    "    ax.plot(\n",
    "        pd.to_datetime(icesat_year_corr['start_year'].astype(int).astype(str) + '-07-01'), \n",
    "        icesat_year_corr[region_name],\n",
    "        '-', color=plt.cm.Accent(4), linewidth=2,\n",
    "        label='ICESat annual mean'\n",
    "    )\n",
    "    ax.fill_between(\n",
    "        pd.to_datetime(icesat_year_corr['start_year'].astype(int).astype(str) + '-07-01'), \n",
    "        icesat_year_corr[region_name] - icesat_uncert_year[region_name]*scalor_factor,\n",
    "        icesat_year_corr[region_name] + icesat_uncert_year[region_name]*scalor_factor,\n",
    "        color='gray', alpha=0.3\n",
    "    )\n",
    "\n",
    "    ax.plot(\n",
    "        hampel_cryosat2_corrected['date'], hampel_cryosat2_corrected[region_name],\n",
    "        's-', color=plt.cm.Accent(1), linewidth=1, markersize=1.5,\n",
    "        label='CryoSat-2'\n",
    "    )\n",
    "    ax.plot(\n",
    "        pd.to_datetime(cryosat2_year_corr['start_year'].astype(int).astype(str) + '-07-01'), \n",
    "        cryosat2_year_corr[region_name],\n",
    "        '-', color=plt.cm.Accent(5), linewidth=2,\n",
    "        label='CryoSat-2 annual mean'\n",
    "    )\n",
    "    ax.fill_between(\n",
    "        pd.to_datetime(cryosat2_year_corr['start_year'].astype(int).astype(str) + '-07-01'), \n",
    "        cryosat2_year_corr[region_name] - cryosat2_uncert_year[region_name]*scalor_factor,\n",
    "        cryosat2_year_corr[region_name] + cryosat2_uncert_year[region_name]*scalor_factor,\n",
    "        color='gray', alpha=0.3\n",
    "    )\n",
    "\n",
    "    ax.plot(\n",
    "        hampel_icesat2_corrected['date'], hampel_icesat2_corrected[region_name],\n",
    "        'o-', color=plt.cm.Accent(2), linewidth=1, markersize=1.5,\n",
    "        label='ICESat-2'\n",
    "    )\n",
    "    ax.plot(\n",
    "        pd.to_datetime(icesat2_year_corr['start_year'].astype(int).astype(str) + '-07-01'), icesat2_year_corr[region_name],\n",
    "        '-', color=plt.cm.Accent(6), linewidth=2,\n",
    "        label='ICESat-2 annual mean'\n",
    "    )\n",
    "    ax.fill_between(\n",
    "        pd.to_datetime(icesat2_year_corr['start_year'].astype(int).astype(str) + '-07-01'), \n",
    "        icesat2_year_corr[region_name] - icesat2_uncert_year[region_name]*scalor_factor,\n",
    "        icesat2_year_corr[region_name] + icesat2_uncert_year[region_name]*scalor_factor,\n",
    "        color='gray', alpha=0.3\n",
    "    )\n",
    "    merged_year_series = pd.concat([icesat_year_corr.set_index('start_year')[region_name], #2003-2009\n",
    "                                     cryosat2_year_corr.set_index('start_year').loc[2010:2018, region_name], #2010-2018\n",
    "                                      icesat2_year_corr.set_index('start_year')[region_name]], axis=0) #2019-2024\n",
    "    \n",
    "    # 绘制OGGM模拟结果\n",
    "    oggm_correction = oggm_corr_list.loc[region_name, 'OGGM_voffset']\n",
    "    # timei = pd.date_range(start=\"07/01/2000\", end=\"07/01/2024\", freq='YS-JUL')\n",
    "    timei = pd.date_range(start=\"01/01/2001\", end=\"01/01/2025\", freq='YS')\n",
    "    ax.plot(timei, ele1 + oggm_correction, 'o-', color='k', \n",
    "            linewidth=2, markersize=3.5, label='OGGM')\n",
    "    compare_year_corr = pd.DataFrame({'oggm_ele': ele1[3:] + oggm_correction,\n",
    "                                        'obs_ele': merged_year_series.values}, index=np.arange(2003, 2025)) #2000-2025\n",
    "    \n",
    "    # 设置标题和标签\n",
    "    title = region_name_clean\n",
    "    ax.set_title(title, y=1.0, fontdict=dict(fontsize=28, color='black', \n",
    "                family='Arial', weight='bold'))\n",
    "    # ax.set_ylabel('Elevation change / m', fontsize=13)\n",
    "    ax.grid(axis='y', color='gray', linestyle='--', linewidth=0.3)\n",
    "    ax.tick_params(length=4, width=1, labelsize=26)\n",
    "\n",
    "    ax.text(0.1, 0.5, f'$CC$ = {compare_year_corr.corr(method='pearson')['oggm_ele']['obs_ele']:.2f}',\n",
    "        fontsize=28, transform=ax.transAxes)\n",
    "\n",
    "    # dh annual\n",
    "    merge_corrected = pd.concat([icesat_year_corr.set_index('start_year')[region_name], #2003-2009\n",
    "                        cryosat2_year_corr.set_index('start_year').loc[2010:2025, region_name]], axis=0) #2010-2024\n",
    "    dh_annual = []\n",
    "    # for year in range(2000, 2025):\n",
    "    #     year_row = merge_corrected.loc[(merge_corrected['date']>=dt(year-1, 11, 1)) & (merge_corrected['date']<dt(year, 4, 1)), region_name]\n",
    "    #     next_year_row = merge_corrected.loc[(merge_corrected['date']>=dt(year, 11, 1)) & (merge_corrected['date']<dt(year+1, 4, 1)), region_name]\n",
    "\n",
    "    #     if year_row.empty or next_year_row.empty:\n",
    "    #         dh_annual.append(np.nan)\n",
    "    #     else:\n",
    "    #         dh_annual.append(next_year_row.median() - year_row.median()) #2000， 20001整年\n",
    "    for year in range(2004, 2025):\n",
    "        year_row = merge_corrected.loc[merge_corrected.index==year-1] #2003/7\n",
    "        next_year_row = merge_corrected.loc[merge_corrected.index==year] #2004/7\n",
    "\n",
    "        if year_row.empty or next_year_row.empty:\n",
    "            dh_annual.append(np.nan)\n",
    "        else:\n",
    "            dh_annual.append(next_year_row.values[0] - year_row.values[0]) #2000， 20001整年\n",
    "    \n",
    "    dh_merge = pd.DataFrame({'dh_annual': dh_annual},\n",
    "                            index=np.arange(2004, 2025))\n",
    "    dh_merge = dh_merge.dropna()\n",
    "    ax_right = ax.twinx()\n",
    "    bar_positions = pd.to_datetime(dh_merge.index.astype(int).astype(str) + '-07-01')\n",
    "    ax_right.bar(bar_positions, dh_merge['dh_annual'], \n",
    "                width=300,  # 宽度为365天（约1年），可以根据需要调整\n",
    "                color='skyblue',  # 蓝色\n",
    "                edgecolor='none',  # 无边框\n",
    "                alpha=0.5,\n",
    "                zorder=0)  # 最底层\n",
    "    ax_right.tick_params(length=4, width=1, labelsize=26)\n",
    "    ax_right.set_ylim([int(dh_merge['dh_annual'].min())-1, 8])\n",
    "\n",
    "\n",
    "    if display_legend:\n",
    "        ax.legend(fontsize=24, ncol=4, \n",
    "           bbox_to_anchor=(2, 1.2, 2.5, 2),\n",
    "           loc='lower right',\n",
    "           mode='expand')\n",
    "    \n",
    "    # plt.tight_layout()\n",
    "    # 保存结果\n",
    "    if output_dir is not None:\n",
    "        if display_legend:\n",
    "            save_path = os.path.join(output_dir, f'【{region_idx+1}】{region_name_clean}_legend.pdf')\n",
    "        else:\n",
    "            save_path = os.path.join(output_dir, f'【{region_idx+1}】{region_name_clean}.pdf')\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        plt.savefig(save_path, dpi=600, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_oggm_with_altimetry(region_idx, oggm_corr_list, output_dir=None, display_legend=False):\n",
    "    \"\"\"\n",
    "    对比OGGM模拟结果与测高观测数据\n",
    "    \n",
    "    参数:\n",
    "        region_idx: int, 区域索引（对应head列表）,16-37\n",
    "        path_idx: int, OGGM数据路径索引\n",
    "        oggm_corr_list: Dataframe\n",
    "    \"\"\"\n",
    "    region_name = head[region_idx]\n",
    "    region_name_clean = region_name\n",
    "\n",
    "    if '.1' in region_name:\n",
    "        region_name_clean = region_name_clean.replace('.1', '')\n",
    "    if '/' in region_name:\n",
    "        region_name_clean = region_name_clean.replace('/', '-')\n",
    "        \n",
    "    # 加载OGGM数据\n",
    "    hist_filename = 'run_output_hist_spinup_all.nc'\n",
    "    ds1 = xr.open_dataset(os.path.join(os.path.dirname(CONFIG['oggm_dir']), f'[{region_idx+1}]{region_name_clean}', hist_filename))\\\n",
    "                .sel(time=slice(2000, 2025)).sum(dim='rgi_id', skipna=True, keep_attrs=True) \n",
    "    \n",
    "    # 计算质量平衡和高程变化\n",
    "    smb1 = calculate_smb_from_volume(ds1) # m w.s./yr, 2000-2024\n",
    "    ele1 = calculate_elevation_change(smb1) # 累计高程变化，2000-2024，2024年整年\n",
    "    \n",
    "    # 绘制时间序列\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "    scalor_factor = 4\n",
    "    ax.plot(\n",
    "        hampel_icesat_corrected['date'], hampel_icesat_corrected[region_name],\n",
    "        '^-', color=plt.cm.Accent(0), linewidth=1, markersize=1.5,\n",
    "        label='ICESat'\n",
    "    )\n",
    "    ax.plot(\n",
    "        pd.to_datetime(icesat_year_corr['start_year'].astype(int).astype(str) + '-07-01'), \n",
    "        icesat_year_corr[region_name],\n",
    "        '-', color=plt.cm.Accent(4), linewidth=2,\n",
    "        label='ICESat annual mean'\n",
    "    )\n",
    "    ax.fill_between(\n",
    "        pd.to_datetime(icesat_year_corr['start_year'].astype(int).astype(str) + '-07-01'), \n",
    "        icesat_year_corr[region_name] - icesat_uncert_year[region_name]*scalor_factor,\n",
    "        icesat_year_corr[region_name] + icesat_uncert_year[region_name]*scalor_factor,\n",
    "        color='gray', alpha=0.3\n",
    "    )\n",
    "\n",
    "    ax.plot(\n",
    "        hampel_cryosat2_corrected['date'], hampel_cryosat2_corrected[region_name],\n",
    "        's-', color=plt.cm.Accent(1), linewidth=1, markersize=1.5,\n",
    "        label='CryoSat-2'\n",
    "    )\n",
    "    ax.plot(\n",
    "        pd.to_datetime(cryosat2_year_corr['start_year'].astype(int).astype(str) + '-07-01'), \n",
    "        cryosat2_year_corr[region_name],\n",
    "        '-', color=plt.cm.Accent(5), linewidth=2,\n",
    "        label='CryoSat-2 annual mean'\n",
    "    )\n",
    "    ax.fill_between(\n",
    "        pd.to_datetime(cryosat2_year_corr['start_year'].astype(int).astype(str) + '-07-01'), \n",
    "        cryosat2_year_corr[region_name] - cryosat2_uncert_year[region_name]*scalor_factor,\n",
    "        cryosat2_year_corr[region_name] + cryosat2_uncert_year[region_name]*scalor_factor,\n",
    "        color='gray', alpha=0.3\n",
    "    )\n",
    "\n",
    "    ax.plot(\n",
    "        hampel_icesat2_corrected['date'], hampel_icesat2_corrected[region_name],\n",
    "        'o-', color=plt.cm.Accent(2), linewidth=1, markersize=1.5,\n",
    "        label='ICESat-2'\n",
    "    )\n",
    "    ax.plot(\n",
    "        pd.to_datetime(icesat2_year_corr['start_year'].astype(int).astype(str) + '-07-01'), icesat2_year_corr[region_name],\n",
    "        '-', color=plt.cm.Accent(6), linewidth=2,\n",
    "        label='ICESat-2 annual mean'\n",
    "    )\n",
    "    ax.fill_between(\n",
    "        pd.to_datetime(icesat2_year_corr['start_year'].astype(int).astype(str) + '-07-01'), \n",
    "        icesat2_year_corr[region_name] - icesat2_uncert_year[region_name]*scalor_factor,\n",
    "        icesat2_year_corr[region_name] + icesat2_uncert_year[region_name]*scalor_factor,\n",
    "        color='gray', alpha=0.3\n",
    "    )\n",
    "    merged_year_series = pd.concat([icesat_year_corr.set_index('start_year')[region_name], #2003-2009\n",
    "                                     cryosat2_year_corr.set_index('start_year').loc[2010:2018, region_name], #2010-2018\n",
    "                                      icesat2_year_corr.set_index('start_year')[region_name]], axis=0) #2019-2024\n",
    "    \n",
    "    # 绘制OGGM模拟结果\n",
    "    oggm_correction = oggm_corr_list.loc[region_name, 'OGGM_voffset']\n",
    "    # timei = pd.date_range(start=\"07/01/2000\", end=\"07/01/2024\", freq='YS-JUL')\n",
    "    timei = pd.date_range(start=\"01/01/2001\", end=\"01/01/2025\", freq='YS')\n",
    "    ax.plot(timei, ele1 + oggm_correction, 'o-', color='k', \n",
    "            linewidth=2, markersize=3.5, label='OGGM')\n",
    "    compare_year_corr = pd.DataFrame({'oggm_ele': ele1[3:] + oggm_correction,\n",
    "                                        'obs_ele': merged_year_series.values}, index=np.arange(2003, 2025)) #2000-2025\n",
    "    \n",
    "    # 设置标题和标签\n",
    "    title = region_name_clean\n",
    "    ax.set_title(title, y=1.0, fontdict=dict(fontsize=28, color='black', \n",
    "                family='Arial', weight='bold'))\n",
    "    # ax.set_ylabel('Elevation change / m', fontsize=13)\n",
    "    ax.grid(axis='y', color='gray', linestyle='--', linewidth=0.3)\n",
    "    ax.tick_params(length=4, width=1, labelsize=26)\n",
    "\n",
    "    ax.text(0.1, 0.5, f'$CC$ = {compare_year_corr.corr(method='pearson')['oggm_ele']['obs_ele']:.2f}',\n",
    "        fontsize=28, transform=ax.transAxes)\n",
    "\n",
    "    # dh annual\n",
    "    merge_corrected = pd.concat([icesat_year_corr.set_index('start_year')[region_name], #2003-2009\n",
    "                        cryosat2_year_corr.set_index('start_year').loc[2010:2025, region_name]], axis=0) #2010-2024\n",
    "    dh_annual = []\n",
    "    # for year in range(2000, 2025):\n",
    "    #     year_row = merge_corrected.loc[(merge_corrected['date']>=dt(year-1, 11, 1)) & (merge_corrected['date']<dt(year, 4, 1)), region_name]\n",
    "    #     next_year_row = merge_corrected.loc[(merge_corrected['date']>=dt(year, 11, 1)) & (merge_corrected['date']<dt(year+1, 4, 1)), region_name]\n",
    "\n",
    "    #     if year_row.empty or next_year_row.empty:\n",
    "    #         dh_annual.append(np.nan)\n",
    "    #     else:\n",
    "    #         dh_annual.append(next_year_row.median() - year_row.median()) #2000， 20001整年\n",
    "    for year in range(2004, 2025):\n",
    "        year_row = merge_corrected.loc[merge_corrected.index==year-1] #2003/7\n",
    "        next_year_row = merge_corrected.loc[merge_corrected.index==year] #2004/7\n",
    "\n",
    "        if year_row.empty or next_year_row.empty:\n",
    "            dh_annual.append(np.nan)\n",
    "        else:\n",
    "            dh_annual.append(next_year_row.values[0] - year_row.values[0]) #2000， 20001整年\n",
    "    \n",
    "    dh_merge = pd.DataFrame({'dh_annual': dh_annual},\n",
    "                            index=np.arange(2004, 2025))\n",
    "    dh_merge = dh_merge.dropna()\n",
    "    ax_right = ax.twinx()\n",
    "    bar_positions = pd.to_datetime(dh_merge.index.astype(int).astype(str) + '-07-01')\n",
    "    ax_right.bar(bar_positions, dh_merge['dh_annual'], \n",
    "                width=300,  # 宽度为365天（约1年），可以根据需要调整\n",
    "                color='skyblue',  # 蓝色\n",
    "                edgecolor='none',  # 无边框\n",
    "                alpha=0.5,\n",
    "                zorder=0)  # 最底层\n",
    "    ax_right.tick_params(length=4, width=1, labelsize=26)\n",
    "    ax_right.set_ylim([int(dh_merge['dh_annual'].min())-1, 8])\n",
    "\n",
    "\n",
    "    if display_legend:\n",
    "        ax.legend(fontsize=24, ncol=4, \n",
    "           bbox_to_anchor=(2, 1.2, 2.5, 2),\n",
    "           loc='lower right',\n",
    "           mode='expand')\n",
    "    \n",
    "    # plt.tight_layout()\n",
    "    # 保存结果\n",
    "    if output_dir is not None:\n",
    "        if display_legend:\n",
    "            save_path = os.path.join(output_dir, f'【{region_idx+1}】{region_name_clean}_legend.pdf')\n",
    "        else:\n",
    "            save_path = os.path.join(output_dir, f'【{region_idx+1}】{region_name_clean}.pdf')\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        plt.savefig(save_path, dpi=600, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oggm_corr_list = pd.DataFrame(\n",
    "    {\n",
    "        head[0]: -2.0,\n",
    "        head[16]: -2.0,\n",
    "        head[17]: -2.2,\n",
    "        head[18]: -2.0,\n",
    "        head[19]: -3.0,\n",
    "        head[20]: -1.5,\n",
    "        head[21]: -4.0,\n",
    "        head[22]: -4.0,\n",
    "        head[23]: -3.0,\n",
    "        head[24]: -2.0,\n",
    "        head[25]: -1.5,\n",
    "        head[26]: -1.5,\n",
    "        head[27]: -3.0,\n",
    "        head[28]: -0.5,\n",
    "        head[29]: -2.5,\n",
    "        head[30]: -5.0,\n",
    "        head[31]: -1.0,\n",
    "        head[32]: -2.8,\n",
    "        head[33]: -3.0,\n",
    "        head[34]: -1.0,\n",
    "        head[35]: -2.0,\n",
    "        head[36]: -1.0,\n",
    "        head[37]: -3.5,\n",
    "    },\n",
    "    index=[0]\n",
    ").T\n",
    "\n",
    "oggm_corr_list.columns = ['OGGM_voffset']\n",
    "\n",
    "region_idx = 0\n",
    "# compare_oggm_with_altimetry(region_idx, oggm_corr_list, \n",
    "#                             output_dir=None, #os.path.join(CONFIG['output_dir'], 'oggm_altimetry_compar', 'legend')\n",
    "#                             display_legend=False)\n",
    "compare_oggm_with_altimetry(region_idx, oggm_corr_list, \n",
    "                            output_dir=os.path.join(CONFIG['output_dir'], 'oggm_altimetry_compar', 'no_legend'),\n",
    "                            display_legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oggm_corr_list = pd.DataFrame(\n",
    "    {\n",
    "        head[0]: -2.0,\n",
    "        head[16]: -2.0,\n",
    "        head[17]: -2.2,\n",
    "        head[18]: -2.0,\n",
    "        head[19]: -3.0,\n",
    "        head[20]: -1.5,\n",
    "        head[21]: -4.0,\n",
    "        head[22]: -4.0,\n",
    "        head[23]: -3.0,\n",
    "        head[24]: -2.0,\n",
    "        head[25]: -1.5,\n",
    "        head[26]: -1.5,\n",
    "        head[27]: -3.0,\n",
    "        head[28]: -0.5,\n",
    "        head[29]: -2.5,\n",
    "        head[30]: -5.0,\n",
    "        head[31]: -1.0,\n",
    "        head[32]: -2.8,\n",
    "        head[33]: -3.0,\n",
    "        head[34]: -1.0,\n",
    "        head[35]: -2.0,\n",
    "        head[36]: -1.0,\n",
    "        head[37]: -3.5,\n",
    "    },\n",
    "    index=[0]\n",
    ").T\n",
    "oggm_corr_list.columns = ['OGGM_voffset']\n",
    "\n",
    "region_indices = [0] + list(range(16,38))\n",
    "\n",
    "for region_idx in region_indices:\n",
    "    saved_path = os.path.join(CONFIG['output_dir'], 'oggm_altimetry_compar')\n",
    "\n",
    "    if '.1' in saved_path:\n",
    "        saved_path = saved_path.replace('.1', '')\n",
    "    if '/' in saved_path:\n",
    "        saved_path = saved_path.replace('/', '-')\n",
    "    \n",
    "    os.makedirs(os.path.dirname(saved_path), exist_ok=True)\n",
    "    \n",
    "    compare_oggm_with_altimetry(region_idx, oggm_corr_list, output_dir=saved_path, display_legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 每年高程变化散点图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STL分解提取趋势项\n",
    "icesat_trend_results = hampel_icesat_corrected[['mid_day', 'date']].copy()\n",
    "cry_trend_results = hampel_cryosat2_corrected[['mid_day', 'date']].copy()\n",
    "\n",
    "for region in head:\n",
    "    region_data = hampel_icesat_corrected[['date', region]].dropna()\n",
    "    stl = STL(\n",
    "            region_data[region].values, \n",
    "            period=3,\n",
    "            robust=True\n",
    "            )\n",
    "    decomposition = stl.fit()\n",
    "    trend_component = decomposition.trend\n",
    "    \n",
    "    icesat_trend_results[region] = np.nan\n",
    "    icesat_trend_results.loc[region_data.index, region] = trend_component\n",
    "\n",
    "    region_data = hampel_cryosat2_corrected[['date', region]].dropna()\n",
    "    stl = STL(\n",
    "            region_data[region].values, \n",
    "            period=12,\n",
    "            robust=True,\n",
    "            seasonal=13,\n",
    "            )\n",
    "    decomposition = stl.fit()\n",
    "    trend_component = decomposition.trend\n",
    "\n",
    "    cry_trend_results[region] = np.nan\n",
    "    cry_trend_results.loc[region_data.index, region] = trend_component\n",
    "\n",
    "merge_corrected = pd.concat([icesat_trend_results, cry_trend_results], axis=0)\n",
    "merge_corrected = merge_corrected.sort_values('date').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用原始观测数据(保留季节项)\n",
    "merge_corrected = pd.concat([hampel_icesat_corrected, hampel_cryosat2_corrected], axis=0)\n",
    "merge_corrected = merge_corrected.sort_values('date').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用原始观测数据(保留季节项)\n",
    "merge_corrected = pd.concat([hampel_icesat_corrected, \n",
    "                            hampel_cryosat2_corrected[hampel_cryosat2_corrected['date']<dt(2019,1,1)],\n",
    "                            hampel_icesat2_corrected[hampel_icesat2_corrected['date']>=dt(2019,1,1)]], \n",
    "                            axis=0)\n",
    "merge_corrected = merge_corrected.sort_values('date').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_scatter(region_idx, merge_corrected, output_dir=None, display_colorbar=False):\n",
    "    \"\"\"\n",
    "    对比OGGM模拟结果与测高观测数据\n",
    "    \n",
    "    参数:\n",
    "        region_idx: int, 区域索引（对应head列表）,16-37\n",
    "        path_idx: int, OGGM数据路径索引\n",
    "    \"\"\"\n",
    "    region_name = head[region_idx]\n",
    "    region_name_clean = region_name\n",
    "\n",
    "    if '.1' in region_name:\n",
    "        region_name_clean = region_name_clean.replace('.1', '')\n",
    "    if '/' in region_name:\n",
    "        region_name_clean = region_name_clean.replace('/', '-')\n",
    "        \n",
    "    # 加载OGGM数据\n",
    "    hist_filename = 'run_output_hist_spinup_all.nc'\n",
    "    ds1 = xr.open_dataset(os.path.join(os.path.dirname(CONFIG['oggm_dir']), f'[{region_idx+1}]{region_name_clean}', hist_filename))\\\n",
    "                .sel(time=slice(2000, 2025)).sum(dim='rgi_id', skipna=True, keep_attrs=True) \n",
    "    \n",
    "    # 计算质量平衡和高程变化\n",
    "    smb1 = calculate_smb_from_volume(ds1) # m w.s./yr, 2000-2024每年质量平衡\n",
    "    dh_year = smb1 / 0.9 # 每年冰川高程变化，2000年，2004年(准确来说，2001.1、2002.1、2003.1)\n",
    "\n",
    "    # 计算整年的测高高程变化，from 2000 to 2024\n",
    "    altimetry_result = []\n",
    "    for year in range(2000, 2025):\n",
    "        year_row = merge_corrected.loc[(merge_corrected['date']>=dt(year-1, 11, 1)) & (merge_corrected['date']<dt(year, 4, 1)), region_name]\n",
    "        next_year_row = merge_corrected.loc[(merge_corrected['date']>=dt(year, 11, 1)) & (merge_corrected['date']<dt(year+1, 4, 1)), region_name]\n",
    "\n",
    "        if year_row.empty or next_year_row.empty:\n",
    "            altimetry_result.append(np.nan)\n",
    "        else:\n",
    "            altimetry_result.append(next_year_row.median() - year_row.median()) #2000， 20001整年\n",
    "    \n",
    "    dh_merge = pd.DataFrame({'altimetry': altimetry_result, 'oggm': dh_year},\n",
    "                            index=np.arange(2000, 2025))\n",
    "    dh_merge = dh_merge.dropna()\n",
    "    # 绘制时间序列\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "    # 散点图\n",
    "    scatter = ax.scatter(dh_merge['altimetry'], dh_merge['oggm'], s=120, c=dh_merge.index,\n",
    "                vmin=2000, vmax=2024, cmap='Reds', \n",
    "                edgecolors='none', marker='o', alpha=0.7)\n",
    "    x_lim = ax.get_xlim() \n",
    "    y_lim = ax.get_ylim()\n",
    "    axis_min = min(x_lim[0], y_lim[0])\n",
    "    axis_max = max(x_lim[1], y_lim[1])\n",
    "    ax.plot([axis_min, axis_max], [axis_min, axis_max], linestyle='--', color='gray', linewidth=1.5, label='1:1 line')\n",
    "    ax.set_xlim([axis_min, axis_max])\n",
    "    ax.set_ylim([axis_min, axis_max])\n",
    "\n",
    "    # 拟合\n",
    "    x_data = dh_merge['altimetry'].values\n",
    "    y_data = dh_merge['oggm'].values\n",
    "    slope, intercept, _, _, _ = stats.linregress(x_data, y_data)\n",
    "    x_fit = np.array([axis_min, axis_max])\n",
    "    y_fit = slope * x_fit + intercept\n",
    "\n",
    "    ax.plot(x_fit, y_fit, color='red', linewidth=2, linestyle='--')\n",
    "    ax.axis('equal')\n",
    "    # ax.vlines(0, axis_min, axis_max, linestyle='-', color='gray',linewidth=2)\n",
    "    # ax.hlines(0, axis_min, axis_max, linestyle='-', color='gray',linewidth=2)\n",
    "\n",
    "    R2 = calc_nse(dh_merge['altimetry'], dh_merge['oggm'])\n",
    "    r = np.sqrt(calc_r2(dh_merge['altimetry'], dh_merge['oggm']))\n",
    "    rmse = calc_rmse(dh_merge['altimetry'], dh_merge['oggm'])\n",
    "    \n",
    "    # 设置标题和标签\n",
    "    title = region_name_clean\n",
    "    ax.set_title(title, y=1.0, fontdict=dict(fontsize=28, color='black', \n",
    "                family='Arial', weight='bold'))\n",
    "    # ax.set_ylabel('Elevation change / m', fontsize=13)\n",
    "    ax.tick_params(length=7, width=3, labelsize=26)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "\n",
    "    text = f'$r$: {r:.2f}\\nRMSE: {rmse:.2f} m/yr'\n",
    "\n",
    "    ax.text(0.05, 0.85, text,\n",
    "        fontsize=24, transform=ax.transAxes, va='bottom')\n",
    "\n",
    "    if display_colorbar:\n",
    "        cbar_ax = fig.add_axes([0, -0.2, 2, 0.07]) \n",
    "        cbar = fig.colorbar(scatter, cax=cbar_ax, orientation='horizontal')\n",
    "        cbar.set_label('Year', fontsize=26, labelpad=15)\n",
    "        cbar.ax.tick_params(labelsize=24, length=7, width=3)\n",
    "        cbar.set_ticks([2000, 2005, 2010, 2015, 2020, 2024])\n",
    "    \n",
    "    # plt.tight_layout()\n",
    "    # 保存结果\n",
    "    if output_dir is not None:\n",
    "        save_path = os.path.join(output_dir, f'【{region_idx+1}】{region_name_clean}.pdf')\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        plt.savefig(save_path, dpi=600, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_idx = 0\n",
    "compare_scatter(region_idx, merge_corrected,\n",
    "                            output_dir=r'E:\\revised_NCC_data\\OGGM model\\figure_plot\\oggm_altimetry_compar_eachyear\\legend', \n",
    "                            display_colorbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_indices = [0] + list(range(16,38))\n",
    "\n",
    "for region_idx in region_indices:\n",
    "    saved_path = os.path.join(CONFIG['output_dir'], 'oggm_altimetry_compar_eachyear')\n",
    "    \n",
    "    os.makedirs(saved_path, exist_ok=True)\n",
    "    \n",
    "    compare_scatter(region_idx, merge_corrected,\n",
    "                            output_dir=saved_path, \n",
    "                            display_colorbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_scatter(region_idlist, merge_corrected, output_dir=None, display_colorbar=False):\n",
    "    \"\"\"\n",
    "    对比OGGM模拟结果与测高观测数据\n",
    "    \n",
    "    参数:\n",
    "        region_idx: int, 区域索引（对应head列表）,16-37\n",
    "        path_idx: int, OGGM数据路径索引\n",
    "    \"\"\"\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    year_data = []\n",
    "    for region_idx in region_idlist:\n",
    "        region_name = head[region_idx]\n",
    "        region_name_clean = region_name\n",
    "\n",
    "        if '.1' in region_name:\n",
    "            region_name_clean = region_name_clean.replace('.1', '')\n",
    "        if '/' in region_name:\n",
    "            region_name_clean = region_name_clean.replace('/', '-')\n",
    "            \n",
    "        # 加载OGGM数据\n",
    "        hist_filename = 'run_output_hist_spinup_all.nc'\n",
    "        ds1 = xr.open_dataset(os.path.join(os.path.dirname(CONFIG['oggm_dir']), f'[{region_idx+1}]{region_name_clean}', hist_filename))\\\n",
    "                    .sel(time=slice(2000,2025)).sum(dim='rgi_id', skipna=True, keep_attrs=True) \n",
    "        \n",
    "        # 计算质量平衡和高程变化\n",
    "        smb1 = calculate_smb_from_volume(ds1) # m w.s./yr, 2000-2024每年质量平衡\n",
    "        dh_year = smb1 / 0.9 # 每年冰川高程变化，2000年，2004年\n",
    "\n",
    "        # 计算整年的测高高程变化\n",
    "        altimetry_result = []\n",
    "        for year in range(2000, 2025):\n",
    "            year_row = merge_corrected.loc[(merge_corrected['date']>=dt(year-1, 7, 1)) & (merge_corrected['date']<dt(year, 7, 1)), region_name]\n",
    "            next_year_row = merge_corrected.loc[(merge_corrected['date']>=dt(year, 7, 1)) & (merge_corrected['date']<dt(year+1, 7, 1)), region_name]\n",
    "\n",
    "            if year_row.empty or next_year_row.empty:\n",
    "                altimetry_result.append(np.nan)\n",
    "            else:\n",
    "                altimetry_result.append(next_year_row.median() - year_row.median()) #2000， 20001整年\n",
    "        \n",
    "        dh_merge = pd.DataFrame({'altimetry': altimetry_result, 'oggm': dh_year},\n",
    "                                index=np.arange(2000, 2025))\n",
    "        dh_merge = dh_merge.dropna()\n",
    "\n",
    "        x_data.extend(dh_merge['altimetry'].values)\n",
    "        y_data.extend(dh_merge['oggm'].values)\n",
    "        year_data.extend(dh_merge.index.values)\n",
    "    # 绘制时间序列\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    x_data = np.array(x_data)\n",
    "    y_data = np.array(y_data)\n",
    "    year_data = np.array(year_data)\n",
    "\n",
    "    # 散点图\n",
    "    scatter = ax.scatter(x_data, y_data, s=100, c=year_data,\n",
    "                vmin=2000, vmax=2024, cmap='Reds', \n",
    "                edgecolors='none', marker='o', alpha=0.7)\n",
    "    x_lim = ax.get_xlim() \n",
    "    y_lim = ax.get_ylim()\n",
    "    axis_min = min(x_lim[0], y_lim[0])\n",
    "    axis_max = max(x_lim[1], y_lim[1])\n",
    "    ax.plot([axis_min, axis_max], [axis_min, axis_max], linestyle='--', color='gray', linewidth=1.5, label='1:1 line')\n",
    "    ax.set_xlim([axis_min, axis_max])\n",
    "    ax.set_ylim([axis_min, axis_max])\n",
    "\n",
    "    # 拟合\n",
    "    X = sm.add_constant(x_data)\n",
    "    rlm_model = RLM(y_data, X, M=sm.robust.norms.HuberT())\n",
    "    rlm_results = rlm_model.fit()\n",
    "\n",
    "    slope = rlm_results.params[1]\n",
    "    intercept = rlm_results.params[0]\n",
    "    y_pred = rlm_results.fittedvalues\n",
    "    r_model = np.corrcoef(y_data, y_pred)[0, 1]\n",
    "    x_fit = np.array([axis_min, axis_max])\n",
    "    y_fit = slope * x_fit + intercept\n",
    "\n",
    "    ax.plot(x_fit, y_fit, color='red', linewidth=2, linestyle='--')\n",
    "    ax.axis('equal')\n",
    "\n",
    "    # R2 = calc_nse(x_data, y_data)\n",
    "    r = np.sqrt(calc_r2(x_data, y_data))\n",
    "    rmse = calc_rmse(x_data, y_data)\n",
    "    \n",
    "    # 设置标题和标签\n",
    "    # title = region_name_clean\n",
    "    # ax.set_title(title, y=1.0, fontdict=dict(fontsize=28, color='black', \n",
    "    #             family='Arial', weight='bold'))\n",
    "    # ax.set_ylabel('Elevation change / m', fontsize=13)\n",
    "    ax.tick_params(length=7, width=3, labelsize=26)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "\n",
    "    text = f'$r$: {r:.2f}\\nRMSE: {rmse:.2f} m/yr'\n",
    "\n",
    "    ax.text(0.05, 0.80, text,\n",
    "        fontsize=26, transform=ax.transAxes, va='bottom')\n",
    "    ax.set_xlabel('Annual elevation change from altimetry (m/yr)', fontsize=28)\n",
    "    ax.set_ylabel('Annual elevation change from OGGM (m/yr)', fontsize=28)\n",
    "\n",
    "    ax.set_xlim([-5, 4])\n",
    "    ax.set_ylim([-5, 4])\n",
    "\n",
    "    if display_colorbar:\n",
    "        cbar = fig.colorbar(scatter, ax=ax, orientation='vertical')\n",
    "        cbar.set_label('Year', fontsize=26, labelpad=15)\n",
    "        cbar.ax.tick_params(labelsize=24, length=7, width=3)\n",
    "        cbar.set_ticks([2000, 2005, 2010, 2015, 2020, 2024])\n",
    "    \n",
    "    # plt.tight_layout()\n",
    "    # 保存结果\n",
    "    if output_dir is not None:\n",
    "        save_path = os.path.join(output_dir, f'HMA.pdf')\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        plt.savefig(save_path, dpi=600, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_indices = [0] + list(range(16,38))\n",
    "compare_scatter(region_indices, merge_corrected,\n",
    "                            output_dir=r'E:\\revised_NCC_data\\OGGM model\\figure_plot\\oggm_altimetry_compar_eachyear\\all_region_merge', \n",
    "                            display_colorbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 2023年气候敏感性分析\n",
    "\n",
    "对比不同情景（原始、2023、2022-23、降水增强等）下的质量平衡与高程变化，并输出关键统计。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sensitivity(region_dirs, save_dir, CONFIG):\n",
    "    \"\"\"\n",
    "    分析2023年气候极端多情景下的质量平衡与高程变化对比。\n",
    "    依赖：CONFIG['oggm_path'], path_list\n",
    "    \"\"\"\n",
    "    results = pd.DataFrame(columns=['Original', 'temp05', 'prcpsum60', 'prcpwin60'])\n",
    "    for region_dir in region_dirs:\n",
    "        ds1 = xr.open_dataset(os.path.join(region_dir, 'run_output_hist.nc')).sum(dim='rgi_id', skipna=True, keep_attrs=True)\n",
    "        ds2 = xr.open_dataset(os.path.join(region_dir, 'run_output_hist_2023tempminus05.nc')).sum(dim='rgi_id', skipna=True, keep_attrs=True)\n",
    "        ds3 = xr.open_dataset(os.path.join(region_dir, 'run_output_hist_2023prcpsumplus60.nc')).sum(dim='rgi_id', skipna=True, keep_attrs=True)\n",
    "        ds4 = xr.open_dataset(os.path.join(region_dir, 'run_output_hist_2023prcpwinplus60.nc')).sum(dim='rgi_id', skipna=True, keep_attrs=True)\n",
    "\n",
    "        smb1 = calculate_smb_from_volume(ds1)\n",
    "        smb2 = calculate_smb_from_volume(ds2)\n",
    "        smb3 = calculate_smb_from_volume(ds3) #m w.s./yr\n",
    "        smb4 = calculate_smb_from_volume(ds4)\n",
    "\n",
    "        # ele1 = calculate_elevation_change(smb1)\n",
    "        # ele2 = calculate_elevation_change(smb2)\n",
    "        # ele3 = calculate_elevation_change(smb3)\n",
    "        # ele4 = calculate_elevation_change(smb4)\n",
    "\n",
    "        region_name = region_dir.split(os.sep)[-1]\n",
    "        region_name_clean = region_name.replace('-','/') if '-' in region_name else region_name\n",
    "        region_name_clean = region_name_clean.split(']')[1]\n",
    "\n",
    "        results.loc[region_name_clean, 'Original'] = smb1[-2]\n",
    "        results.loc[region_name_clean, 'temp05'] = smb2[-2]\n",
    "        results.loc[region_name_clean, 'prcpsum60'] = smb3[-2]\n",
    "        results.loc[region_name_clean, 'prcpwin60'] = smb4[-2]\n",
    "\n",
    "        ds1.close()\n",
    "        ds2.close()\n",
    "        ds3.close()\n",
    "        ds4.close()\n",
    "\n",
    "        print(f'{region_name} recorded.')\n",
    "\n",
    "    results = results.sort_values(by='Original', ascending=True)\n",
    "    fig, ax = plt.subplots(figsize=(8,12), dpi= 600)\n",
    "    # x = np.arange(1,len(region_dirs))\n",
    "\n",
    "    ax.scatter(results['Original'], results.index, marker='o', s=80, label='Original')\n",
    "    ax.scatter(results['temp05'], results.index, marker='v', s=80, label='Temperature -0.5 °C')\n",
    "    ax.scatter(results['prcpsum60'], results.index, marker='*', s=130, label='Summer precipitation +60 mm')\n",
    "    ax.scatter(results['prcpwin60'], results.index, marker='x', s=80, label='Winter precipitation +60 mm')\n",
    "\n",
    "    ax.tick_params(axis='both',labelsize=16)\n",
    "    # ax.invert_yaxis()  # labels read top-to-bottom\n",
    "    ax.set_xlabel('Mass balance 2023 (m w.e./yr)', fontsize=18, labelpad=5)\n",
    "    ax.grid(axis='y',linestyle='--', alpha=0.5,linewidth=0.3)\n",
    "    ax.grid(axis='x',linestyle='--', alpha=1,linewidth=0.8)\n",
    "    ax.legend(fontsize=15)\n",
    "\n",
    "    save_path = os.path.join(save_dir, 'Sensitivity test', 'sensitivity test 2023.pdf')\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    plt.savefig(save_path, dpi=600, bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n",
    "        \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.path.dirname(CONFIG['oggm_dir'])\n",
    "region_dirs = [os.path.join(base_dir, item) for item in os.listdir(base_dir)]\n",
    "save_dir = CONFIG['output_dir']\n",
    "\n",
    "sensitivity_results = plot_sensitivity(region_dirs, save_dir, CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 十年尺度气候极端分析（In[6]）\n",
    "\n",
    "对比 2004-2013、2014-2023 两个十年段的冰川质量平衡统计，识别极端年份。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decades_compar(region_dirs, save_dir, display_legend=False):\n",
    "    \"\"\"\n",
    "    十年尺度对比：2005-2014 vs 2015-2024，计算均值与标准差并可视化。\n",
    "    \"\"\"\n",
    "    for region_dir in region_dirs:\n",
    "        ds1 = xr.open_dataset(os.path.join(region_dir, 'run_output_hist_spinup_all.nc'))\\\n",
    "                    .sel(time=slice(2000,2025)).sum(dim='rgi_id', skipna=True, keep_attrs=True)\n",
    "        ds2 = xr.open_dataset(os.path.join(region_dir, 'run_output_hist_warm_add_spinup_all.nc'))\\\n",
    "                    .sel(time=slice(2000,2025)).sum(dim='rgi_id', skipna=True, keep_attrs=True)\n",
    "        ds3 = xr.open_dataset(os.path.join(region_dir, 'run_output_hist_warm_rm_spinup_all.nc'))\\\n",
    "                    .sel(time=slice(2000,2025)).sum(dim='rgi_id', skipna=True, keep_attrs=True)\n",
    "        # mass balance\n",
    "        smb1 = calculate_smb_from_volume(ds1) # normal\n",
    "        smb2 = calculate_smb_from_volume(ds2) # 2005-2014 + warm\n",
    "        smb3 = calculate_smb_from_volume(ds3) #m w.e./yr, 2015-2024 - warm\n",
    "\n",
    "        # mean and std\n",
    "        smb_pre = smb1[5:15]\n",
    "        smb_cur = smb1[15:]\n",
    "        smb_pre_warm = smb2[5:15] \n",
    "        smb_cur_dewarm = smb3[15:]\n",
    "\n",
    "        mean_pre = np.mean(smb1[5:15])\n",
    "        mean_cur = np.mean(smb1[15:])\n",
    "        mean_pre_warm = np.mean(smb2[5:15])\n",
    "        mean_cur_dewarm = np.mean(smb3[15:])\n",
    "\n",
    "        std_pre = np.std(smb1[5:15])\n",
    "        std_cur = np.std(smb1[15:])\n",
    "        std_pre_warm = np.std(smb2[5:15])\n",
    "        std_cur_dewarm = np.std(smb3[15:])\n",
    "\n",
    "        region_name = region_dir.split(os.sep)[-1]\n",
    "        region_name_clean = region_name.replace('-','/') if '-' in region_name else region_name\n",
    "        \n",
    "        # 迷你图\n",
    "        fig, ax = plt.subplots(1,1,figsize=(4,3), dpi=600)\n",
    "        t = ds1.time[:-1]\n",
    "\n",
    "        ax.plot(t[15:], smb_cur, '^-', color=plt.cm.Accent(6), linewidth=1,markersize=3,label='Current decade')\n",
    "        ax.plot(t[15:], smb_cur_dewarm, 'v-', color=plt.cm.Accent(7), linewidth=1,markersize=3,label='Current decade with warming trend removed')\n",
    "        ax.plot(t[5:15], smb_pre, 's-', color=plt.cm.Accent(4), linewidth=1,markersize=3,label='Previous decade');\n",
    "        ax.plot(t[5:15], smb_pre_warm, 'o-', color=plt.cm.Accent(5), linewidth=1,markersize=3,label='Previous decade with warming trend added');\n",
    "\n",
    "        ax.hlines(mean_cur,2015,2024, linestyles='dashed',linewidth=2,color=plt.cm.Accent(6),zorder=5)\n",
    "        ax.fill_between(t[15:], mean_cur-std_cur, mean_cur+std_cur,color=plt.cm.Accent(6),alpha = 0.11)\n",
    "        ax.hlines(mean_cur_dewarm,2015,2024, linestyles='dashed',linewidth=2,color=plt.cm.Accent(7),zorder=5)\n",
    "        ax.fill_between(t[15:],mean_cur_dewarm-std_cur_dewarm, mean_cur_dewarm+std_cur_dewarm,color=plt.cm.Accent(7),alpha = 0.11)\n",
    "        ax.hlines(mean_pre,2005,2014, linestyles='dashed',linewidth=2,color=plt.cm.Accent(4),zorder=5)\n",
    "        ax.fill_between(t[5:15],mean_pre-std_pre, mean_pre+std_pre,color=plt.cm.Accent(4),alpha = 0.11)\n",
    "        ax.hlines(mean_pre_warm,2005,2014, linestyles='dashed',linewidth=2,color=plt.cm.Accent(5),zorder=5)\n",
    "        ax.fill_between(t[5:15],mean_pre_warm-std_pre_warm, mean_pre_warm+std_pre_warm,color=plt.cm.Accent(5),alpha = 0.11)\n",
    "        \n",
    "        ax.set_title(region_name_clean.split(']')[1], fontsize=16, weight='bold')\n",
    "        # ax.grid(axis='y',color='gray',linestyle=((0,(5,10))),linewidth = 0.3)\n",
    "        ax.tick_params(labelsize=14)\n",
    "        # ax.xaxis.set_major_locator(MultipleLocator(4))\n",
    "\n",
    "        contribution = (1- ((mean_pre_warm - mean_pre) + (mean_cur - mean_cur_dewarm))/2/(mean_cur - mean_pre)) * 100\n",
    "        ax.text(0.08,0.07,f'CR = {contribution:.1f}%',fontdict=dict(fontsize=16, color='k'),transform=ax.transAxes)\n",
    "\n",
    "        if display_legend:\n",
    "            ax.legend(fontsize=16, ncol=2, \n",
    "                bbox_to_anchor=(2, 1.2, 4, 2),\n",
    "                loc='lower right',\n",
    "                mode='expand')\n",
    "    \n",
    "        # plt.tight_layout()\n",
    "        # 保存结果\n",
    "        if display_legend:\n",
    "            save_path = os.path.join(save_dir, 'pre_cur_decade_compar', 'legend', f'{region_name_clean}_legend.pdf')\n",
    "        else:\n",
    "            save_path = os.path.join(save_dir, 'pre_cur_decade_compar', f'{region_name_clean}.pdf')\n",
    "        \n",
    "        save_path = save_path.replace('/', '-')\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        plt.savefig(save_path, dpi=600, bbox_inches='tight')\n",
    "    \n",
    "        # plt.show()\n",
    "        print(f'{region_name} finished! Most serious year: {np.argmin(smb1)+2000}')\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decades_compar(region_dirs, save_dir, display_legend=False):\n",
    "    \"\"\"\n",
    "    十年尺度对比：2005-2014 vs 2015-2024，计算均值与标准差并可视化。\n",
    "    \"\"\"\n",
    "    for region_dir in region_dirs:\n",
    "        ds1 = xr.open_dataset(os.path.join(region_dir, 'run_output_hist_spinup_all.nc'))\\\n",
    "                    .sel(time=slice(2000,2025)).sum(dim='rgi_id', skipna=True, keep_attrs=True)\n",
    "        ds2 = xr.open_dataset(os.path.join(region_dir, 'run_output_hist_warm_add_spinup_all.nc'))\\\n",
    "                    .sel(time=slice(2000,2025)).sum(dim='rgi_id', skipna=True, keep_attrs=True)\n",
    "        ds3 = xr.open_dataset(os.path.join(region_dir, 'run_output_hist_warm_rm_spinup_all.nc'))\\\n",
    "                    .sel(time=slice(2000,2025)).sum(dim='rgi_id', skipna=True, keep_attrs=True)\n",
    "        # mass balance\n",
    "        smb1 = calculate_smb_from_volume(ds1) # normal\n",
    "        smb2 = calculate_smb_from_volume(ds2) # 2005-2014 + warm\n",
    "        smb3 = calculate_smb_from_volume(ds3) #m w.e./yr, 2015-2024 - warm\n",
    "\n",
    "        # mean and std\n",
    "        smb_pre = smb1[5:15]\n",
    "        smb_cur = smb1[15:]\n",
    "        smb_pre_warm = smb2[5:15] \n",
    "        smb_cur_dewarm = smb3[15:]\n",
    "\n",
    "        mean_pre = np.mean(smb1[5:15])\n",
    "        mean_cur = np.mean(smb1[15:])\n",
    "        mean_pre_warm = np.mean(smb2[5:15])\n",
    "        mean_cur_dewarm = np.mean(smb3[15:])\n",
    "\n",
    "        std_pre = np.std(smb1[5:15])\n",
    "        std_cur = np.std(smb1[15:])\n",
    "        std_pre_warm = np.std(smb2[5:15])\n",
    "        std_cur_dewarm = np.std(smb3[15:])\n",
    "\n",
    "        region_name = region_dir.split(os.sep)[-1]\n",
    "        region_name_clean = region_name.replace('-','/') if '-' in region_name else region_name\n",
    "        \n",
    "        # 迷你图\n",
    "        fig, ax = plt.subplots(1,1,figsize=(7,4), dpi=600)\n",
    "        t = ds1.time[:-1]\n",
    "\n",
    "        ax.plot(t[15:], smb_cur, '^-', color=plt.cm.Accent(6), linewidth=1,markersize=3,label='Current decade')\n",
    "        ax.plot(t[15:], smb_cur_dewarm, 'v-', color=plt.cm.Accent(7), linewidth=1,markersize=3,label='Current decade with warming trend removed')\n",
    "        ax.plot(t[5:15], smb_pre, 's-', color=plt.cm.Accent(4), linewidth=1,markersize=3,label='Previous decade');\n",
    "        ax.plot(t[5:15], smb_pre_warm, 'o-', color=plt.cm.Accent(5), linewidth=1,markersize=3,label='Previous decade with warming trend added');\n",
    "\n",
    "        ax.hlines(mean_cur,2015,2024, linestyles='dashed',linewidth=2,color=plt.cm.Accent(6),zorder=5)\n",
    "        ax.fill_between(t[15:], mean_cur-std_cur, mean_cur+std_cur,color=plt.cm.Accent(6),alpha = 0.11)\n",
    "        ax.hlines(mean_cur_dewarm,2015,2024, linestyles='dashed',linewidth=2,color=plt.cm.Accent(7),zorder=5)\n",
    "        ax.fill_between(t[15:],mean_cur_dewarm-std_cur_dewarm, mean_cur_dewarm+std_cur_dewarm,color=plt.cm.Accent(7),alpha = 0.11)\n",
    "        ax.hlines(mean_pre,2005,2014, linestyles='dashed',linewidth=2,color=plt.cm.Accent(4),zorder=5)\n",
    "        ax.fill_between(t[5:15],mean_pre-std_pre, mean_pre+std_pre,color=plt.cm.Accent(4),alpha = 0.11)\n",
    "        ax.hlines(mean_pre_warm,2005,2014, linestyles='dashed',linewidth=2,color=plt.cm.Accent(5),zorder=5)\n",
    "        ax.fill_between(t[5:15],mean_pre_warm-std_pre_warm, mean_pre_warm+std_pre_warm,color=plt.cm.Accent(5),alpha = 0.11)\n",
    "        \n",
    "        ax.set_title(region_name_clean.split(']')[1], fontsize=16, weight='bold')\n",
    "        # ax.grid(axis='y',color='gray',linestyle=((0,(5,10))),linewidth = 0.3)\n",
    "        ax.tick_params(labelsize=14)\n",
    "        ax.xaxis.set_major_locator(MultipleLocator(4))\n",
    "\n",
    "        contribution = (1- ((mean_pre_warm - mean_pre) + (mean_cur - mean_cur_dewarm))/2/(mean_cur - mean_pre)) * 100\n",
    "        ax.text(0.08,0.07,f'CR = {contribution:.1f}%',fontdict=dict(fontsize=16, color='k'),transform=ax.transAxes)\n",
    "\n",
    "        if display_legend:\n",
    "            ax.legend(fontsize=16, ncol=2, \n",
    "                bbox_to_anchor=(2, 1.2, 4, 2),\n",
    "                loc='lower right',\n",
    "                mode='expand')\n",
    "    \n",
    "        # plt.tight_layout()\n",
    "        # 保存结果\n",
    "        if display_legend:\n",
    "            save_path = os.path.join(save_dir, 'pre_cur_decade_compar', 'legend', f'{region_name_clean}_legend.pdf')\n",
    "        else:\n",
    "            save_path = os.path.join(save_dir, 'pre_cur_decade_compar', f'{region_name_clean}.pdf')\n",
    "        \n",
    "        save_path = save_path.replace('/', '-')\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        plt.savefig(save_path, dpi=600, bbox_inches='tight')\n",
    "    \n",
    "        # plt.show()\n",
    "        print(f'{region_name} finished! Most serious year: {np.argmin(smb1)+2000}')\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.path.dirname(CONFIG['oggm_dir'])\n",
    "region_dirs = [os.path.join(base_dir, item) for item in os.listdir(base_dir)]\n",
    "save_dir = CONFIG['output_dir']\n",
    "\n",
    "plot_decades_compar([region_dirs[3]], save_dir, display_legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decades_compar(region_dirs, save_dir, display_legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 冰川消融与气候极端强度的非线性关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载气候极端评分数据\n",
    "score_path= r'E:\\revised_NCC_data\\OGGM model\\climate_score\\glacier_extreme_scores_ERA5.xlsx'\n",
    "\n",
    "score_all = pd.read_excel(\n",
    "    score_path, \n",
    "    sheet_name='Compound_Extreme_Scores', \n",
    "    header=0, \n",
    "    index_col=0\n",
    ")\n",
    "score_all.columns = score_all.columns.astype('int')\n",
    "\n",
    "score_temp = pd.read_excel(\n",
    "    score_path, \n",
    "    sheet_name='Temperature_Extreme_Scores', \n",
    "    header=0, \n",
    "    index_col=0\n",
    ")\n",
    "score_temp.columns = score_temp.columns.astype('int')\n",
    "\n",
    "score_prcp = pd.read_excel(\n",
    "    score_path, \n",
    "    sheet_name='Precipitation_Extreme_Scores', \n",
    "    header=0, \n",
    "    index_col=0\n",
    ")\n",
    "score_prcp.columns = score_prcp.columns.astype('int')\n",
    "\n",
    "base_dir = os.path.dirname(CONFIG['oggm_dir'])\n",
    "region_dirs = [os.path.join(base_dir, item) for item in os.listdir(base_dir)]\n",
    "region_names = [item.split(os.sep)[-1].split(']')[-1] for item in region_dirs]\n",
    "region_names = [item.replace('-', '/') for item in region_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_average(array, weight):\n",
    "    # 创建有效值掩码（排除 NaN 和对应的权重）\n",
    "    valid_mask = ~np.isnan(array) & ~np.isnan(weight) & (weight > 0)\n",
    "    result = np.average(\n",
    "            array[valid_mask], \n",
    "            weights=weight[valid_mask])\n",
    "    return result\n",
    "\n",
    "def analyze_climate_extreme_impact(region_dirs):\n",
    "    \"\"\"\n",
    "    分析气候极端事件对冰川质量平衡的影响\n",
    "    \n",
    "    参数:\n",
    "        ds = xr.open_dataset(\n",
    "            os.path.join(region_dir, 'run_output_hist.nc')\n",
    "        )\n",
    "        ds_total = ds.sum(dim='rgi_id', skipna=True, keep_attrs=True)\n",
    "        \n",
    "        pd.DataFrame: 包含各区域统计结果的数据框\n",
    "    \"\"\"\n",
    "    \n",
    "    # 2. 初始化结果数据框\n",
    "    results = pd.DataFrame(\n",
    "        data=np.zeros((12, len(region_names))), \n",
    "        columns=region_names\n",
    "    )\n",
    "    results.index = [\n",
    "        'sum0410', 'sum1117', 'sum1824',  # 三个时间段的总质量平衡\n",
    "        'avg0410', 'avg1117', 'avg1824',\n",
    "        'ext0410', 'ext1117', 'ext1824',  # 极端年份的质量平衡\n",
    "        'avg_ext0410', 'avg_ext1117', 'avg_ext1824'   # 极端年份数量\n",
    "    ]\n",
    "    \n",
    "    # 3. 处理每个区域\n",
    "    for region_dir in region_dirs:\n",
    "        region_name = region_dir.split(os.sep)[-1]\n",
    "        region_name_clean = region_name.replace('-','/') if '-' in region_name else region_name\n",
    "        region_name_clean = region_name_clean.split(']')[-1] # N/W Tien Shen\n",
    "\n",
    "        # 加载OGGM数据\n",
    "        ds = xr.open_dataset(\n",
    "            os.path.join(region_dir, 'run_output_hist_spinup_all.nc')\n",
    "        ).sel(time=slice(2000,2025))\n",
    "        # ds_total = ds.sum(dim='rgi_id', skipna=True, keep_attrs=True)\n",
    "        \n",
    "        # # 计算总质量平衡\n",
    "        # smb = calculate_smb_from_volume(ds_total) #2000-2024\n",
    "        total_smb_sum_0410 = []\n",
    "        total_smb_mean_0410 = []\n",
    "        total_smb_sum_1117 = []\n",
    "        total_smb_mean_1117 = []\n",
    "        total_smb_sum_1824 = []\n",
    "        total_smb_mean_1824 = []\n",
    "\n",
    "        weight_0410 = []\n",
    "        weight_1117 = []\n",
    "        weight_1824 = []\n",
    "\n",
    "        extreme_smb_sum_0410 = []\n",
    "        extreme_smb_mean_0410 = []\n",
    "        extreme_smb_sum_1117 = []\n",
    "        extreme_smb_mean_1117 = []\n",
    "        extreme_smb_sum_1824 = []\n",
    "        extreme_smb_mean_1824 = []\n",
    "\n",
    "        # 计算极端年总质量平衡和平均质量平衡\n",
    "        for rgi_id in ds['rgi_id'].values:\n",
    "            rgi_all_score = score_all.loc[rgi_id]\n",
    "            rgi_smb = calculate_smb_from_volume(ds.sel(rgi_id=rgi_id)) #array，25个值, 2000-2024\n",
    "            rgi_area = ds.sel(rgi_id=rgi_id)['area'].values\n",
    "\n",
    "            weight_0410.append(np.min(rgi_area[4:11]))\n",
    "            weight_1117.append(np.min(rgi_area[11:18]))\n",
    "            weight_1824.append(np.min(rgi_area[18:25]))\n",
    "\n",
    "            rgi_smb = np.where(np.abs(rgi_smb)>100, np.nan, rgi_smb)\n",
    "            total_smb_sum_0410.append(np.nansum(rgi_smb[4:11]))\n",
    "            total_smb_sum_1117.append(np.nansum(rgi_smb[11:18]))\n",
    "            total_smb_sum_1824.append(np.nansum(rgi_smb[18:]))\n",
    "            total_smb_mean_0410.append(np.nanmean(rgi_smb[4:11]))\n",
    "            total_smb_mean_1117.append(np.nanmean(rgi_smb[11:18]))\n",
    "            total_smb_mean_1824.append(np.nanmean(rgi_smb[18:]))\n",
    "            # 极端年\n",
    "            extreme_year = np.array(rgi_all_score.nlargest(5).index.tolist()) # (2012,2013,2022)\n",
    "            \n",
    "            extreme_year_filtered = extreme_year[(extreme_year >= 2004) & (extreme_year <= 2010)]\n",
    "            if len(extreme_year_filtered) > 0:\n",
    "                temp_smb = rgi_smb[extreme_year_filtered-2000]\n",
    "                temp_sum_0410 = np.nansum(temp_smb)\n",
    "                temp_mean_0410 = np.nanmean(temp_smb)\n",
    "            else:\n",
    "                temp_sum_0410 = np.nan\n",
    "                temp_mean_0410 = np.nan\n",
    "            \n",
    "            extreme_year_filtered = extreme_year[(extreme_year >= 2011) & (extreme_year <= 2017)]\n",
    "            if len(extreme_year_filtered) > 0:\n",
    "                temp_smb = rgi_smb[extreme_year_filtered-2000]\n",
    "                temp_sum_1117 = np.nansum(temp_smb)\n",
    "                temp_mean_1117 = np.nanmean(temp_smb)\n",
    "            else:\n",
    "                temp_sum_1117 = np.nan\n",
    "                temp_mean_1117 = np.nan\n",
    "            \n",
    "            extreme_year_filtered = extreme_year[(extreme_year >= 2018) & (extreme_year <= 2024)]\n",
    "            if len(extreme_year_filtered) > 0:\n",
    "                temp_smb = rgi_smb[extreme_year_filtered-2000]\n",
    "                temp_sum_1824 = np.nansum(temp_smb)\n",
    "                temp_mean_1824 = np.nanmean(temp_smb)\n",
    "            else:\n",
    "                temp_sum_1824 = np.nan\n",
    "                temp_mean_1824 = np.nan\n",
    "\n",
    "            extreme_smb_sum_0410.append(temp_sum_0410)\n",
    "            extreme_smb_mean_0410.append(temp_mean_0410)\n",
    "            extreme_smb_sum_1117.append(temp_sum_1117)\n",
    "            extreme_smb_mean_1117.append(temp_mean_1117)\n",
    "            extreme_smb_sum_1824.append(temp_sum_1824)\n",
    "            extreme_smb_mean_1824.append(temp_mean_1824)\n",
    "\n",
    "        total_smb_mean_0410 = np.where(np.abs(total_smb_sum_0410)>500, np.nan, total_smb_mean_0410) # all rgis\n",
    "        total_smb_sum_0410 = np.where(np.abs(total_smb_sum_0410)>500, np.nan, total_smb_sum_0410)\n",
    "        total_smb_mean_1117 = np.where(np.abs(total_smb_sum_1117)>500, np.nan, total_smb_mean_1117)\n",
    "        total_smb_sum_1117 = np.where(np.abs(total_smb_sum_1117)>500, np.nan, total_smb_sum_1117)\n",
    "        total_smb_mean_1824 = np.where(np.abs(total_smb_sum_1824)>500, np.nan, total_smb_mean_1824)\n",
    "        total_smb_sum_1824 = np.where(np.abs(total_smb_sum_1824)>500, np.nan, total_smb_sum_1824)\n",
    "\n",
    "        extreme_smb_mean_0410 = np.where(np.abs(extreme_smb_sum_0410)>500, np.nan, extreme_smb_mean_0410)\n",
    "        extreme_smb_sum_0410 = np.where(np.abs(extreme_smb_sum_0410)>500, np.nan, extreme_smb_sum_0410)\n",
    "        extreme_smb_mean_1117 = np.where(np.abs(extreme_smb_sum_1117)>500, np.nan, extreme_smb_mean_1117)\n",
    "        extreme_smb_sum_1117 = np.where(np.abs(extreme_smb_sum_1117)>500, np.nan, extreme_smb_sum_1117)\n",
    "        extreme_smb_mean_1824 = np.where(np.abs(extreme_smb_sum_1824)>500, np.nan, extreme_smb_mean_1824)\n",
    "        extreme_smb_sum_1824 = np.where(np.abs(extreme_smb_sum_1824)>500, np.nan, extreme_smb_sum_1824)\n",
    "\n",
    "        weight_0410 = np.array(weight_0410)\n",
    "        weight_1117 = np.array(weight_1117)\n",
    "        weight_1824 = np.array(weight_1824)\n",
    "        \n",
    "        # 计算统计量\n",
    "        results.loc['sum0410', region_name_clean] = weight_average(total_smb_sum_0410, weight_0410)\n",
    "        results.loc['sum1117', region_name_clean] = weight_average(total_smb_sum_1117, weight_1117)\n",
    "        results.loc['sum1824', region_name_clean] = weight_average(total_smb_sum_1824, weight_1824)\n",
    "        results.loc['avg0410', region_name_clean] = weight_average(total_smb_mean_0410, weight_0410)\n",
    "        results.loc['avg1117', region_name_clean] = weight_average(total_smb_mean_1117, weight_1117)\n",
    "        results.loc['avg1824', region_name_clean] = weight_average(total_smb_mean_1824, weight_1824)\n",
    "\n",
    "        results.loc['ext0410', region_name_clean] = weight_average(extreme_smb_sum_0410, weight_0410)\n",
    "        results.loc['ext1117', region_name_clean] = weight_average(extreme_smb_sum_1117, weight_1117)\n",
    "        results.loc['ext1824', region_name_clean] = weight_average(extreme_smb_sum_1824, weight_1824)\n",
    "        results.loc['avg_ext0410', region_name_clean] = weight_average(extreme_smb_mean_0410, weight_0410)\n",
    "        results.loc['avg_ext1117', region_name_clean] = weight_average(extreme_smb_mean_1117, weight_1117)\n",
    "        results.loc['avg_ext1824', region_name_clean] = weight_average(extreme_smb_mean_1824, weight_1824)\n",
    "        \n",
    "        print(f\"✓ 处理完成: {region_name_clean}\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = analyze_climate_extreme_impact(region_dirs)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_excel(r'E:\\revised_NCC_data\\OGGM model\\climate_score\\three_periods_smb_statistic.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 三个7年对比（04-10，11-17，18-24）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.T  # row_index: region_name\n",
    "results = results.sort_values(by='sum1824', ascending=True)\n",
    "\n",
    "results['per0410'] = results['ext0410']/results['sum0410']\n",
    "results['per1117'] = results['ext1117']/results['sum1117']\n",
    "results['per1824'] = results['ext1824']/results['sum1824']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_climate_extreme_comparison(results, save_path=None, display_legend=False):\n",
    "    \"\"\"\n",
    "    绘制气候极端事件对冰川质量平衡影响的对比图\n",
    "    \n",
    "    参数:\n",
    "        results_path: str, 结果数据Excel文件路径\n",
    "        save_path: str, 保存路径（可选）\n",
    "    \"\"\"\n",
    "\n",
    "    region_names = results.index\n",
    "    y_pos = np.arange(len(region_names))\n",
    "    \n",
    "    # 2. 创建图形\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 17), dpi=600)\n",
    "    \n",
    "    # 3. 绘制子图1：累积质量平衡对比\n",
    "    # 绘制总质量平衡（背景条）\n",
    "    ax1.barh(y_pos - 0.3, results['sum1824'], height=0.3, align='center',\n",
    "             color=plt.cm.Accent(3), edgecolor='k', label='2018-2024')\n",
    "    ax1.barh(y_pos, results['sum1117'], height=0.3, align='center',\n",
    "             color=plt.cm.Accent(2), edgecolor='k', label='2011-2017')\n",
    "    ax1.barh(y_pos + 0.3, results['sum0410'], height=0.3, align='center',\n",
    "             color=plt.cm.Accent(1), edgecolor='k', label='2004-2010')\n",
    "    \n",
    "    # 绘制极端年份质量平衡（前景条）\n",
    "    ax1.barh(y_pos - 0.3, results['ext1824'], height=0.3, align='center',\n",
    "             color=plt.cm.Accent(6), label='2018-2024 extreme years')\n",
    "    ax1.barh(y_pos, results['ext1117'], height=0.3, align='center',\n",
    "             color=plt.cm.Accent(5), label='2011-2017 extreme years')\n",
    "    ax1.barh(y_pos + 0.3, results['ext0410'], height=0.3, align='center',\n",
    "             color=plt.cm.Accent(4), label='2004-2010 extreme years')\n",
    "    \n",
    "    # 添加百分比标签（2018-2024）\n",
    "    for i, region_name in enumerate(region_names):\n",
    "        if (results.loc[region_name, 'per1824'] > 0) and (results.loc[region_name, 'per1824'] < 1) and (results.loc[region_name, 'ext1824'] > 0):\n",
    "            ax1.text(results.loc[region_name, 'ext1824'] + 1.2, y_pos[i] - 0.43,\n",
    "                    f'{results.loc[region_name, 'per1824']:.0%}',\n",
    "                    color=plt.cm.Accent(6), fontsize=14, fontweight='bold')\n",
    "        elif (results.loc[region_name, 'per1824'] > 0) and (results.loc[region_name, 'per1824'] < 1) and (results.loc[region_name, 'ext1824'] < 0):\n",
    "            ax1.text(results.loc[region_name, 'ext1824'] - 0.2, y_pos[i] - 0.43,\n",
    "                    f'{results.loc[region_name, 'per1824']:.0%}',\n",
    "                    color=plt.cm.Accent(6), fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # 添加百分比标签（2011-2017）\n",
    "    for i, region_name in enumerate(region_names):\n",
    "        if (results.loc[region_name, 'per1117'] > 0) and (results.loc[region_name, 'per1117'] < 1) and (results.loc[region_name, 'ext1117'] > 0):\n",
    "            ax1.text(results.loc[region_name, 'ext1117'] + 1.2, y_pos[i] - 0.13,\n",
    "                    f'{results.loc[region_name, 'per1117']:.0%}',\n",
    "                    color=plt.cm.Accent(5), fontsize=14, fontweight='bold')\n",
    "        elif (results.loc[region_name, 'per1117'] > 0) and (results.loc[region_name, 'per1117'] < 1) and (results.loc[region_name, 'ext1117'] < 0):\n",
    "            ax1.text(results.loc[region_name, 'ext1117'] - 0.15, y_pos[i] - 0.13,\n",
    "                    f'{results.loc[region_name, 'per1117']:.0%}',\n",
    "                    color=plt.cm.Accent(5), fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # 添加百分比标签（2004-2010）\n",
    "    for i, region_name in enumerate(region_names):\n",
    "        if (results.loc[region_name, 'per0410'] > 0) and (results.loc[region_name, 'per0410'] < 1) and (results.loc[region_name, 'ext0410'] > 0):\n",
    "            ax1.text(results.loc[region_name, 'ext0410'] + 1.2, y_pos[i] + 0.17,\n",
    "                    f'{results.loc[region_name, 'per0410']:.0%}',\n",
    "                    color=plt.cm.Accent(4), fontsize=14, fontweight='bold')\n",
    "        elif (results.loc[region_name, 'per0410'] > 0) and (results.loc[region_name, 'per0410'] < 1) and (results.loc[region_name, 'ext0410'] < 0):\n",
    "            ax1.text(results.loc[region_name, 'ext0410'] - 0.1, y_pos[i] + 0.17,\n",
    "                    f'{results.loc[region_name, 'per0410']:.0%}',\n",
    "                    color=plt.cm.Accent(4), fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # 设置子图1的坐标轴和标签\n",
    "    ax1.set_yticks(y_pos)\n",
    "    ax1.set_yticklabels(region_names, fontdict=dict(fontsize=16, color='black', \n",
    "                                              family='Arial', weight='normal', alpha=1.0))\n",
    "    ax1.set_xlabel('m w.e.', fontsize=18, labelpad=5)\n",
    "    ax1.tick_params(axis='x', labelsize=16)\n",
    "    ax1.set_title('a Cumulative mass balance',\n",
    "                 fontdict=dict(fontsize=18, color='black', family='Arial', \n",
    "                              weight='bold', alpha=1.0), pad=10)\n",
    "\n",
    "    # ax1.grid(linestyle='--', alpha=0.5, linewidth=0.8)\n",
    "    \n",
    "    # 添加分隔线\n",
    "    lim1 = ax1.get_xlim()\n",
    "    for i in range(len(region_names)+1):\n",
    "        ax1.hlines(i - 0.5, lim1[0], lim1[1], linestyles='dashed',\n",
    "                  linewidth=1.5, color='k', alpha=0.6, zorder=5)\n",
    "    # ax1.set_xlim(lim1[0], lim1[1])\n",
    "    ax1.set_ylim(-1, len(region_names))\n",
    "    ax1.invert_xaxis()\n",
    "    \n",
    "    # 4. 绘制子图2：极端年份平均质量平衡\n",
    "    hbars1 = ax2.barh(y_pos - 0.3, results['avg_ext1824'], height=0.3, align='center',\n",
    "                     color=plt.cm.Accent(6), label='2018-2024 extreme years')\n",
    "    hbars2 = ax2.barh(y_pos, results['avg_ext1117'], height=0.3, align='center',\n",
    "                     color=plt.cm.Accent(5), label='2011-2017 extreme years')\n",
    "    hbars3 = ax2.barh(y_pos + 0.3, results['avg_ext0410'], height=0.3, align='center',\n",
    "                     color=plt.cm.Accent(4), label='2004-2010 extreme years')\n",
    "    # 总体平均质量平衡\n",
    "    ax2.barh(y_pos - 0.3, results['avg1824'], height=0.3, align='center',\n",
    "                     color=plt.cm.Accent(3), edgecolor='k', label='2018-2024')\n",
    "    ax2.barh(y_pos, results['avg1117'], height=0.3, align='center',\n",
    "                     color=plt.cm.Accent(2), edgecolor='k', label='2011-2017')\n",
    "    ax2.barh(y_pos + 0.3, results['avg0410'], height=0.3, align='center',\n",
    "                     color=plt.cm.Accent(1), edgecolor='k', label='2004-2010')\n",
    "    \n",
    "    # # 添加标签（极端年份数量）\n",
    "    # ax2.bar_label(hbars1, labels=[f'{r:.0f}' for r in results['len1824']],\n",
    "    #              padding=-8, color=plt.cm.Accent(6), fontsize=14)\n",
    "    # ax2.bar_label(hbars2, labels=[f'{r:.0f}' for r in results['len1117']],\n",
    "    #              padding=-8, color=plt.cm.Accent(5), fontsize=14)\n",
    "    # ax2.bar_label(hbars3, labels=[f'{r:.0f}' for r in results['len0010']],\n",
    "    #              padding=-8, color=plt.cm.Accent(4), fontsize=14)\n",
    "    \n",
    "    # 设置子图2的坐标轴\n",
    "    ax2.yaxis.set_major_locator(MultipleLocator(1))\n",
    "    ax2.yaxis.set_visible(False)\n",
    "    ax2.set_xlabel('m w.e./yr', fontsize=18, labelpad=5)\n",
    "    ax2.tick_params(axis='x', labelsize=16)\n",
    "    ax2.set_title('b Mean mass balance',\n",
    "                 fontdict=dict(fontsize=18, color='black', family='Arial', \n",
    "                              weight='bold', alpha=1.0), pad=10)\n",
    "    # ax2.legend()\n",
    "    # ax2.grid(linestyle='--', alpha=0.5, linewidth=0.8)\n",
    "    \n",
    "    # 添加分隔线\n",
    "    lim2 = ax2.get_xlim()\n",
    "    for i in range(len(region_names)):\n",
    "        ax2.hlines(i - 0.5, lim2[0], lim2[1], linestyles='dashed',\n",
    "                  linewidth=1.5, color='k', alpha=0.6, zorder=5)\n",
    "    ax2.set_xlim(lim2[0], lim2[1])\n",
    "    ax2.set_ylim(-1, len(region_names))\n",
    "    ax2.invert_xaxis()\n",
    "\n",
    "    if display_legend:\n",
    "        ax1.legend(fontsize=16, ncol=3, \n",
    "            bbox_to_anchor=(0, -0.12, 2, 0.1),\n",
    "            loc='lower right',\n",
    "            mode='expand')\n",
    "    \n",
    "    # 5. 调整布局\n",
    "    plt.subplots_adjust(wspace=0.1)\n",
    "    \n",
    "    # 6. 保存或显示\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=600, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(CONFIG['output_dir'], 'three_period_compar_bar', 'three_period_compar_bar.pdf')\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "plot_climate_extreme_comparison(results, save_path=save_path, display_legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 前后十年对比散点图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_climate_extreme_impact2(region_dirs):\n",
    "    # 2. 初始化结果数据框\n",
    "    results = pd.DataFrame(\n",
    "        data=np.zeros((8, len(region_names))), \n",
    "        columns=region_names\n",
    "    )\n",
    "\n",
    "    results.index = [\n",
    "        'sum0514', 'sum1524',  # 三个时间段的总质量平衡\n",
    "        'avg0514', 'avg1524', \n",
    "        'ext0514', 'ext1524',  # 极端年份的质量平衡\n",
    "        'avg_ext0514', 'avg_ext1524'   # 极端年份数量\n",
    "    ]\n",
    "    \n",
    "    # 3. 处理每个区域\n",
    "    for region_dir in region_dirs:\n",
    "        region_name = region_dir.split(os.sep)[-1]\n",
    "        region_name_clean = region_name.replace('-','/') if '-' in region_name else region_name\n",
    "        region_name_clean = region_name_clean.split(']')[-1] # N/W Tien Shen\n",
    "\n",
    "        # 加载OGGM数据\n",
    "        ds = xr.open_dataset(\n",
    "            os.path.join(region_dir, 'run_output_hist_spinup_all.nc')\n",
    "        ).sel(time=slice(2000,2025))\n",
    "        \n",
    "        # 计算质量平衡\n",
    "        # smb = calculate_smb_from_volume(ds) #2000-2024\n",
    "        \n",
    "        # 合并数据\n",
    "        total_smb_sum_0514 = []\n",
    "        total_smb_mean_0514 = []\n",
    "        total_smb_sum_1524 = []\n",
    "        total_smb_mean_1524 = []\n",
    "\n",
    "        weight_0514 = []\n",
    "        weight_1524 = []\n",
    "\n",
    "        extreme_smb_sum_0514 = []\n",
    "        extreme_smb_mean_0514 = []\n",
    "        extreme_smb_sum_1524 = []\n",
    "        extreme_smb_mean_1524 = []\n",
    "        \n",
    "        # 计算极端年总质量平衡和平均质量平衡\n",
    "        for rgi_id in ds['rgi_id'].values:\n",
    "            rgi_all_score = score_all.loc[rgi_id]\n",
    "            rgi_smb = calculate_smb_from_volume(ds.sel(rgi_id=rgi_id)) #array，25个值\n",
    "            rgi_area = ds.sel(rgi_id=rgi_id)['area'].values\n",
    "\n",
    "            weight_0514.append(np.min(rgi_area[5:15]))\n",
    "            weight_1524.append(np.min(rgi_area[15:25]))\n",
    "\n",
    "            rgi_smb = np.where(np.abs(rgi_smb)>100, np.nan, rgi_smb)\n",
    "            total_smb_sum_0514.append(np.nansum(rgi_smb[5:15]))\n",
    "            total_smb_sum_1524.append(np.nansum(rgi_smb[15:]))\n",
    "            total_smb_mean_0514.append(np.nanmean(rgi_smb[5:15]))\n",
    "            total_smb_mean_1524.append(np.nanmean(rgi_smb[15:]))\n",
    "            # 极端年\n",
    "            extreme_year = np.array(rgi_all_score.nlargest(5).index.tolist()) # (2012,2013,2022)\n",
    "            \n",
    "            extreme_year_filtered = extreme_year[(extreme_year >= 2005) & (extreme_year <= 2014)]\n",
    "            if len(extreme_year_filtered) > 0:\n",
    "                temp_smb = rgi_smb[extreme_year_filtered-2000]\n",
    "                temp_sum_0514 = np.nansum(temp_smb)\n",
    "                temp_mean_0514 = np.nanmean(temp_smb)\n",
    "            else:\n",
    "                temp_sum_0514 = np.nan\n",
    "                temp_mean_0514 = np.nan\n",
    "            \n",
    "            extreme_year_filtered = extreme_year[(extreme_year >= 2015) & (extreme_year <= 2024)]\n",
    "            if len(extreme_year_filtered) > 0:\n",
    "                temp_smb = rgi_smb[extreme_year_filtered-2000]\n",
    "                temp_sum_1524 = np.nansum(temp_smb)\n",
    "                temp_mean_1524 = np.nanmean(temp_smb)\n",
    "            else:\n",
    "                temp_sum_1524 = np.nan\n",
    "                temp_mean_1524 = np.nan\n",
    "\n",
    "            extreme_smb_sum_0514.append(temp_sum_0514)\n",
    "            extreme_smb_mean_0514.append(temp_mean_0514)\n",
    "            extreme_smb_sum_1524.append(temp_sum_1524)\n",
    "            extreme_smb_mean_1524.append(temp_mean_1524)\n",
    "\n",
    "        total_smb_mean_0514 = np.where(np.abs(total_smb_sum_0514)>500, np.nan, total_smb_mean_0514)\n",
    "        total_smb_sum_0514 = np.where(np.abs(total_smb_sum_0514)>500, np.nan, total_smb_sum_0514)\n",
    "        total_smb_mean_1524 = np.where(np.abs(total_smb_sum_1524)>500, np.nan, total_smb_mean_1524)\n",
    "        total_smb_sum_1524 = np.where(np.abs(total_smb_sum_1524)>500, np.nan, total_smb_sum_1524)\n",
    "\n",
    "        extreme_smb_mean_0514 = np.where(np.abs(extreme_smb_sum_0514)>500, np.nan, extreme_smb_mean_0514)\n",
    "        extreme_smb_sum_0514 = np.where(np.abs(extreme_smb_sum_0514)>500, np.nan, extreme_smb_sum_0514)\n",
    "        extreme_smb_mean_1524 = np.where(np.abs(extreme_smb_sum_1524)>500, np.nan, extreme_smb_mean_1524)\n",
    "        extreme_smb_sum_1524 = np.where(np.abs(extreme_smb_sum_1524)>500, np.nan, extreme_smb_sum_1524)\n",
    "\n",
    "        weight_0514 = np.array(weight_0514)\n",
    "        weight_1524 = np.array(weight_1524)\n",
    "        \n",
    "        # 计算统计量\n",
    "        results.loc['sum0514', region_name_clean] = weight_average(total_smb_sum_0514, weight_0514)\n",
    "        results.loc['sum1524', region_name_clean] = weight_average(total_smb_sum_1524, weight_1524)\n",
    "        results.loc['avg0514', region_name_clean] = weight_average(total_smb_mean_0514, weight_0514)\n",
    "        results.loc['avg1524', region_name_clean] = weight_average(total_smb_mean_1524, weight_1524)\n",
    "\n",
    "        results.loc['ext0514', region_name_clean] = weight_average(extreme_smb_sum_0514, weight_0514)\n",
    "        results.loc['ext1524', region_name_clean] = weight_average(extreme_smb_sum_1524, weight_1524)\n",
    "        results.loc['avg_ext0514', region_name_clean] = weight_average(extreme_smb_mean_0514, weight_0514)\n",
    "        results.loc['avg_ext1524', region_name_clean] = weight_average(extreme_smb_mean_1524, weight_1524)\n",
    "        \n",
    "        print(f\"✓ 处理完成: {region_name_clean}\")\n",
    "\n",
    "    return results\n",
    "\n",
    "results = analyze_climate_extreme_impact2(region_dirs)\n",
    "\n",
    "results = results.T  # row_index: region_name\n",
    "results = results.sort_values(by='sum1524', ascending=True)\n",
    "\n",
    "results['per0514'] = results['ext0514']/results['sum0514']\n",
    "results['per1524'] = results['ext1524']/results['sum1524']\n",
    "\n",
    "results.to_excel(r'E:\\revised_NCC_data\\OGGM model\\climate_score\\two_period_smb_statistic.xlsx')\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_10yr_climate_extreme_scatter(results, save_path=None):\n",
    "    \"\"\"\n",
    "    绘制十年尺度气候极端事件对冰川质量平衡影响的散点图\n",
    "    \n",
    "    参数:\n",
    "        results_path: str, 结果数据Excel文件路径\n",
    "        save_path: str, 保存路径（可选）\n",
    "    \"\"\"\n",
    "    region_names = results.index\n",
    "    y_pos = np.arange(len(region_names))\n",
    "\n",
    "    # 2. 根据数值正负创建颜色列（向量化操作）\n",
    "    results['color_sum0514'] = ['red' if y < 0 else 'blue' for y in results['sum0514']]\n",
    "    results['color_sum1524'] = ['red' if y < 0 else 'blue' for y in results['sum1524']]\n",
    "    results['color_ext0514'] = ['red' if y < 0 else 'blue' for y in results['ext0514']]\n",
    "    results['color_ext1524'] = ['red' if y < 0 else 'blue' for y in results['ext1524']]\n",
    "    \n",
    "    label = ['Total', 'Extreme years']\n",
    "    \n",
    "    # 3. 创建图形\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(17, 22), dpi=600)\n",
    "    \n",
    "    # 4. 子图1：2004-2013累积质量平衡\n",
    "    ax1.scatter(np.zeros(len(region_names)), y_pos, \n",
    "               s=500*(np.abs(results['sum0514'])**0.5), \n",
    "               c=results['color_sum0514'], alpha=0.5)\n",
    "    ax1.scatter(np.zeros(len(region_names))+1, y_pos, \n",
    "               s=500*(np.abs(results['ext0514'])**0.5), \n",
    "               c=results['color_ext0514'], alpha=1)\n",
    "    \n",
    "    ax1.set_xticks([0, 1])\n",
    "    ax1.set_xticklabels(label, fontdict=dict(fontsize=18, color='black', \n",
    "                                             family='Arial', weight='bold', alpha=1.0))\n",
    "    ax1.tick_params(axis='x', which='major', pad=13, width=3, length=7)\n",
    "\n",
    "    ax1.set_yticks(y_pos)\n",
    "    ax1.set_yticklabels(region_names, fontdict=dict(fontsize=18, color='black', \n",
    "                                              family='Arial', weight='bold', alpha=1.0))\n",
    "    ax1.tick_params(axis='y', width=3, length=8, pad=5)\n",
    "\n",
    "    ax1.set_title('a Cumulative mass balance \\n (2005 - 2014, m w.e.)',\n",
    "                 fontdict=dict(fontsize=20, color='black', family='Arial', \n",
    "                              weight='bold', alpha=1.0), pad=15)\n",
    "    ax1.set_xlim(-0.5, 2.1)\n",
    "    ax1.set_ylim(-0.8, len(region_names)-0.2)\n",
    "    # ax1.grid(linestyle='--', alpha=0.5, linewidth=0.3)\n",
    "    ax1.invert_yaxis()\n",
    "    \n",
    "    # 添加数值标签和百分比\n",
    "    for i, region_name in enumerate(region_names):\n",
    "        ax1.text(0.2, y_pos[i]+0.15, f'{results.loc[region_name, 'sum0514']:.1f}',\n",
    "                color='k', fontsize=17, fontweight='bold')\n",
    "        ax1.text(1.2, y_pos[i]+0.15, f'{results.loc[region_name, 'ext0514']:.1f}',\n",
    "                color='k', fontsize=17, fontweight='bold')\n",
    "        if (results.loc[region_name, 'per0514'] > 0) and (results.loc[region_name, 'per0514'] < 1.5):\n",
    "            ax1.text(1.6, y_pos[i]+0.15, f'{results.loc[region_name, 'per0514']:.1%}',\n",
    "                    color='red', fontsize=17, fontweight='bold')\n",
    "    \n",
    "    # 5. 子图2：2014-2023累积质量平衡\n",
    "    ax2.scatter(np.zeros(len(region_names)), y_pos, \n",
    "               s=500*(np.abs(results['sum1524'])**0.5), \n",
    "               c=results['color_sum1524'], alpha=0.6)\n",
    "    ax2.scatter(np.zeros(len(region_names))+1, y_pos, \n",
    "               s=500*(np.abs(results['ext1524'])**0.5), \n",
    "               c=results['color_ext1524'], alpha=1)\n",
    "    \n",
    "    ax2.set_xticks([0, 1])\n",
    "    ax2.set_xticklabels(label, fontdict=dict(fontsize=18, color='black', \n",
    "                                             family='Arial', weight='bold', alpha=1.0))\n",
    "    ax2.tick_params(axis='x', which='major', pad=13, width=3, length=7)\n",
    "\n",
    "    ax2.set_yticks(y_pos)\n",
    "    ax2.set_yticklabels([])\n",
    "    ax2.tick_params(axis='y', width=3, length=8)\n",
    "    ax2.set_title('b Cumulative mass balance \\n (2015 - 2024, m w.e.)',\n",
    "                 fontdict=dict(fontsize=20, color='black', family='Arial', \n",
    "                              weight='bold', alpha=1.0), pad=15)\n",
    "    ax2.set_xlim(-0.5, 2.1)\n",
    "    ax2.set_ylim(-0.8, len(region_names)-0.2)\n",
    "    # ax2.grid(linestyle='--', alpha=0.5, linewidth=0.3)\n",
    "    ax2.invert_yaxis()\n",
    "    \n",
    "    # 添加数值标签和百分比\n",
    "    for i, region_name in enumerate(region_names):\n",
    "        ax2.text(0.2, y_pos[i]+0.15, f'{results.loc[region_name, 'sum1524']:.1f}',\n",
    "                color='k', fontsize=17, fontweight='bold')\n",
    "        ax2.text(1.2, y_pos[i]+0.15, f'{results.loc[region_name, 'ext1524']:.1f}',\n",
    "                color='k', fontsize=17, fontweight='bold')\n",
    "        if (results.loc[region_name, 'per1524'] > 0) and (results.loc[region_name, 'per1524'] < 1.5):\n",
    "            ax2.text(1.6, y_pos[i]+0.15, f'{results.loc[region_name, 'per1524']:.1%}',\n",
    "                    color='red', fontsize=17, fontweight='bold')\n",
    "    \n",
    "    # 6. 子图3：极端年份平均质量平衡\n",
    "    hbars1 = ax3.barh(y_pos-0.175, results['avg_ext0514'], height=0.3, align='center',\n",
    "                     color=plt.cm.Accent(1), label='2005-2014')\n",
    "    hbars2 = ax3.barh(y_pos+0.175, results['avg_ext1524'], height=0.3, align='center',\n",
    "                     color=plt.cm.Accent(5), label='2015-2024')\n",
    "    \n",
    "    ax3.invert_xaxis()\n",
    "    ax3.set_yticks(y_pos)\n",
    "    ax3.set_yticklabels([])\n",
    "    ax3.tick_params(axis='y', width=3, length=8)\n",
    "    ax3.tick_params(axis='x', width=3, length=7)\n",
    "    ax3.tick_params(labelsize=18)\n",
    "\n",
    "    # ax3.set_xlabel('Mean mass balance (m w.e./yr)', fontsize=16, \n",
    "    #                fontweight='bold', labelpad=5)\n",
    "    ax3.set_title('c Mean mass balance of\\n extreme years (m w.e./yr)',\n",
    "                 fontdict=dict(fontsize=20, color='black', family='Arial', \n",
    "                              weight='bold', alpha=1.0), pad=15)\n",
    "    # ax3.grid(linestyle='--', alpha=0.5, linewidth=0.3)\n",
    "    ax3.set_ylim(-0.8, len(region_names)-0.2)\n",
    "    ax3.invert_yaxis()\n",
    "    \n",
    "    # 添加标签（极端年份数量）\n",
    "#     ax3.bar_label(hbars1, labels=[f'{r:.0f}' for r in results['len0514']], \n",
    "#                  padding=-10, color='black', fontsize=16, fontweight='bold')\n",
    "#     ax3.bar_label(hbars2, labels=[f'{r:.0f}' for r in results['len1524']], \n",
    "#                  padding=-10, color='black', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 7. 保存或显示\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=600, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(CONFIG['output_dir'], 'two_period_compar_scatter', 'two_period_compar_scatter.pdf')\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "plot_10yr_climate_extreme_scatter(results, save_path=save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 未来气候投影对比（In[7]）\n",
    "\n",
    "对比OGGM不同投影场景与CMIP6（示例使用EC-Earth3 SSP245）的体积/面积/高程变化，评估一致性与不确定性。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_future_projection_comparison(region_dirs, plot_type='elevation', save_dir=None, display_legend=False):\n",
    "    \"\"\"\n",
    "    绘制未来气候投影对比图：历史模拟与多种未来情景的对比\n",
    "    \n",
    "    参数:\n",
    "        oggm_path: str, OGGM数据基础路径\n",
    "        plot_type: str, 绘制类型，可选 'elevation'（高程）、'volume'（体积）、'area'（面积）\n",
    "        save_dir: str, 保存目录（可选）\n",
    "    \"\"\"\n",
    "    # 定义文件列表\n",
    "    file_suffixes = [\n",
    "        'run_output_hist_spinup_all.nc',                    # ds1: 历史模拟\n",
    "        'run_output_cmip6_normal_spinup_all.nc', \n",
    "        'run_output_cmip6_repu_his_extremes_QDM_spinup_all.nc',\n",
    "    ]\n",
    "    cmip6_scenarios = ['ssp126', 'ssp245', 'ssp370', 'ssp585']\n",
    "\n",
    "    # 绘图配置\n",
    "    plot_configs = [\n",
    "        {'color': 'b', 'label': 'Historical simulation', 'time_slice': slice(0, 25)},\n",
    "        {'color': plt.cm.Accent(0), 'label': 'SSP126', 'time_slice': slice(25, 101)},\n",
    "        {'color': plt.cm.Accent(2), 'label': 'SSP245', 'time_slice': slice(25, 101)},\n",
    "        {'color': plt.cm.Accent(6), 'label': 'SSP370', 'time_slice': slice(25, 101)},\n",
    "        {'color': 'red', 'label': 'SSP585', 'time_slice': slice(25, 101)}\n",
    "    ]\n",
    "    \n",
    "    # 根据绘制类型设置参数\n",
    "    plot_params = {\n",
    "        'elevation': {\n",
    "            'unit_scale': 1.0,\n",
    "            'ylabel': 'Glacier elevation Change (m)',\n",
    "            'text_prefix': 'Elevation Change',\n",
    "            'calc_method': 'difference',  # elechange1 = ele2[0] - ele2[23]\n",
    "            'subfolder': 'pr_ele'\n",
    "        },\n",
    "        'volume': {\n",
    "            'unit_scale': 1e-9,  # 转换为 km³\n",
    "            'ylabel': 'Glacier volume (km³)',\n",
    "            'text_prefix': 'Volume loss',\n",
    "            'calc_method': 'percentage',  # ablation = 1 - vol2[23]/vol2[0]\n",
    "            'subfolder': 'pr_vol'\n",
    "        },\n",
    "        'area': {\n",
    "            'unit_scale': 1e-6,  # 转换为 km²\n",
    "            'ylabel': 'Glacier area (km²)',\n",
    "            'text_prefix': 'Area loss',\n",
    "            'calc_method': 'percentage',  # ablation = 1 - are2[23]/are2[0]\n",
    "            'subfolder': 'pr_area'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    params = plot_params[plot_type]\n",
    "    \n",
    "    for region_dir in region_dirs:\n",
    "        # 1. 加载所有数据集\n",
    "        datasets = []\n",
    "        std_datasets = [] # uncertainty, * sqrt(n)\n",
    "        std2_datasets = [] \n",
    "        plot_attrs = []\n",
    "        \n",
    "        base_file = os.path.join(region_dir, file_suffixes[0])\n",
    "        # hist file\n",
    "        ds_hist = xr.open_dataset(base_file).sel(time=slice(2000, 2025))\n",
    "        n = len(ds_hist.rgi_id)\n",
    "        ds_sum = ds_hist.sum(dim='rgi_id', skipna=True, keep_attrs=True)\n",
    "        ds_err = ds_hist.std(dim='rgi_id', skipna=True, keep_attrs=True) * np.sqrt(n)\n",
    "        ds_err2 = ds_hist.std(dim='rgi_id', skipna=True, keep_attrs=True)\n",
    "\n",
    "        datasets.append(ds_sum)\n",
    "        std_datasets.append(ds_err)\n",
    "        plot_attrs.append(plot_configs[0])\n",
    "        std2_datasets.append(ds_err2)\n",
    "        \n",
    "        # Load projection files\n",
    "        for suffix in file_suffixes[1:]:\n",
    "            file_path = os.path.join(region_dir, suffix)\n",
    "            ds_proj = xr.open_dataset(file_path).sel(time=slice(2000, 2101))\n",
    "            n = len(ds_proj.rgi_id)\n",
    "            \n",
    "            for ssp_i, ssp in enumerate(cmip6_scenarios):\n",
    "                ds_sum = ds_proj.sel(SSP=ssp).sum(dim='rgi_id', skipna=True, keep_attrs=True) #所有冰川的面积/体积\n",
    "                ds_err = ds_proj.sel(SSP=ssp).std(dim='rgi_id', skipna=True, keep_attrs=True) * np.sqrt(n) #所有冰川的不确定性，总体\n",
    "                ds_err2 = ds_proj.sel(SSP=ssp).std(dim='rgi_id', skipna=True, keep_attrs=True)\n",
    "\n",
    "                datasets.append(ds_sum)\n",
    "                std_datasets.append(ds_err)\n",
    "                plot_attrs.append(plot_configs[ssp_i+1])\n",
    "                std2_datasets.append(ds_err2)\n",
    "        \n",
    "        # 2. 根据绘制类型计算数据\n",
    "        data_list = []\n",
    "        error_list = []\n",
    "        \n",
    "        if plot_type == 'elevation':\n",
    "            # 计算质量平衡和高程变化\n",
    "            for ds, std in zip(datasets, std_datasets):            \n",
    "                # 计算质量平衡\n",
    "                smb = calculate_smb_from_volume(ds) #2000全年-2100全年，通量\n",
    "                ele = calculate_elevation_change(smb) #2001/1-2101/1\n",
    "\n",
    "                smb_err = (std['volume'].values[1:])**2 + (std['volume'].values[:-1])**2 + (smb * std['area'].values[1:])**2\n",
    "                smb_err = np.sqrt(smb_err) / ds['area'].values[1:] # m\n",
    "                smb_err = np.where((smb_err>1e2) | (np.isnan(smb_err)), np.nanmedian(smb_err), smb_err)\n",
    "\n",
    "                ele_err = smb_err/0.9 # ice elevation, m\n",
    "                # ele_err = np.sqrt(np.cumsum(ele_err**2))\n",
    "\n",
    "                data_list.append(ele)\n",
    "                error_list.append(ele_err)\n",
    "        \n",
    "        else:  # volume 或 area\n",
    "            var_name = 'volume' if plot_type == 'volume' else 'area'\n",
    "            for ds, std in zip(datasets, std_datasets):\n",
    "                # 提取数据并应用单位转换\n",
    "                data = ds[var_name][:-1].values * params['unit_scale'] #2000/1/1-2100/1/1，状态量\n",
    "                error = std[var_name][:-1].values * params['unit_scale']\n",
    "                \n",
    "                data_list.append(data)\n",
    "                error_list.append(error)\n",
    "        \n",
    "        region_name = region_dir.split(os.sep)[-1]\n",
    "        region_name_clean = region_name.replace('-','/') if '-' in region_name else region_name\n",
    "        region_name_clean = region_name_clean.split(']')[1]\n",
    "\n",
    "        # 3. 绘图\n",
    "        title = region_name_clean\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(5, 3), dpi=600)\n",
    "        \n",
    "        # 绘制所有情景\n",
    "        for i, (ds, data, error, config) in enumerate(zip(datasets, data_list, error_list, plot_attrs)):\n",
    "            time_slice = config['time_slice']\n",
    "\n",
    "            time_data = ds.time[time_slice]\n",
    "            data_plot = data[time_slice]\n",
    "            error_plot = error[time_slice]\n",
    "            \n",
    "            if (i==0) or (i>=5): # QDM\n",
    "                ax.fill_between(time_data, data_plot - error_plot, data_plot + error_plot,\n",
    "                                color=config['color'], alpha=0.15)\n",
    "                ax.plot(time_data, data_plot, color=config['color'], \n",
    "                        linewidth=3 if i == 0 else 2, label=config['label'], zorder=5)\n",
    "            else: # CMIP6 Normal\n",
    "                ax.fill_between(time_data, data_plot - error_plot, data_plot + error_plot,\n",
    "                                color=config['color'], alpha=0.15)\n",
    "                ax.plot(time_data, data_plot, color=config['color'], linestyle='--',\n",
    "                        linewidth=3 if i == 0 else 2, zorder=5)\n",
    "        \n",
    "        # 添加垂直线标记\n",
    "        bound = ax.get_ylim()\n",
    "        ax.vlines(2024, bound[0] + 0.7*(bound[1] - bound[0]), bound[1],\n",
    "                    linestyles='dashed', linewidth=1.5, color='k', zorder=5)\n",
    "        # ax.vlines(2100, bound[0], bound[1],\n",
    "        #             linestyles='dashed', linewidth=1.5, color='k', zorder=5)\n",
    "        ax.set_ylim(bound)\n",
    "        \n",
    "        # 添加50%线（仅对volume和area）\n",
    "        if plot_type in ['volume', 'area']:\n",
    "            half_value = data_list[0][0] / 2\n",
    "            ax.hlines(half_value, 2000, 2100, linestyles='dashed', \n",
    "                        linewidth=1.5, color=plt.cm.Accent(5), zorder=5)\n",
    "        \n",
    "        ax.tick_params(labelsize=14)\n",
    "        data2 = data_list[-1] #最严重的， SSP585，QDM\n",
    "        \n",
    "        if params['calc_method'] == 'difference':\n",
    "            # 高程变化：直接差值\n",
    "            change1 = -data2[24]\n",
    "            change2 = data2[24] - data2[99]\n",
    "            text1 = f\"{params['text_prefix']}$_{{2000-2024}}$ = {change1:.1f} m\"\n",
    "            text2 = f\"{params['text_prefix']}$_{{2025-2100}}$ = {change2:.1f} m\"\n",
    "        else:\n",
    "            # 体积/面积损失：百分比\n",
    "            ablation1 = 1 - data2[25] / data2[0]\n",
    "            ablation2 = (data2[25] - data2[100]) / data2[0]\n",
    "            text1 = f\"{params['text_prefix']}$_{{2000-2024}}$ = {ablation1:.1%}\"\n",
    "            text2 = f\"{params['text_prefix']}$_{{2025-2100}}$ = {ablation2:.1%}\"\n",
    "        \n",
    "        ax.text(0.05, 0.16, text1,\n",
    "                fontdict=dict(fontsize=15, color='k'), transform=ax.transAxes, zorder=6)\n",
    "        ax.text(0.05, 0.08, text2,\n",
    "                fontdict=dict(fontsize=15, color='k'), transform=ax.transAxes, zorder=6)\n",
    "        \n",
    "        # 设置标题和格式\n",
    "        ax.set_title(title, pad=5, fontdict=dict(fontsize=16, color='black', \n",
    "                                                family='Arial', weight='bold', alpha=1.0))\n",
    "        # ax.grid(axis='both', color='gray', linestyle=((0, (5, 10))), linewidth=0.3)\n",
    "        # ax.xaxis.set_major_locator(MultipleLocator(base=20))\n",
    "\n",
    "        if plot_type in ['volume', 'area']:\n",
    "            for i in range(1, len(datasets)):\n",
    "                datai = np.array(data_list[i])\n",
    "\n",
    "                if i < 5:\n",
    "                    print(f'{plot_attrs[i]['label']} (Normal):\\n'\n",
    "                     f'     2025-2050: {(datai[25] - datai[50]) / datai[0] *100:.2f} %;\\n'\n",
    "                     f'     2025-2100: {(datai[25] - datai[100]) / datai[0] *100:.2f} %')\n",
    "                else:\n",
    "                    print(f'{plot_attrs[i]['label']} (QDM):\\n'\n",
    "                     f'     2025-2050: {(datai[25] - datai[50]) / datai[0] *100:.2f} %;\\n'\n",
    "                     f'     2025-2100: {(datai[25] - datai[100]) / datai[0] *100:.2f} %')\n",
    "\n",
    "                try:\n",
    "                    critical_year = np.where(datai <= datai[0]*0.5)[0][0] + 2000\n",
    "                    print(f'    Year with half {plot_type}: {critical_year}')\n",
    "                except:\n",
    "                    print(f'    Year with half {plot_type}: None')\n",
    "                try:\n",
    "                    critical_year = np.where(datai <= datai[0]*0.25)[0][0] + 2000\n",
    "                    print(f'    Year with 25% {plot_type}: {critical_year}')\n",
    "                except:\n",
    "                    print(f'    Year with 25% {plot_type}: None')\n",
    "\n",
    "        if display_legend:\n",
    "            ax.legend(fontsize=18, ncol=2, \n",
    "                        bbox_to_anchor=(1.2, 1.2, 1.5, 2),\n",
    "                        loc='lower right',\n",
    "                        mode='expand')\n",
    "        \n",
    "        # 保存或显示\n",
    "        if save_dir:\n",
    "            if display_legend:\n",
    "                save_path = os.path.join(save_dir, 'legend', f'{region_name}_legend.pdf')\n",
    "            else:\n",
    "                save_path = os.path.join(save_dir, f'{region_name}.pdf')\n",
    "\n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "            plt.savefig(save_path, dpi=600, bbox_inches='tight')\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.show()\n",
    "        \n",
    "        # print(f'{region_name} processed!')\n",
    "\n",
    "        for ds in datasets:\n",
    "            if ds is not None:\n",
    "                ds.close()\n",
    "        for std in std_datasets:\n",
    "            if std is not None:\n",
    "                std.close()\n",
    "                \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.path.dirname(CONFIG['oggm_dir'])\n",
    "region_dirs = [os.path.join(base_dir, item) for item in os.listdir(base_dir)]\n",
    "save_dir = os.path.join(CONFIG['output_dir'], 'future_proj')\n",
    "\n",
    "plot_future_projection_comparison([region_dirs[3]], plot_type='elevation', save_dir=save_dir, display_legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.path.dirname(CONFIG['oggm_dir'])\n",
    "region_dirs = [os.path.join(base_dir, item) for item in os.listdir(base_dir)]\n",
    "\n",
    "plot_future_projection_comparison([region_dirs[3]], plot_type='volume', save_dir=None, display_legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.path.dirname(CONFIG['oggm_dir'])\n",
    "region_dirs = [os.path.join(base_dir, item) for item in os.listdir(base_dir)]\n",
    "\n",
    "plot_future_projection_comparison([region_dirs[3]], plot_type='area', save_dir=None, display_legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(CONFIG['output_dir'], 'future_proj', 'area')\n",
    "plot_future_projection_comparison(region_dirs, plot_type='area', save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(CONFIG['output_dir'], 'future_proj', 'volume')\n",
    "plot_future_projection_comparison(region_dirs, plot_type='volume', save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(CONFIG['output_dir'], 'future_proj', 'elevation')\n",
    "plot_future_projection_comparison(region_dirs, plot_type='elevation', save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1 环形图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_white_to_color_cmap(target_color, name='custom', N=20):\n",
    "    if isinstance(target_color, str):\n",
    "        target_rgb = mcolors.to_rgb(target_color)\n",
    "    else:\n",
    "        target_rgb = target_color[:3]\n",
    "\n",
    "    colors_list = [(1, 1, 1), target_rgb] # from white fading to target color\n",
    "    return mcolors.LinearSegmentedColormap.from_list(name, colors_list, N=N)\n",
    "\n",
    "# plot_configs = {\n",
    "#     'ssp126': create_white_to_color_cmap(plt.cm.Accent(0), 'ssp126_cmap'),\n",
    "#     'ssp245': create_white_to_color_cmap(plt.cm.Accent(2), 'ssp245_cmap'),\n",
    "#     'ssp370': create_white_to_color_cmap(plt.cm.Accent(6), 'ssp370_cmap'),\n",
    "#     'ssp585': create_white_to_color_cmap('red', 'ssp585_cmap'),\n",
    "# }\n",
    "plot_configs = {\n",
    "    'ssp126': create_white_to_color_cmap(plt.cm.Accent(0), 'ssp126_cmap'),\n",
    "    'ssp245': plt.cm.Blues,\n",
    "    'ssp370': create_white_to_color_cmap(plt.cm.Accent(6), 'ssp370_cmap'),\n",
    "    'ssp585': plt.cm.Reds,\n",
    "}\n",
    "\n",
    "presented_scenarios = ['ssp245', 'ssp585']\n",
    "presented_regions = region_dirs[3:4]\n",
    "presented_files = [\n",
    "    'run_output_cmip6_normal_spinup_all.nc', \n",
    "    'run_output_cmip6_repu_his_extremes_QDM_spinup_all.nc',\n",
    "]\n",
    "plot_var = 'volume'\n",
    "\n",
    "for region_dir in presented_regions:\n",
    "    presented_region_files = [os.path.join(region_dir, item) for item in presented_files]\n",
    "    region_name = region_dir.split(os.sep)[-1]\n",
    "    region_name_clean = region_name.replace('-','/') if '-' in region_name else region_name\n",
    "    region_name_clean = region_name_clean.split(']')[1]\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(5, 4), dpi=600, subplot_kw=dict(projection='polar'))\n",
    "\n",
    "    volume_matrix = []\n",
    "    plot_matrix = []\n",
    "    time_array = np.arange(2000, 2101)\n",
    "    for ssp in presented_scenarios:\n",
    "        for path in presented_region_files:\n",
    "            volume_array = xr.open_dataset(path).sum(\n",
    "                        dim='rgi_id', skipna=True, keep_attrs=True\n",
    "                    ).sel(time=slice(2000, 2100), SSP=ssp)[plot_var]\n",
    "            volume_matrix.append(volume_array.values) # 2000.1-2100.1\n",
    "            plot_matrix.append(plot_configs[ssp]) # [normal, extreme, normal, extreme, ...]\n",
    "    # volume percentage series\n",
    "    volume_percent_matrix = [array/array[0]*100 for array in volume_matrix]\n",
    "    \n",
    "    n_intervals = len(time_array) - 25 #76, 2025-2100, for ring display\n",
    "    theta = np.linspace(0, 2*np.pi, n_intervals, endpoint=False)\n",
    "    width = 2 * np.pi / n_intervals\n",
    "    norm = plt.Normalize(vmin=10, vmax=100)\n",
    "\n",
    "    # radius\n",
    "    r_inner_start = 0\n",
    "    ring_height = 0.25\n",
    "    r_outer = r_inner_start + len(volume_percent_matrix) * ring_height\n",
    "\n",
    "    for ring_i in range(len(volume_percent_matrix)):\n",
    "        r_inner = r_inner_start + ring_i * ring_height\n",
    "        percent_array = volume_percent_matrix[ring_i][25:] # 2025-2100\n",
    "        cmap = plot_matrix[ring_i]\n",
    "        colors = cmap(norm(percent_array))\n",
    "\n",
    "        bars = ax.bar(theta, ring_height, width=width, bottom=r_inner, \n",
    "                        color=colors, edgecolor='none', linewidth=0)\n",
    "        \n",
    "        idx_50 = np.where(percent_array <= 50)[0]\n",
    "        if len(idx_50) > 0:\n",
    "            ax.vlines(theta[idx_50[0]], r_inner, r_inner+ring_height, color='black', linewidth=1.5, linestyle='-', zorder=10)\n",
    "        \n",
    "        idx_25 = np.where(percent_array <= 25)[0]\n",
    "        if len(idx_25) > 0:\n",
    "            ax.vlines(theta[idx_25[0]], r_inner, r_inner+ring_height, color='black', linewidth=1.5, linestyle='--', zorder=10)\n",
    "        \n",
    "        if ring_i > 0:\n",
    "            ax.plot(np.linspace(0, 2*np.pi, 100), [r_inner]*100, color='k', linewidth=1, zorder=10, alpha=0.7)\n",
    "\n",
    "    ax.plot(np.linspace(0, 2*np.pi, 100), [r_outer]*100, color='k', linewidth=1.5, zorder=10)\n",
    "    ax.set_theta_zero_location('N')\n",
    "    ax.set_theta_direction(-1) # along clock\n",
    "    ax.set_yticks([])\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "    year_labels = np.arange(2025, 2100, 15)\n",
    "    label_angles = [theta[year-2025] for year in year_labels]\n",
    "    ax.set_xticks(label_angles)\n",
    "    ax.set_xticklabels(year_labels, fontsize=14)\n",
    "\n",
    "    tick_space = 0.08 \n",
    "    ax.set_rlim(0, r_outer + tick_space)\n",
    "    ax.tick_params(axis='x', pad=8)\n",
    "    ax.spines['polar'].set_visible(False)\n",
    "    for angle in label_angles:\n",
    "        ax.vlines(angle, r_outer, r_outer + tick_space, color='k', linewidth=1.5, zorder=10)\n",
    "\n",
    "    custom_cmap = plt.cm.Greys #create_white_to_color_cmap('k', N=20)\n",
    "    sm = plt.cm.ScalarMappable(cmap=custom_cmap, norm=norm)\n",
    "    sm.set_array([])\n",
    "    cbar_ax = fig.add_axes(rect=[1.1, -0.2, 0.05, 1.4])  # [left, bottom, width, height]\n",
    "    cbar = fig.colorbar(sm, cax=cbar_ax, shrink=0.6)\n",
    "    cbar.ax.set_ylim([0, 100])\n",
    "    cbar.ax.yaxis.set_major_locator(MultipleLocator(20))\n",
    "    cbar.set_label('Volume percentage (%)', fontsize=14)\n",
    "\n",
    "    # ax.set_title(region_name_clean, fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### by percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_white_to_color_cmap(target_color, name='custom', N=20):\n",
    "    if isinstance(target_color, str):\n",
    "        target_rgb = mcolors.to_rgb(target_color)\n",
    "    else:\n",
    "        target_rgb = target_color[:3]\n",
    "\n",
    "    colors_list = [(1, 1, 1), target_rgb] # from white fading to target color\n",
    "    return mcolors.LinearSegmentedColormap.from_list(name, colors_list, N=N)\n",
    "\n",
    "# plot_configs = {\n",
    "#     'ssp126': create_white_to_color_cmap(plt.cm.Accent(0), 'ssp126_cmap'),\n",
    "#     'ssp245': create_white_to_color_cmap(plt.cm.Accent(2), 'ssp245_cmap'),\n",
    "#     'ssp370': create_white_to_color_cmap(plt.cm.Accent(6), 'ssp370_cmap'),\n",
    "#     'ssp585': create_white_to_color_cmap('red', 'ssp585_cmap'),\n",
    "# }\n",
    "plot_configs = {\n",
    "    'ssp126': create_white_to_color_cmap(plt.cm.Accent(0), 'ssp126_cmap'),\n",
    "    'ssp245': plt.cm.Blues_r,\n",
    "    'ssp370': create_white_to_color_cmap(plt.cm.Accent(6), 'ssp370_cmap'),\n",
    "    'ssp585': plt.cm.Reds_r,\n",
    "}\n",
    "\n",
    "presented_scenarios = ['ssp245', 'ssp585']\n",
    "presented_regions = region_dirs[3:4]\n",
    "presented_files = [\n",
    "    'run_output_cmip6_normal_spinup_all.nc', \n",
    "    'run_output_cmip6_repu_his_extremes_QDM_spinup_all.nc',\n",
    "]\n",
    "plot_var = 'volume'\n",
    "\n",
    "for region_dir in presented_regions:\n",
    "    presented_region_files = [os.path.join(region_dir, item) for item in presented_files]\n",
    "    region_name = region_dir.split(os.sep)[-1]\n",
    "    region_name_clean = region_name.replace('-','/') if '-' in region_name else region_name\n",
    "    region_name_clean = region_name_clean.split(']')[1]\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(5, 4), dpi=600, subplot_kw=dict(projection='polar'))\n",
    "\n",
    "    volume_matrix = []\n",
    "    plot_matrix = []\n",
    "    time_array = np.arange(2000, 2101)\n",
    "    for ssp in presented_scenarios:\n",
    "        for path in presented_region_files:\n",
    "            volume_array = xr.open_dataset(path).sum(\n",
    "                        dim='rgi_id', skipna=True, keep_attrs=True\n",
    "                    ).sel(time=slice(2000, 2100), SSP=ssp)[plot_var]\n",
    "            volume_matrix.append(volume_array.values) # 2000.1-2100.1\n",
    "            plot_matrix.append(plot_configs[ssp]) # [normal, extreme, normal, extreme, ...]\n",
    "    # volume percentage series\n",
    "    volume_percent_matrix = [array/array[0]*100 for array in volume_matrix]\n",
    "    percent_cuts = np.arange(90, -0.5, -2) # 90, 87.5, ..., 0\n",
    "\n",
    "    n_intervals = len(percent_cuts)\n",
    "    theta = np.linspace(0, 2*np.pi, n_intervals, endpoint=False)\n",
    "    width = 2 * np.pi / n_intervals\n",
    "    norm = plt.Normalize(vmin=2025, vmax=2100)\n",
    "\n",
    "    # radius\n",
    "    r_inner_start = 0\n",
    "    ring_height = 0.25\n",
    "    r_outer = r_inner_start + len(volume_percent_matrix) * ring_height\n",
    "\n",
    "    for ring_i in range(len(volume_percent_matrix)):\n",
    "        r_inner = r_inner_start + ring_i * ring_height\n",
    "        percent_array = volume_percent_matrix[ring_i] # 2000-2100\n",
    "        year_array = []\n",
    "        for cut in percent_cuts:\n",
    "            idx = np.where(percent_array <= cut)[0]\n",
    "            if len(idx) > 0:\n",
    "                year_array.append(idx[0]+2000)\n",
    "            else:\n",
    "                year_array.append(np.nan)\n",
    "\n",
    "        year_array = np.array(year_array)\n",
    "        cmap = plot_matrix[ring_i]\n",
    "        colors = cmap(norm(year_array))\n",
    "        colors = np.where(np.isnan(year_array)[:, np.newaxis], [1, 1, 1, 1], colors)\n",
    "\n",
    "        bars = ax.bar(theta, ring_height, width=width, bottom=r_inner, \n",
    "                        color=colors, edgecolor='none', linewidth=0)\n",
    "        \n",
    "        idx_2050 = np.where(year_array >= 2050)[0]\n",
    "        ax.vlines(theta[idx_2050[0]], r_inner, r_inner+ring_height, color='black', linewidth=1.5, linestyle='-', zorder=10)\n",
    "        \n",
    "        idx_2075 = np.where(year_array >= 2075)[0]\n",
    "        ax.vlines(theta[idx_2075[0]], r_inner, r_inner+ring_height, color='black', linewidth=1.5, linestyle='--', zorder=10)\n",
    "        \n",
    "        if ring_i > 0:\n",
    "            ax.plot(np.linspace(0, 2*np.pi, 100), [r_inner]*100, color='k', linewidth=1, zorder=10, alpha=0.7)\n",
    "\n",
    "    ax.plot(np.linspace(0, 2*np.pi, 100), [r_outer]*100, color='k', linewidth=1.5, zorder=10)\n",
    "    ax.set_theta_zero_location('N')\n",
    "    ax.set_theta_direction(-1) # along clock\n",
    "    ax.set_yticks([])\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "    xlabels = [f'-{(100-item):.0f} %' for item in percent_cuts[::10]] #-10%, 30, 50\n",
    "    label_angles = theta[::10]\n",
    "    ax.set_xticks(label_angles)\n",
    "    ax.set_xticklabels(xlabels, fontsize=14)\n",
    "    \n",
    "    tick_space = 0.08 \n",
    "    ax.set_rlim(0, r_outer + tick_space)\n",
    "    ax.tick_params(axis='x', pad=8)\n",
    "    ax.spines['polar'].set_visible(False)\n",
    "    for angle in label_angles:\n",
    "        ax.vlines(angle, r_outer, r_outer + tick_space, color='k', linewidth=1.5, zorder=10)\n",
    "\n",
    "    custom_cmap = plt.cm.Greys_r #create_white_to_color_cmap('k', N=20)\n",
    "    sm = plt.cm.ScalarMappable(cmap=custom_cmap, norm=norm)\n",
    "    sm.set_array([])\n",
    "    cbar_ax = fig.add_axes(rect=[1.1, -0.2, 0.05, 1.4])  # [left, bottom, width, height]\n",
    "    cbar = fig.colorbar(sm, cax=cbar_ax, shrink=0.6)\n",
    "    cbar.ax.set_ylim([2025, 2100])\n",
    "    cbar.ax.yaxis.set_major_locator(MultipleLocator(15))\n",
    "    cbar.set_label('Year', fontsize=14)\n",
    "\n",
    "    # ax.set_title(region_name_clean, fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Batch process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_percent_ring(region_dirs, plot_var='volume', presented_scenarios = ['ssp245', 'ssp585'],\n",
    "                        save_dir=None, display_legend=False, display_ticklabel=False):\n",
    "    plot_configs = {\n",
    "        'ssp126': plt.cm.Greens_r,\n",
    "        'ssp245': plt.cm.Blues_r,\n",
    "        'ssp370': plt.cm.Oranges_r,\n",
    "        'ssp585': plt.cm.Reds_r,\n",
    "    }\n",
    "\n",
    "    presented_files = [\n",
    "        'run_output_cmip6_normal_spinup_all.nc', \n",
    "        'run_output_cmip6_repu_his_extremes_QDM_spinup_all.nc',\n",
    "    ]\n",
    "\n",
    "    for region_dir in region_dirs:\n",
    "        presented_region_files = [os.path.join(region_dir, item) for item in presented_files]\n",
    "        region_name = region_dir.split(os.sep)[-1]\n",
    "        region_name_clean = region_name.replace('-','/') if '-' in region_name else region_name\n",
    "        region_name_clean = region_name_clean.split(']')[1]\n",
    "\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(5, 4), dpi=600, subplot_kw=dict(projection='polar'))\n",
    "\n",
    "        volume_matrix = []\n",
    "        plot_matrix = []\n",
    "        time_array = np.arange(2000, 2101)\n",
    "        for ssp in presented_scenarios:\n",
    "            for path in presented_region_files:\n",
    "                volume_array = xr.open_dataset(path).sum(\n",
    "                            dim='rgi_id', skipna=True, keep_attrs=True\n",
    "                        ).sel(time=slice(2000, 2100), SSP=ssp)[plot_var]\n",
    "                volume_matrix.append(volume_array.values) # 2000.1-2100.1\n",
    "                plot_matrix.append(plot_configs[ssp]) # [normal, extreme, normal, extreme, ...]\n",
    "        # volume percentage series\n",
    "        volume_percent_matrix = [array/array[0]*100 for array in volume_matrix]\n",
    "        percent_cuts = np.arange(90, -0.5, -1) # 90, 88, ..., 0\n",
    "\n",
    "        n_intervals = len(percent_cuts)\n",
    "        theta = np.linspace(0, 2*np.pi, n_intervals, endpoint=False)\n",
    "        width = 2 * np.pi / n_intervals\n",
    "        norm = plt.Normalize(vmin=2025, vmax=2100)\n",
    "\n",
    "        # radius\n",
    "        r_inner_start = 0\n",
    "        ring_height = 0.25\n",
    "        r_outer = r_inner_start + len(volume_percent_matrix) * ring_height\n",
    "\n",
    "        for ring_i in range(len(volume_percent_matrix)):\n",
    "            r_inner = r_inner_start + ring_i * ring_height\n",
    "            percent_array = volume_percent_matrix[ring_i] # 2000-2100\n",
    "            year_array = []\n",
    "            for cut in percent_cuts:\n",
    "                idx = np.where(percent_array <= cut)[0]\n",
    "                if len(idx) > 0:\n",
    "                    year_array.append(idx[0]+2000)\n",
    "                else:\n",
    "                    year_array.append(np.nan)\n",
    "\n",
    "            year_array = np.array(year_array)\n",
    "            cmap = plot_matrix[ring_i]\n",
    "            colors = cmap(norm(year_array))\n",
    "            colors = np.where(np.isnan(year_array)[:, np.newaxis], [1, 1, 1, 1], colors)\n",
    "\n",
    "            bars = ax.bar(theta, ring_height, width=width, bottom=r_inner, \n",
    "                            color=colors, edgecolor='none', linewidth=0)\n",
    "            \n",
    "            idx_2050 = np.where(year_array >= 2050)[0]\n",
    "            ax.vlines(theta[idx_2050[0]], r_inner, r_inner+ring_height, color='black', linewidth=1.5, linestyle='-', zorder=10)\n",
    "            \n",
    "            idx_2075 = np.where(year_array >= 2075)[0]\n",
    "            ax.vlines(theta[idx_2075[0]], r_inner, r_inner+ring_height, color='black', linewidth=1.5, linestyle='--', zorder=10)\n",
    "            \n",
    "            if ring_i > 0:\n",
    "                ax.plot(np.linspace(0, 2*np.pi, 100), [r_inner]*100, color='k', linewidth=1, zorder=10, alpha=0.7)\n",
    "\n",
    "        ax.plot(np.linspace(0, 2*np.pi, 100), [r_outer]*100, color='k', linewidth=1.5, zorder=10)\n",
    "        ax.set_theta_zero_location('N')\n",
    "        ax.set_theta_direction(-1) # along clock\n",
    "        ax.set_yticks([])\n",
    "        ax.set_yticklabels([])\n",
    "\n",
    "        xlabels = [f'-{(100-item):.0f} %' for item in percent_cuts[::20]] #-10%, 30, 50\n",
    "        label_angles = theta[::20]\n",
    "        ax.set_xticks(label_angles)\n",
    "        if display_ticklabel:\n",
    "            ax.set_xticklabels(xlabels, fontsize=14)\n",
    "        else:\n",
    "            ax.set_xticklabels([])\n",
    "        \n",
    "        tick_space = 0.08 \n",
    "        ax.set_rlim(0, r_outer + tick_space)\n",
    "        ax.tick_params(axis='x', pad=8)\n",
    "        ax.spines['polar'].set_visible(False)\n",
    "        for angle in label_angles:\n",
    "            ax.vlines(angle, r_outer, r_outer + tick_space, color='k', linewidth=1.5, zorder=10)\n",
    "\n",
    "        if display_legend:\n",
    "            custom_cmap = plt.cm.Blues_r #create_white_to_color_cmap('k', N=20)\n",
    "            sm = plt.cm.ScalarMappable(cmap=custom_cmap, norm=norm)\n",
    "            sm.set_array([])\n",
    "            cbar_ax = fig.add_axes(rect=[-0.1, -0.2, 1.2, 0.05])  # [left, bottom, width, height]\n",
    "            cbar = fig.colorbar(sm, cax=cbar_ax, orientation='horizontal', extend='both')\n",
    "            cbar.ax.set_xlim([2025, 2100])\n",
    "            cbar.ax.xaxis.set_major_locator(MultipleLocator(15))\n",
    "            cbar.set_label('Year', fontsize=14)\n",
    "\n",
    "        # ax.set_title(region_name_clean, fontsize=14, fontweight='bold', pad=20)\n",
    "        if save_dir:\n",
    "            if display_legend:\n",
    "                save_path = os.path.join(save_dir, 'legend', f'{region_name}_legend.pdf')\n",
    "            else:\n",
    "                save_path = os.path.join(save_dir, f'{region_name}.pdf')\n",
    "\n",
    "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(save_path, dpi=600, bbox_inches='tight')\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.path.dirname(CONFIG['oggm_dir'])\n",
    "region_dirs = [os.path.join(base_dir, item) for item in os.listdir(base_dir)]\n",
    "save_dir = os.path.join(CONFIG['output_dir'], 'future_proj', 'ring_fig')\n",
    "\n",
    "plot_percent_ring([region_dirs[3]], save_dir=save_dir, display_legend=True, display_ticklabel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.path.dirname(CONFIG['oggm_dir'])\n",
    "region_dirs = [os.path.join(base_dir, item) for item in os.listdir(base_dir)]\n",
    "save_dir = os.path.join(CONFIG['output_dir'], 'future_proj', 'ring_fig')\n",
    "\n",
    "plot_percent_ring(region_dirs, save_dir=save_dir, display_legend=False, display_ticklabel=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Basemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 地理边界数据路径配置\n",
    "boundary_paths = {\n",
    "    'studyregion': \"E:/HMA/HMA_boundary/HMA_one.shp\",  # 研究区总边界\n",
    "    'subregion': \"E:/HMA_subregion/regions_hma_v03_zheng/boundary_mountain_regions_hma_v3_zheng_20200601.shp\",  # 子区域\n",
    "    'glacier': \"E:/HMA/Hugonnet/per_glacier/rgi_hma.shp\"  # 冰川边界（可选）\n",
    "}\n",
    "\n",
    "# 加载边界数据\n",
    "studyregion = gpd.read_file(boundary_paths['studyregion'])\n",
    "subregion = gpd.read_file(boundary_paths['subregion'])\n",
    "glacier = gpd.read_file(boundary_paths['glacier'])\n",
    "glacier['geometry'] = glacier.geometry.simplify(tolerance=0.1, preserve_topology=True)\n",
    "\n",
    "print(\"✓ 边界数据加载成功\")\n",
    "print(f\"  研究区边界: {len(studyregion)} 个要素\")\n",
    "print(f\"  子区域: {len(subregion)} 个\")\n",
    "print(f\"  冰川: {len(glacier)} 个\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_study_region_boundaries(\n",
    "    show_studyregion=False,\n",
    "    show_subregion=True,\n",
    "    show_glacier=False,\n",
    "    show_rivers=True,\n",
    "    show_lakes=True,\n",
    "    studyregion_color='darkred',\n",
    "    studyregion_linewidth=3,\n",
    "    subregion_color='maroon',\n",
    "    subregion_linewidth=2,\n",
    "    glacier_color='dodgerblue',\n",
    "    glacier_alpha=0.5,\n",
    "    basemap='arcgis_terrain',\n",
    "    figsize=(16, 12),\n",
    "    dpi=600,\n",
    "    title=None,\n",
    "    save_path=None\n",
    "):\n",
    "    \"\"\"\n",
    "    绘制高亚洲研究区边界地图（仅边界，无数据可视化）\n",
    "    \n",
    "    参数:\n",
    "        show_studyregion: bool, 是否显示研究区总边界\n",
    "        show_subregion: bool, 是否显示子区域边界\n",
    "        show_glacier: bool, 是否显示冰川边界\n",
    "        show_rivers: bool, 是否显示河流\n",
    "        show_lakes: bool, 是否显示湖泊\n",
    "        studyregion_color: str, 研究区总边界颜色\n",
    "        studyregion_linewidth: float, 研究区总边界线宽\n",
    "        subregion_color: str, 子区域边界颜色\n",
    "        subregion_linewidth: float, 子区域边界线宽\n",
    "        glacier_color: str, 冰川颜色\n",
    "        glacier_alpha: float, 冰川透明度\n",
    "        basemap: str, 底图类型 ('arcgis_terrain', 'arcgis_imagery', 'none')\n",
    "        figsize: tuple, 图形大小\n",
    "        dpi: int, 图形分辨率\n",
    "        title: str, 地图标题（可选）\n",
    "        save_path: str, 保存路径（可选）\n",
    "    \n",
    "    返回:\n",
    "        fig, ax: matplotlib图形对象\n",
    "    \"\"\"\n",
    "    # 底图配置\n",
    "    basemap_urls = {\n",
    "        'arcgis_terrain': 'https://server.arcgisonline.com/arcgis/rest/services/World_Terrain_Base/MapServer/tile/{z}/{y}/{x}.jpg',\n",
    "        'arcgis_imagery': 'https://server.arcgisonline.com/arcgis/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}.jpg',\n",
    "        'arcgis_shaded': 'https://server.arcgisonline.com/arcgis/rest/services/World_Shaded_Relief/MapServer/tile/{z}/{y}/{x}.jpg'\n",
    "    }\n",
    "    \n",
    "    # 创建图形\n",
    "    fig = plt.figure(figsize=figsize, dpi=dpi)\n",
    "    ax = fig.add_subplot(1, 1, 1, projection=ccrs.PlateCarree())\n",
    "    \n",
    "    # 设置地图范围（高亚洲区域）\n",
    "    extent = [63.6, 108.3, 24.5, 47.5]\n",
    "    ax.set_extent(extent, crs=ccrs.PlateCarree())\n",
    "    \n",
    "    # 添加底图\n",
    "    if basemap != 'none' and basemap in basemap_urls:\n",
    "        tiles = cimgt.GoogleTiles(url=basemap_urls[basemap])\n",
    "        ax.add_image(tiles, 9)\n",
    "    \n",
    "    # 添加自然地理要素\n",
    "    if show_rivers:\n",
    "        ax.add_feature(cfeat.RIVERS.with_scale('10m'), \n",
    "                      linewidth=1.5, zorder=1, alpha=0.6, color='steelblue')\n",
    "    if show_lakes:\n",
    "        ax.add_feature(cfeat.LAKES.with_scale('10m'), \n",
    "                      zorder=1, alpha=0.6, facecolor='lightblue', edgecolor='steelblue')\n",
    "    \n",
    "    # 绘制冰川（如果需要，在边界之前绘制）\n",
    "    if show_glacier:\n",
    "        glacier.plot(ax=ax, color=glacier_color, edgecolor=glacier_color, \n",
    "                    linewidth=0.2, alpha=glacier_alpha, zorder=2)\n",
    "    \n",
    "    # 绘制子区域边界\n",
    "    if show_subregion:\n",
    "        subregion.plot(ax=ax, facecolor=\"none\", edgecolor=subregion_color, \n",
    "                      linewidth=subregion_linewidth, zorder=5)\n",
    "    \n",
    "    # 绘制研究区总边界（在最上层）\n",
    "    if show_studyregion:\n",
    "        studyregion.plot(ax=ax, facecolor=\"none\", edgecolor=studyregion_color, \n",
    "                        linewidth=studyregion_linewidth, zorder=6, linestyle='-')\n",
    "    \n",
    "    # 添加网格线\n",
    "    #ax.grid(True, linestyle='--', linewidth=0.5, alpha=0.3, color='gray', zorder=0)\n",
    "    \n",
    "    # 设置坐标轴\n",
    "    # ax.set_xticks(np.arange(65, extent[1], 5), crs=ccrs.PlateCarree())\n",
    "    # ax.set_yticks(np.arange(26, extent[3], 3), crs=ccrs.PlateCarree())\n",
    "    # ax.xaxis.set_major_formatter(LongitudeFormatter())\n",
    "    # ax.yaxis.set_major_formatter(LatitudeFormatter())\n",
    "    # ax.tick_params(axis='both', labelsize=15, direction='out', pad=5)\n",
    "    # ax.tick_params(axis='y', labelrotation=0)\n",
    "    # ax.tick_params(axis='x', labelrotation=0)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.axis('off') \n",
    "    \n",
    "    # 添加标题\n",
    "    if title:\n",
    "        ax.set_title(title, fontsize=22, pad=20, family='Arial', weight='bold')\n",
    "    \n",
    "    # 保存图形\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=dpi, bbox_inches='tight', \n",
    "                   facecolor='white', edgecolor='none')\n",
    "    \n",
    "    return fig, ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_study_region_boundaries(\n",
    "    show_studyregion=False,\n",
    "    show_subregion=True,\n",
    "    show_glacier=True,          # 显示冰川\n",
    "    show_rivers=True,\n",
    "    show_lakes=True,\n",
    "    subregion_color='maroon',\n",
    "    subregion_linewidth=2,\n",
    "    glacier_color='dodgerblue', # 冰川用蓝色\n",
    "    glacier_alpha=0.5,          # 半透明\n",
    "    basemap='arcgis_terrain',\n",
    "    figsize=(18, 14),\n",
    "    dpi=600,\n",
    "    save_path=os.path.join(CONFIG['output_dir'], 'future_proj', 'ring_fig', 'basemap', 'basemap.pdf')\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 非线性关系分析（In[11]）\n",
    "\n",
    "分析气候极端强度与冰川质量平衡之间的非线性关系，支持多项式拟合与上下界估计。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_nonlinear_relationship(region_dirs, score, \n",
    "                                   poly_degree=2,  # 可配置多项式阶数\n",
    "                                   factor=2,\n",
    "                                   remove_last_n=0,  # 去除最后n个点\n",
    "                                   display_legend=False,\n",
    "                                   show_mean_poly=True,\n",
    "                                   save_path=None):\n",
    "    \"\"\"\n",
    "    分析气候极端强度与冰川质量平衡之间的非线性关系\n",
    "    \n",
    "    参数:\n",
    "        region_dirs: list\n",
    "        score_path: str, 极端强度数据Excel文件路径\n",
    "        poly_degree: int, 多项式拟合阶数（建议2或3）\n",
    "        factor: float, 标准差倍数（用于计算上下界）\n",
    "        remove_last_n: int, 去除最后n个极端值点（避免不稳定）\n",
    "        save_path: str, 保存路径（可选）\n",
    "    \"\"\"\n",
    "    # 1. 加载数据\n",
    "    smb_df = pd.DataFrame({}, columns=np.arange(2000, 2025))\n",
    "    score_df = pd.DataFrame({}, columns=np.arange(2000, 2025))\n",
    "    for region_dir in region_dirs:\n",
    "        file_path = os.path.join(region_dir, 'run_output_hist_spinup_all.nc')\n",
    "        ds = xr.open_dataset(file_path).sel(time=slice(2000,2025))\n",
    "        ds_total = ds.sum(dim='rgi_id', skipna=True, keep_attrs=True)\n",
    "\n",
    "        region_name = region_dir.split(os.sep)[-1]\n",
    "        region_name_clean = region_name.replace('-','/') if '-' in region_name else region_name\n",
    "        region_name_clean = region_name_clean.split(']')[1]\n",
    "\n",
    "        smb = calculate_smb_from_volume(ds_total)\n",
    "        smb_df.loc[region_name_clean] = smb #每个子区域，mb from 2000 to 2024\n",
    "\n",
    "        # aggregrate score to subregions\n",
    "        rgi_ids = ds['rgi_id'].values\n",
    "        selected_score = score.loc[rgi_ids]\n",
    "        for year in range(2000, 2025):\n",
    "            area_weight = ds.sel(time=year)['area'].values\n",
    "            # score_df.loc[region_name_clean, year] = weight_average(selected_score[year].values, area_weight)#面积加权\n",
    "            score_df.loc[region_name_clean, year] = selected_score[year].median()\n",
    "\n",
    "        ds.close()\n",
    "        print(f'{region_name_clean} done!')\n",
    "\n",
    "    smb_df = smb_df.T # columns: regions\n",
    "    score_df = score_df.T\n",
    "\n",
    "    # 2. 合并数据\n",
    "    n_regions = len(smb_df.columns)\n",
    "    n_years = len(smb_df.index)\n",
    "    region_names = smb_df.columns.tolist()\n",
    "    data = np.zeros([n_regions * n_years, 2])\n",
    "    \n",
    "    for i in range(n_regions):\n",
    "        for j in range(n_years):\n",
    "            k = n_regions * i + j\n",
    "            data[k][0] = score_df.loc[2000+j, region_names[i]]\n",
    "            data[k][1] = smb_df.loc[2000+j, region_names[i]]\n",
    "    \n",
    "    # # 3. OLS线性回归（用于参考）\n",
    "    # res = sm.OLS(data[:, 1], data[:, 0]).fit()\n",
    "    # pred_ols = res.get_prediction()\n",
    "    # iv_l = pred_ols.summary_frame()[\"obs_ci_lower\"]\n",
    "    # iv_u = pred_ols.summary_frame()[\"obs_ci_upper\"]\n",
    "    \n",
    "    # 4. 计算唯一值和动态范围\n",
    "    ind = np.sort(np.unique(data[:, 0]))\n",
    "    imax = np.max(ind) + 1\n",
    "    x_fit = np.arange(0, imax, 0.5) # np.max(ind) + 0.5\n",
    "    \n",
    "    # 5. 多项式拟合\n",
    "    # 线性拟合\n",
    "    p1 = np.polyfit(data[:, 0], data[:, 1], 1)\n",
    "    y1 = np.polyval(p1, x_fit)\n",
    "    \n",
    "    # 多项式拟合（可配置阶数）\n",
    "    p2 = np.polyfit(data[:, 0], data[:, 1], poly_degree)\n",
    "    y2 = np.polyval(p2, x_fit)\n",
    "    \n",
    "    # 6. 计算每个极端强度值下的统计量\n",
    "    mean = np.zeros(len(ind)) # 和ind对应\n",
    "    std = np.zeros(len(ind))\n",
    "    \n",
    "    for i in range(len(ind)):\n",
    "        mask = data[:, 0] == ind[i]\n",
    "        mean[i] = np.mean(data[mask, 1])\n",
    "        std[i] = np.std(data[mask, 1])\n",
    "    \n",
    "    # 7. 拟合上下界（基于统计）\n",
    "    # 上界：mean + factor*std\n",
    "    if remove_last_n > 0 and len(ind) > remove_last_n:\n",
    "        p3 = np.polyfit(ind[0:-remove_last_n], \n",
    "                        mean[0:-remove_last_n] + factor * std[0:-remove_last_n], 2)\n",
    "    else:\n",
    "        p3 = np.polyfit(ind, mean + factor * std, 2)\n",
    "    y3 = np.polyval(p3, x_fit)\n",
    "    \n",
    "    # 下界：mean - factor*std（去除最后n个点，因为极端值可能不稳定）\n",
    "    if remove_last_n > 0 and len(ind) > remove_last_n:\n",
    "        p4 = np.polyfit(ind[0:-remove_last_n], \n",
    "                       mean[0:-remove_last_n] - factor * std[0:-remove_last_n], 2)\n",
    "    else:\n",
    "        p4 = np.polyfit(ind, mean - factor * std, 2)\n",
    "    y4 = np.polyval(p4, x_fit)\n",
    "    \n",
    "    # 均值线\n",
    "    if remove_last_n > 0 and len(ind) > remove_last_n:\n",
    "        p5 = np.polyfit(ind[0:-remove_last_n], mean[0:-remove_last_n], 2)\n",
    "    else:\n",
    "        p5 = np.polyfit(ind, mean, 2)\n",
    "    y5 = np.polyval(p5, x_fit)\n",
    "    \n",
    "    # 8. 计算相关系数（使用多项式拟合）\n",
    "    cc = np.corrcoef(data[:, 0], data[:, 1])[0, 1]\n",
    "    \n",
    "    # 9. 绘图\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 4), dpi=300)\n",
    "    \n",
    "    # 绘制散点图\n",
    "    for region_name in region_names:\n",
    "        ax.scatter(score_df[region_name], smb_df[region_name], s=30)\n",
    "    \n",
    "    # 绘制拟合曲线,整体拟合\n",
    "    ax.plot(x_fit, y1, linestyle='--', linewidth=5, label='Linear fitting', color='k')\n",
    "    if show_mean_poly:\n",
    "        ax.plot(x_fit, y5, linestyle='-', linewidth=5, label='Polynomial fitting', color='orange')\n",
    "    \n",
    "    # 绘制上下界（使用统计拟合的下界，而不是固定偏移）\n",
    "    ax.plot(x_fit, y3, linestyle='-', linewidth=3, color='b', label='Upper and lower bound')\n",
    "    ax.plot(x_fit, y4, linestyle='-', linewidth=3, color='b')\n",
    "    \n",
    "    # 添加相关系数文本\n",
    "    ax.text(0.7, 0.80, f'$CC$ = {cc:.2f}',\n",
    "           fontdict=dict(fontsize=18, family='Arial', color='k'), \n",
    "           transform=ax.transAxes)\n",
    "    \n",
    "    # 设置坐标轴\n",
    "    ax.tick_params(axis='both', labelsize=16)\n",
    "    ax.set_ylabel('Mass balance (m w.e./yr)', fontsize=18, labelpad=2)\n",
    "    ax.set_xlabel('Extreme intensity ', \n",
    "                 fontsize=18, labelpad=2)\n",
    "    # ax.xaxis.set_major_locator(MultipleLocator(base=1))\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.5, linewidth=0.3)\n",
    "    ax.grid(axis='x', linestyle='--', alpha=1, linewidth=0.8)\n",
    "    \n",
    "    if display_legend:\n",
    "        ax.legend(fontsize=20, ncol=3, \n",
    "                    bbox_to_anchor=(1.2, 1.2, 3, 2),\n",
    "                    loc='lower right',\n",
    "                    mode='expand')\n",
    "    \n",
    "    # 保存或显示\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=600, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.show()\n",
    "    \n",
    "    # 返回结果\n",
    "    results = {\n",
    "        'data': data,\n",
    "        'cc': cc,\n",
    "        'unique_intensities': ind,\n",
    "        'mean_by_intensity': mean,\n",
    "        'std_by_intensity': std,\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(CONFIG['output_dir'], 'nonlinear_median')\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "save_path = os.path.join(save_dir, 'nonlinear_all_legend.pdf')\n",
    "stat_summary = analyze_nonlinear_relationship(region_dirs, score_all, \n",
    "                                                poly_degree=2,\n",
    "                                                factor=2,\n",
    "                                                remove_last_n=2,\n",
    "                                                display_legend=True,\n",
    "                                                show_mean_poly=True,\n",
    "                                                save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(CONFIG['output_dir'], 'nonlinear_median')\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "save_path = os.path.join(save_dir, 'nonlinear_all.pdf')\n",
    "stat_summary = analyze_nonlinear_relationship(region_dirs, score_all, \n",
    "                                                poly_degree=2,\n",
    "                                                factor=2,\n",
    "                                                remove_last_n=2,\n",
    "                                                display_legend=False,\n",
    "                                                show_mean_poly=True,\n",
    "                                                save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(CONFIG['output_dir'], 'nonlinear_median')\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "save_path = os.path.join(save_dir, 'nonlinear_temp.pdf')\n",
    "stat_summary = analyze_nonlinear_relationship(region_dirs, score_temp, \n",
    "                                                poly_degree=2,\n",
    "                                                factor=2,\n",
    "                                                remove_last_n=0,\n",
    "                                                display_legend=False,\n",
    "                                                show_mean_poly=True,\n",
    "                                                save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(CONFIG['output_dir'], 'nonlinear_median')\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "save_path = os.path.join(save_dir, 'nonlinear_prcp.pdf')\n",
    "stat_summary = analyze_nonlinear_relationship(region_dirs, score_prcp, \n",
    "                                                poly_degree=2,\n",
    "                                                factor=2,\n",
    "                                                remove_last_n=2,\n",
    "                                                display_legend=False,\n",
    "                                                show_mean_poly=True,\n",
    "                                                save_path=save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 面积加权尝试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_nonlinear_relationship(region_dirs, score, \n",
    "                                   poly_degree=2,  # 可配置多项式阶数\n",
    "                                   factor=2,\n",
    "                                   remove_last_n=0,  # 去除最后n个点\n",
    "                                   display_legend=False,\n",
    "                                   show_mean_poly=True,\n",
    "                                   save_path=None):\n",
    "    # 1. 加载数据\n",
    "    smb_df = pd.DataFrame({}, columns=np.arange(2000, 2025))\n",
    "    score_df = pd.DataFrame({}, columns=np.arange(2000, 2025))\n",
    "    for region_dir in region_dirs:\n",
    "        file_path = os.path.join(region_dir, 'run_output_hist_spinup_all.nc')\n",
    "        ds = xr.open_dataset(file_path).sel(time=slice(2000,2025))\n",
    "        ds_total = ds.sum(dim='rgi_id', skipna=True, keep_attrs=True)\n",
    "\n",
    "        region_name = region_dir.split(os.sep)[-1]\n",
    "        region_name_clean = region_name.replace('-','/') if '-' in region_name else region_name\n",
    "        region_name_clean = region_name_clean.split(']')[1]\n",
    "\n",
    "        smb = calculate_smb_from_volume(ds_total)\n",
    "        smb_df.loc[region_name_clean] = smb #每个子区域，mb from 2000 to 2024\n",
    "\n",
    "        # aggregrate score to subregions\n",
    "        rgi_ids = ds['rgi_id'].values\n",
    "        selected_score = score.loc[rgi_ids]\n",
    "        for year in range(2000, 2025):\n",
    "            area_weight = ds.sel(time=year)['area'].values\n",
    "            score_df.loc[region_name_clean, year] = weight_average(selected_score[year].values, area_weight)#面积加权\n",
    "            # score_df.loc[region_name_clean, year] = selected_score[year].median()\n",
    "\n",
    "        ds.close()\n",
    "        # print(f'{region_name_clean} done!')\n",
    "\n",
    "    smb_df = smb_df.T # columns: regions\n",
    "    score_df = score_df.T\n",
    "\n",
    "    # 2. 合并数据\n",
    "    n_regions = len(smb_df.columns)\n",
    "    n_years = len(smb_df.index)\n",
    "    region_names = smb_df.columns.tolist()\n",
    "    data = np.zeros([n_regions * n_years, 2])\n",
    "    \n",
    "    for i in range(n_regions):\n",
    "        for j in range(n_years):\n",
    "            k = n_regions * i + j\n",
    "            data[k][0] = score_df.loc[2000+j, region_names[i]]\n",
    "            data[k][1] = smb_df.loc[2000+j, region_names[i]]\n",
    "    \n",
    "    # # 3. OLS线性回归（用于参考）\n",
    "    # res = sm.OLS(data[:, 1], data[:, 0]).fit()\n",
    "    # pred_ols = res.get_prediction()\n",
    "    # iv_l = pred_ols.summary_frame()[\"obs_ci_lower\"]\n",
    "    # iv_u = pred_ols.summary_frame()[\"obs_ci_upper\"]\n",
    "    \n",
    "    # 4. 计算范围\n",
    "    imax = ceil(np.max(data[:, 0]))\n",
    "    x_fit = np.arange(0, imax, 0.2) # np.max(ind) + 0.5\n",
    "    \n",
    "    # 5. 多项式拟合\n",
    "    # 线性拟合\n",
    "    p1 = np.polyfit(data[:, 0], data[:, 1], 1)\n",
    "    y1 = np.polyval(p1, x_fit)\n",
    "    \n",
    "    # 多项式拟合（可配置阶数）\n",
    "    p2 = np.polyfit(data[:, 0], data[:, 1], poly_degree)\n",
    "    y2 = np.polyval(p2, x_fit)\n",
    "    \n",
    "    # 6. 计算每个极端强度值下的统计量\n",
    "    mean = []\n",
    "    std = []\n",
    "    ind = []\n",
    "    for i in range(imax):\n",
    "        idx = (data[:, 0] >= i) & (data[:, 0] < i+1)\n",
    "        if np.sum(idx) > 5:\n",
    "            mean.append(np.mean(data[idx, 1]))\n",
    "            std.append(np.std(data[idx, 1]))\n",
    "            ind.append(np.mean(data[idx, 0]))\n",
    "\n",
    "    mean = np.array(mean)\n",
    "    std = np.array(std)\n",
    "    ind = np.array(ind)\n",
    "    # 7. 拟合上下界（基于统计）\n",
    "    # 上界：mean + factor*std\n",
    "    if remove_last_n > 0 and len(ind) > remove_last_n:\n",
    "        p3 = np.polyfit(ind[0:-remove_last_n], \n",
    "                        mean[0:-remove_last_n] + factor * std[0:-remove_last_n], 2)\n",
    "    else:\n",
    "        p3 = np.polyfit(ind, mean + factor * std, 2)\n",
    "    y3 = np.polyval(p3, x_fit)\n",
    "    \n",
    "    # 下界：mean - factor*std（去除最后n个点，因为极端值可能不稳定）\n",
    "    if remove_last_n > 0 and len(ind) > remove_last_n:\n",
    "        p4 = np.polyfit(ind[0:-remove_last_n], \n",
    "                       mean[0:-remove_last_n] - factor * std[0:-remove_last_n], 2)\n",
    "    else:\n",
    "        p4 = np.polyfit(ind, mean - factor * std, 2)\n",
    "    y4 = np.polyval(p4, x_fit)\n",
    "    \n",
    "    # 均值线\n",
    "    if remove_last_n > 0 and len(ind) > remove_last_n:\n",
    "        p5 = np.polyfit(ind[0:-remove_last_n], mean[0:-remove_last_n], 2)\n",
    "    else:\n",
    "        p5 = np.polyfit(ind, mean, 2)\n",
    "    y5 = np.polyval(p5, x_fit)\n",
    "    \n",
    "    # 8. 计算相关系数（使用多项式拟合）\n",
    "    cc = np.corrcoef(data[:, 0], data[:, 1])[0, 1]\n",
    "    \n",
    "    # 9. 绘图\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 4), dpi=300)\n",
    "    \n",
    "    # 绘制散点图\n",
    "    for region_name in region_names:\n",
    "        ax.scatter(score_df[region_name], smb_df[region_name], s=30)\n",
    "    \n",
    "    # 绘制拟合曲线,整体拟合\n",
    "    ax.plot(x_fit, y1, linestyle='--', linewidth=5, label='Linear fitting', color='k')\n",
    "    if show_mean_poly:\n",
    "        ax.plot(x_fit, y5, linestyle='-', linewidth=5, label='Polynomial fitting', color='orange')\n",
    "    \n",
    "    # 绘制上下界（使用统计拟合的下界，而不是固定偏移）\n",
    "    ax.plot(x_fit, y3, linestyle='-', linewidth=3, color='b', label='Upper and lower bound')\n",
    "    ax.plot(x_fit, y4, linestyle='-', linewidth=3, color='b')\n",
    "    \n",
    "    # 添加相关系数文本\n",
    "    ax.text(0.7, 0.80, f'$CC$ = {cc:.2f}',\n",
    "           fontdict=dict(fontsize=18, family='Arial', color='k'), \n",
    "           transform=ax.transAxes)\n",
    "    \n",
    "    # 设置坐标轴\n",
    "    ax.tick_params(axis='both', labelsize=16)\n",
    "    ax.set_ylabel('Mass balance (m w.e./yr)', fontsize=18, labelpad=2)\n",
    "    ax.set_xlabel('Extreme intensity ', \n",
    "                 fontsize=18, labelpad=2)\n",
    "    # ax.xaxis.set_major_locator(MultipleLocator(base=1))\n",
    "    ax.grid(axis='y', linestyle='--', alpha=0.5, linewidth=0.3)\n",
    "    ax.grid(axis='x', linestyle='--', alpha=1, linewidth=0.8)\n",
    "    \n",
    "    if display_legend:\n",
    "        ax.legend(fontsize=20, ncol=3, \n",
    "                    bbox_to_anchor=(1.2, 1.2, 3, 2),\n",
    "                    loc='lower right',\n",
    "                    mode='expand')\n",
    "    \n",
    "    # 保存或显示\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=600, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.show()\n",
    "    \n",
    "    # 返回结果\n",
    "    results = {\n",
    "        'data': data,\n",
    "        'cc': cc,\n",
    "        'unique_intensities': ind,\n",
    "        'mean_by_intensity': mean,\n",
    "        'std_by_intensity': std,\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(CONFIG['output_dir'], 'nonlinear_area_average')\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "save_path = os.path.join(save_dir, 'nonlinear_all.pdf')\n",
    "stat_summary = analyze_nonlinear_relationship(region_dirs, score_all, \n",
    "                                                poly_degree=2,\n",
    "                                                factor=2,\n",
    "                                                remove_last_n=2,\n",
    "                                                display_legend=False,\n",
    "                                                show_mean_poly=True,\n",
    "                                                save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(CONFIG['output_dir'], 'nonlinear_area_average')\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "save_path = os.path.join(save_dir, 'nonlinear_all_legend.pdf')\n",
    "stat_summary = analyze_nonlinear_relationship(region_dirs, score_all, \n",
    "                                                poly_degree=2,\n",
    "                                                factor=2,\n",
    "                                                remove_last_n=2,\n",
    "                                                display_legend=True,\n",
    "                                                show_mean_poly=True,\n",
    "                                                save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(CONFIG['output_dir'], 'nonlinear_area_average')\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "save_path = os.path.join(save_dir, 'nonlinear_temp.pdf')\n",
    "stat_summary = analyze_nonlinear_relationship(region_dirs, score_temp, \n",
    "                                                poly_degree=2,\n",
    "                                                factor=2,\n",
    "                                                remove_last_n=1,\n",
    "                                                display_legend=False,\n",
    "                                                show_mean_poly=True,\n",
    "                                                save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(CONFIG['output_dir'], 'nonlinear_area_average')\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "save_path = os.path.join(save_dir, 'nonlinear_prcp.pdf')\n",
    "stat_summary = analyze_nonlinear_relationship(region_dirs, score_prcp, \n",
    "                                                poly_degree=2,\n",
    "                                                factor=2,\n",
    "                                                remove_last_n=2,\n",
    "                                                display_legend=False,\n",
    "                                                show_mean_poly=True,\n",
    "                                                save_path=save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. ERA5 climate vs. CMIP6 climate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_configs = {\n",
    "    'ssp126': plt.cm.Accent(0),\n",
    "    'ssp245': plt.cm.Accent(2),\n",
    "    'ssp370': plt.cm.Accent(6), \n",
    "    'ssp585': 'red',\n",
    "}\n",
    "ssp_scenarios = ['ssp126', 'ssp245', 'ssp370', 'ssp585']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2 , figsize=(14, 5))\n",
    "axes = axes.flatten()\n",
    "ax1 = axes[0]\n",
    "ax2 = axes[1]\n",
    "\n",
    "era_climate = xr.open_dataset(os.path.join(CONFIG['oggm_dir'], 'ERA5_HMA_climate_all.nc')).\\\n",
    "                sel(time=slice(\"1950\", \"2025\")).mean(dim='rgi_id', skipna=True, keep_attrs=True)\n",
    "cmip_climate = xr.open_dataset(os.path.join(CONFIG['oggm_dir'], 'CMIP6_HMA_climate_gcm_mean_all.nc')).\\\n",
    "                sel(time=slice(\"1950\", \"2101\")).mean(dim='rgi_id', skipna=True, keep_attrs=True)\n",
    "era_climate['time'] = pd.date_range(start=\"01/01/1950\", end=\"12/01/2024\", freq=\"MS\")\n",
    "cmip_climate['time'] = pd.date_range(start=\"01/01/1950\", end=\"12/01/2100\", freq=\"MS\")\n",
    "\n",
    "plot_date_range = pd.date_range(start=\"01/01/2000\", end=\"12/01/2024\", freq=\"MS\")\n",
    "era_climate_df = era_climate.to_dataframe().loc[plot_date_range] # df\n",
    "era_temp = era_climate_df['temp'] # series\n",
    "era_temp_positive = era_temp[era_temp>0]\n",
    "annual_positive_degree_days = era_temp_positive.groupby(era_temp_positive.index.year).sum()\n",
    "ax1.plot(annual_positive_degree_days.index, annual_positive_degree_days, color='k', linewidth=2, label='ERA5-Land', zorder=5)\n",
    "\n",
    "era_prcp = era_climate_df['prcp'] # series\n",
    "annual_prcp = era_prcp.groupby(era_prcp.index.year).sum()\n",
    "ax2.plot(annual_prcp.index, annual_prcp, color='k', linewidth=2, zorder=5)\n",
    "\n",
    "for ssp in ssp_scenarios:\n",
    "    cmip_climate_df = cmip_climate.sel(SSP=ssp).to_dataframe().loc[plot_date_range]\n",
    "\n",
    "    cmip_temp = cmip_climate_df['temp'] # series\n",
    "    cmip_temp_positive = cmip_temp[cmip_temp>0]\n",
    "    annual_positive_degree_days = cmip_temp_positive.groupby(cmip_temp_positive.index.year).sum()\n",
    "    ax1.plot(annual_positive_degree_days.index, annual_positive_degree_days, color=plot_configs[ssp],\n",
    "            linestyle='-', linewidth=1.5, label=f'CMIP6 {ssp.upper()}')\n",
    "\n",
    "    cmip_prcp = cmip_climate_df['prcp'] # series\n",
    "    annual_prcp = cmip_prcp.groupby(cmip_prcp.index.year).sum()\n",
    "    ax2.plot(annual_prcp.index, annual_prcp, color=plot_configs[ssp],\n",
    "            linestyle='-', linewidth=1.5)\n",
    "\n",
    "ax1.set_ylabel('Annual PDD sum (°C)', fontsize=16)\n",
    "ax2.set_ylabel('Annual precipitation (mm)', fontsize=16)\n",
    "ax1.set_xlabel('Year', fontsize=16)\n",
    "ax2.set_xlabel('Year', fontsize=16)\n",
    "\n",
    "ax1.tick_params(labelsize=14)\n",
    "ax1.yaxis.set_major_locator(MultipleLocator(2))\n",
    "ax2.tick_params(labelsize=14)\n",
    "\n",
    "ax1.legend(fontsize=16, ncol=3, \n",
    "        bbox_to_anchor=(0.3, -0.4, 1.7, 2),\n",
    "        loc='lower right',\n",
    "        mode='expand')\n",
    "\n",
    "# plt.savefig(os.path.join(CONFIG['output_dir'], 'climate', 'ERAvsCMIP_hist.png'), dpi=600, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prcp trend using robust fitting\n",
    "X = sm.add_constant(np.arange(len(era_prcp)))\n",
    "y = era_prcp.values\n",
    "\n",
    "rlm_model = RLM(y, X, M=sm.robust.norms.HuberT())\n",
    "rlm_results = rlm_model.fit()\n",
    "\n",
    "slope = rlm_results.params[1]\n",
    "pvalue = rlm_results.pvalues[1]\n",
    "\n",
    "print(f'slope = {slope} mm/month')\n",
    "print(f'p = {pvalue}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prcp trend using robust fitting\n",
    "X = sm.add_constant(np.arange(len(annual_prcp)))[:-2]\n",
    "y = annual_prcp.values[:-2]\n",
    "\n",
    "rlm_model = RLM(y, X, M=sm.robust.norms.HuberT())\n",
    "rlm_results = rlm_model.fit()\n",
    "\n",
    "slope = rlm_results.params[1]\n",
    "pvalue = rlm_results.pvalues[1]\n",
    "\n",
    "print(f'slope = {slope} mm/year')\n",
    "print(f'p = {pvalue}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp trend using robust fitting\n",
    "X = sm.add_constant(np.arange(len(era_temp)))\n",
    "y = era_temp.values\n",
    "\n",
    "rlm_model = RLM(y, X, M=sm.robust.norms.HuberT())\n",
    "rlm_results = rlm_model.fit()\n",
    "\n",
    "slope = rlm_results.params[1]\n",
    "pvalue = rlm_results.pvalues[1]\n",
    "\n",
    "print(f'slope = {slope} ℃/month')\n",
    "print(f'p = {pvalue}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp trend using robust fitting\n",
    "X = sm.add_constant(np.arange(len(annual_positive_degree_days)))[:-2]\n",
    "y = annual_positive_degree_days.values[:-2]\n",
    "\n",
    "rlm_model = RLM(y, X, M=sm.robust.norms.HuberT())\n",
    "rlm_results = rlm_model.fit()\n",
    "\n",
    "slope = rlm_results.params[1]\n",
    "pvalue = rlm_results.pvalues[1]\n",
    "\n",
    "print(f'slope = {slope} ℃/year')\n",
    "print(f'p = {pvalue}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gee_dask",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
